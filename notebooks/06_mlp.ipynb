{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate.data_loader import DataLoader\n",
    "from datasets import ClassLabel, Dataset, load_dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch import nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(\n",
    "    model_output: torch.Tensor, attention_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    token_embeddings = model_output[\n",
    "        0\n",
    "    ]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model = model.cuda() if torch.cuda.is_available() else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance = load_dataset(\"Marina-C/question-answer-Subject-Finance-Instruct\")\n",
    "health = load_dataset(\"iecjsu/lavita-ChatDoctor-HealthCareMagic-100k\")\n",
    "law = load_dataset(\"dim/law_stackexchange_prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_df = pd.DataFrame(\n",
    "    {\"messages\": [row[1][\"content\"] for row in finance[\"train\"][\"messages\"]]}\n",
    ")\n",
    "\n",
    "health_df = pd.DataFrame({\"messages\": health[\"train\"][\"input\"]})\n",
    "\n",
    "law_df = pd.DataFrame({\"messages\": law[\"train\"][\"prompt\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels\n",
    "finance_df[\"label\"] = 0\n",
    "health_df[\"label\"] = 1\n",
    "law_df[\"label\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([finance_df, health_df, law_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = Dataset.from_pandas(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = combined_dataset.cast_column(\n",
    "    \"label\", ClassLabel(num_classes=3, names=[\"finance\", \"health\", \"law\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "FREEZE_TRAIN_LR = 1e-4\n",
    "UNFROZEN_TRAIN_LR_WARMUP = 1e-6\n",
    "UNFROZEN_TRAIN_LR = 1e-5\n",
    "WARMUP_STEPS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_func = partial(\n",
    "    tokenizer, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer_func(combined_dataset[\"messages\"][0]).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(**encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mean_pooling(out, encoded[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = F.normalize(out, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DyT(nn.Module):\n",
    "    def __init__(self, hidden_size: int, init_alpha: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(1) * init_alpha)\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.gamma * torch.tanh(self.alpha * x) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            DyT(128),\n",
    "            nn.Linear(128, 64),\n",
    "            DyT(64),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head = Head(384, 3).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_frozen = AdamW(model.head.parameters(), lr=FREEZE_TRAIN_LR, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = combined_dataset.train_test_split(\n",
    "    test_size=0.1, stratify_by_column=\"label\"\n",
    ")\n",
    "train_dataset = combined_dataset[\"train\"]\n",
    "test_dataset = combined_dataset[\"test\"]\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_frozen() -> None:\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.head.train()\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    last_test_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    while True:\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            optim_frozen.zero_grad()\n",
    "            input_msg = batch[\"messages\"]\n",
    "            input = tokenizer_func(input_msg).to(model.device)\n",
    "            target = batch[\"label\"].to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(**input)\n",
    "                out = mean_pooling(out, input[\"attention_mask\"])\n",
    "                out = F.normalize(out, p=2, dim=1)\n",
    "\n",
    "            out = model.head(out)\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optim_frozen.step()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            losses = []\n",
    "            for batch in test_dataloader:\n",
    "                input_msg = batch[\"messages\"]\n",
    "                input = tokenizer_func(input_msg).to(model.device)\n",
    "                target = batch[\"label\"].to(model.device)\n",
    "                out = model.forward(**input)\n",
    "                out = mean_pooling(out, input[\"attention_mask\"])\n",
    "                out = F.normalize(out, p=2, dim=1)\n",
    "                out = model.head(out)\n",
    "                loss = criterion(out, target)\n",
    "                losses.append(loss.item())\n",
    "            test_loss = sum(losses) / len(losses)\n",
    "            print(f\"Test loss: {test_loss}\")\n",
    "            if test_loss < last_test_loss:\n",
    "                last_test_loss = test_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frozen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw_splits = {'train': 'train_dataset.csv', 'validation': 'val_dataset.csv', 'test': 'test_dataset.csv'}\n",
    "inference_df = pd.read_csv(\"hf://datasets/Arsive/toxicity_classification_jigsaw/\" + jigsaw_splits[\"validation\"])\n",
    "\n",
    "olid_splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "olid_df = pd.read_csv(\"hf://datasets/christophsonntag/OLID/\" + olid_splits[\"train\"])\n",
    "\n",
    "inference_df = inference_df[(inference_df[\"toxic\"] == 1) |\n",
    "                            (inference_df[\"severe_toxic\"] == 1) |\n",
    "                            (inference_df[\"obscene\"] == 1) |\n",
    "                            (inference_df[\"threat\"] == 1) |\n",
    "                            (inference_df[\"insult\"] == 1) |\n",
    "                            (inference_df[\"identity_hate\"] == 1)]\n",
    "\n",
    "olid_df = olid_df.rename(columns={\"cleaned_tweet\": \"prompt\"})\n",
    "olid_df[\"label\"] = 0\n",
    "\n",
    "inference_df = inference_df.rename(columns={\"comment_text\": \"prompt\"})\n",
    "inference_df[\"label\"] = 0\n",
    "\n",
    "inference_df = pd.concat([inference_df, olid_df], ignore_index=True)\n",
    "inference_df = inference_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = Dataset.from_pandas(inference_df)\n",
    "inference_dataloader = DataLoader(\n",
    "    inference_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy() -> float:\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(inference_dataloader, desc=\"Evaluating\"):\n",
    "            input_msg = batch[\"messages\"]\n",
    "            input = tokenizer_func(input_msg).to(model.device)\n",
    "            target = batch[\"label\"].to(model.device)\n",
    "\n",
    "            # Get embeddings from base model\n",
    "            out = model.forward(**input)\n",
    "            out = mean_pooling(out, input[\"attention_mask\"])\n",
    "            out = F.normalize(out, p=2, dim=1)\n",
    "\n",
    "            # Forward through head\n",
    "            logits = model.head(out)\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Print per-class accuracy\n",
    "    class_correct = [0, 0, 0]\n",
    "    class_total = [0, 0, 0]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Class accuracy\"):\n",
    "            input_msg = batch[\"messages\"]\n",
    "            input = tokenizer_func(input_msg).to(model.device)\n",
    "            target = batch[\"label\"].to(model.device)\n",
    "\n",
    "            out = model.forward(**input)\n",
    "            out = mean_pooling(out, input[\"attention_mask\"])\n",
    "            out = F.normalize(out, p=2, dim=1)\n",
    "            logits = model.head(out)\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            # Calculate per-class accuracy\n",
    "            for i in range(target.size(0)):\n",
    "                label = target[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i] == target[i]:\n",
    "                    class_correct[label] += 1\n",
    "\n",
    "    for i in range(3):\n",
    "        class_name = combined_dataset[\"test\"].features[\"label\"].names[i]\n",
    "        class_acc = 100 * class_correct[i] / class_total[i]\n",
    "        print(f\"Accuracy of {class_name}: {class_acc:.2f}%\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Run the evaluation\n",
    "accuracy = evaluate_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix() -> None:\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Collecting predictions\"):\n",
    "            input_msg = batch[\"messages\"]\n",
    "            input = tokenizer_func(input_msg).to(model.device)\n",
    "            target = batch[\"label\"].to(model.device)\n",
    "\n",
    "            out = model.forward(**input)\n",
    "            out = mean_pooling(out, input[\"attention_mask\"])\n",
    "            out = F.normalize(out, p=2, dim=1)\n",
    "            logits = model.head(out)\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=combined_dataset[\"test\"].features[\"label\"].names,\n",
    "        yticklabels=combined_dataset[\"test\"].features[\"label\"].names,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-validation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
