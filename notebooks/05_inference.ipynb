{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "import xgboost as xgb\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "import fasttext\n",
    "\n",
    "from prompt_classifier.modeling.dspy_gpt import GPT4oMini\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-03-16 18:44:20.101159776 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-03-16 18:44:20.101202116 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "general_prompts = pd.read_csv(\"data/processed/general_prompts.csv\")\n",
    "finance_prompts = pd.read_csv(\"data/processed/finance_prompts.csv\")\n",
    "\n",
    "finance_dataset = (\n",
    "    pd.concat([finance_prompts, general_prompts])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "general_prompts_interim = pd.read_csv(\"data/interim/general_prompts.csv\")\n",
    "finance_prompts_interim = pd.read_csv(\"data/interim/finance_prompts.csv\")\n",
    "\n",
    "finance_dataset_interim = (\n",
    "    pd.concat([finance_prompts_interim, general_prompts_interim])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data = finance_dataset_interim.sample(n=800)\n",
    "test_data = finance_dataset_interim.drop(train_data.index).sample(n=4000)\n",
    "\n",
    "# GPT Classifier\n",
    "gpt_classifier = GPT4oMini(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    proxy_url=os.getenv(\"PROXY_URL\"),\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    domain=\"finance\",\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    ")\n",
    "\n",
    "try:\n",
    "    gpt_classifier.load_model(\"models/gpt-4o-mini-finance.json\")\n",
    "\n",
    "    test_predictions, test_actuals, test_latency = gpt_classifier.predict()\n",
    "    print(f\"Test Accuracy: {metrics.accuracy_score(test_actuals, test_predictions)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error running GPT model: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    test_data = finance_dataset_interim.sample(n=30_000)\n",
    "    # ModernBERT Classifier\n",
    "    bert_classifier = ModernBERTNLI(domain=\"finance\")\n",
    "    bert_classifier.classifier.model.to(\"cuda\")\n",
    "\n",
    "    # Test predictions\n",
    "    test_predictions = []\n",
    "    test_times = []\n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        start_time = time.perf_counter_ns()\n",
    "        pred = bert_classifier.predict(row[\"prompt\"])\n",
    "        test_predictions.append(pred)\n",
    "        test_times.append(time.perf_counter_ns() - start_time)\n",
    "\n",
    "    test_acc = metrics.accuracy_score(test_data[\"label\"], test_predictions)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error running ModernBERT model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = finance_dataset.sample(frac=0.7).reset_index(drop=True)\n",
    "test_data = finance_dataset.drop(train_data.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 9000/9000 [00:00<00:00, 18818.39it/s]\n"
     ]
    }
   ],
   "source": [
    "actuals_ft = []\n",
    "predictions_ft = []\n",
    "prediction_times_ft = []\n",
    "\n",
    "# fastText\n",
    "fasttext_classifier = FastTextClassifier(train_data=train_data, test_data=test_data)\n",
    "fasttext_classifier.model = fasttext.load_model(\"models/fastText_finance_fasttext.bin\")\n",
    "\n",
    "for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "    text = str(row[\"prompt\"])\n",
    "    query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "    start_time = time.perf_counter_ns()\n",
    "    prediction = fasttext_classifier.model.predict(query)\n",
    "    end_time = time.perf_counter_ns()\n",
    "\n",
    "    prediction_times_ft.append(end_time - start_time)\n",
    "\n",
    "    if prediction[0][0] == \"__label__1\":\n",
    "        predictions_ft.append(1)\n",
    "    else:\n",
    "        predictions_ft.append(0)\n",
    "\n",
    "    actuals_ft.append(row[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedding test data\n",
    "start_time = time.perf_counter_ns()\n",
    "test_embeds = np.array(list(baai_embedding.embed(test_data[\"prompt\"])))\n",
    "end_time = time.perf_counter_ns()\n",
    "embed_times = end_time - start_time\n",
    "\n",
    "mean_embed_time = embed_times / len(train_data + test_data)\n",
    "\n",
    "with open(\"models/SVM_finance_baai.pkl\", \"rb\") as svm_file:\n",
    "    svm_classifier = pkl.load(svm_file)\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.load_model(\"models/XGBoost_finance_baai.json\")\n",
    "\n",
    "predictions_xgb = []\n",
    "predictions_svm = []\n",
    "\n",
    "prediction_times_xgb = []\n",
    "prediction_times_svm = []\n",
    "\n",
    "for _, test_embed in enumerate(test_embeds):\n",
    "    start_time = time.perf_counter_ns()\n",
    "    prediction = svm_classifier.predict(test_embed.reshape(1, -1))\n",
    "    end_time = time.perf_counter_ns()\n",
    "\n",
    "    prediction_times_svm.append(end_time - start_time)\n",
    "    predictions_svm.append(prediction[0])\n",
    "\n",
    "\n",
    "for _, test_embed in enumerate(test_embeds):\n",
    "    start_time = time.perf_counter_ns()\n",
    "    prediction = xgb_classifier.predict(test_embed.reshape(1, -1))\n",
    "    end_time = time.perf_counter_ns()\n",
    "\n",
    "    prediction_times_xgb.append(end_time - start_time)\n",
    "    predictions_xgb.append(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_ms = {\n",
    "    'fastText': np.array(prediction_times_ft) / 1_000_000,\n",
    "    'XGBoost': np.array(prediction_times_xgb) / 1_000_000,\n",
    "    'SVM': np.array(prediction_times_svm) / 1_000_000\n",
    "}\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'Model': [k for k,v in times_ms.items() for _ in v],\n",
    "    'Latency (ms)': [x for v in times_ms.values() for x in v]\n",
    "})\n",
    "\n",
    "# Create boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=plot_data, x='Model', y='Latency (ms)', \n",
    "            order=[x[0] for x in sorted(zip(times_ms.keys(), \n",
    "                   [np.mean(v) for v in times_ms.values()]), \n",
    "                   key=lambda x: x[1])])\n",
    "\n",
    "plt.title('Model Latency Comparison')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print average latencies\n",
    "for model, times in times_ms.items():\n",
    "    print(f\"{model} average latency: {times.mean():.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
