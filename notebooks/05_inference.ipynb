{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Domain Generalization: Hate Speech Detection\n",
    "\n",
    "This notebook evaluates how well domain-specific classifiers generalize to out-of-domain hate speech detection.\n",
    "We test various model architectures trained on domain classification to see if they can effectively identify hate speech.\n",
    "\n",
    "Key aspects evaluated:\n",
    "- Few-shot transfer capabilities using DSPy\n",
    "- Model robustness across different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "This notebook evaluates several classification models:\n",
    "1. **LLM-based**: Using Qwen 2.5 for zero-shot classification\n",
    "2. **BERT-based**: ModernBERT with NLI approach\n",
    "3. **Traditional ML**: \n",
    "   - fastText for efficient text classification\n",
    "   - SVM and XGBoost with different embeddings\n",
    "\n",
    "Each model is evaluated on hate speech detection as an out-of-domain task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import dependencies and initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import cupy as cp\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as ort\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from prompt_classifier.metrics import evaluate_run\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(22)\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch_results(batch_results, model_name):\n",
    "    filename = f'reports/batch_{model_name}.csv'\n",
    "    \n",
    "    df = pd.DataFrame(batch_results)\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        # If file exists, append without header\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # If file doesn't exist, create new with header\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "def load_batch_data():\n",
    "    batch_data = pd.read_csv('data/batch_data.csv')\n",
    "    return batch_data[\"prompt\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "batch_data = load_batch_data()\n",
    "batch_sizes = [1, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Datasets\n",
    "\n",
    "Load various hate speech datasets for evaluation:\n",
    "- Jigsaw Toxicity\n",
    "- OLID\n",
    "- HateXplain\n",
    "- TUKE Slovak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_data = pd.read_csv(\"data/domain_eval.csv\")\n",
    "ood_data = pd.read_csv(\"data/ood_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jigsaw_splits = {\n",
    "#     \"train\": \"train_dataset.csv\",\n",
    "#     \"validation\": \"val_dataset.csv\",\n",
    "#     \"test\": \"test_dataset.csv\",\n",
    "# }\n",
    "# jigsaw_df = pd.read_csv(\n",
    "#     \"hf://datasets/Arsive/toxicity_classification_jigsaw/\"\n",
    "#     + jigsaw_splits[\"validation\"]\n",
    "# )\n",
    "\n",
    "# jigsaw_df = jigsaw_df[\n",
    "#     (jigsaw_df[\"toxic\"] == 1)\n",
    "#     | (jigsaw_df[\"severe_toxic\"] == 1)\n",
    "#     | (jigsaw_df[\"obscene\"] == 1)\n",
    "#     | (jigsaw_df[\"threat\"] == 1)\n",
    "#     | (jigsaw_df[\"insult\"] == 1)\n",
    "#     | (jigsaw_df[\"identity_hate\"] == 1)\n",
    "# ]\n",
    "\n",
    "# jigsaw_df = jigsaw_df.rename(columns={\"comment_text\": \"prompt\"})\n",
    "# jigsaw_df[\"label\"] = 0\n",
    "# jigsaw_df = jigsaw_df[[\"prompt\", \"label\"]]\n",
    "# jigsaw_df = jigsaw_df.dropna(subset=[\"prompt\"])\n",
    "# jigsaw_df = jigsaw_df[jigsaw_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# # Load OLID dataset\n",
    "# olid_splits = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
    "# olid_df = pd.read_csv(\"hf://datasets/christophsonntag/OLID/\" + olid_splits[\"test\"])\n",
    "# olid_df = olid_df.rename(columns={\"cleaned_tweet\": \"prompt\"})\n",
    "# olid_df[\"label\"] = 0\n",
    "# olid_df = olid_df[[\"prompt\", \"label\"]]\n",
    "# olid_df = olid_df.dropna(subset=[\"prompt\"])\n",
    "# olid_df = olid_df[olid_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# # Load hateXplain dataset\n",
    "# hate_xplain = pd.read_parquet(\n",
    "#     \"hf://datasets/nirmalendu01/hateXplain_filtered/data/train-00000-of-00001.parquet\"\n",
    "# )\n",
    "# hate_xplain = hate_xplain.rename(columns={\"test_case\": \"prompt\"})\n",
    "# hate_xplain = hate_xplain[(hate_xplain[\"gold_label\"] == \"hateful\")]\n",
    "# hate_xplain = hate_xplain[[\"prompt\", \"label\"]]\n",
    "# hate_xplain[\"label\"] = 0\n",
    "# hate_xplain = hate_xplain.dropna(subset=[\"prompt\"])\n",
    "# hate_xplain = hate_xplain[hate_xplain[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# # Load TUKE Slovak dataset\n",
    "# tuke_sk_splits = {\"train\": \"train.json\", \"test\": \"test.json\"}\n",
    "# tuke_sk_df = pd.read_json(\n",
    "#     \"hf://datasets/TUKE-KEMT/hate_speech_slovak/\" + tuke_sk_splits[\"test\"],\n",
    "#     lines=True,\n",
    "# )\n",
    "# tuke_sk_df = tuke_sk_df.rename(columns={\"text\": \"prompt\"})\n",
    "# tuke_sk_df = tuke_sk_df[tuke_sk_df[\"label\"] == 0]\n",
    "# tuke_sk_df = tuke_sk_df[[\"prompt\", \"label\"]]\n",
    "# tuke_sk_df = tuke_sk_df.dropna(subset=[\"prompt\"])\n",
    "# tuke_sk_df = tuke_sk_df[tuke_sk_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "\n",
    "# dkk_all = pd.read_parquet(\"data/test-00000-of-00001.parquet\")\n",
    "# dkk_all = dkk_all.rename(columns={\"text\": \"prompt\"})\n",
    "# dkk_all[\"label\"] = 0\n",
    "# dkk_all = dkk_all.dropna(subset=[\"prompt\"])\n",
    "# dkk_all = dkk_all[dkk_all[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# web_questions = pd.read_parquet(\"hf://datasets/Stanford/web_questions/\" + splits[\"test\"])\n",
    "\n",
    "# web_questions['prompt'] = web_questions['question']\n",
    "# web_questions['label'] = 0\n",
    "# web_questions['dataset'] = 'web_questions'\n",
    "# web_questions = web_questions[['prompt', 'label']]\n",
    "\n",
    "# splits = {'train': 'data/train-00000-of-00001-7ebb9cdef03dd950.parquet', 'test': 'data/test-00000-of-00001-fbd3905b045b12b8.parquet'}\n",
    "# ml_questions = pd.read_parquet(\"hf://datasets/mjphayes/machine_learning_questions/\" + splits[\"test\"])\n",
    "\n",
    "# ml_questions['prompt'] = ml_questions['question']\n",
    "# ml_questions['label'] = 0\n",
    "# ml_questions['dataset'] = 'machine_learning_questions'\n",
    "# ml_questions = ml_questions[['prompt', 'label']]\n",
    "\n",
    "# datasets = {\n",
    "#     \"jigsaw\": jigsaw_df,\n",
    "#     \"olid\": olid_df,\n",
    "#     \"hate_xplain\": hate_xplain,\n",
    "#     \"tuke_sk\": tuke_sk_df,\n",
    "#     \"dkk\": dkk_all,\n",
    "#     \"web_questions\": web_questions,\n",
    "#     \"ml_questions\": ml_questions,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Overview\n",
    "\n",
    "We use four major hate speech datasets:\n",
    "\n",
    "1. **Jigsaw Toxicity**\n",
    "   - Multi-label toxicity classification\n",
    "   - Includes toxic, severe_toxic, obscene, threat, insult, identity_hate labels\n",
    "\n",
    "2. **OLID (Offensive Language Identification Dataset)**\n",
    "   - Hierarchical labeling of offensive language\n",
    "   - Focuses on Twitter content\n",
    "\n",
    "3. **HateXplain**\n",
    "   - Annotated with rationales for hate speech\n",
    "   - Includes target community information\n",
    "\n",
    "4. **TUKE Slovak**\n",
    "   - Slovak language hate speech dataset\n",
    "   - Tests cross-lingual generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n",
    "Using multiple embedding approaches:\n",
    "- **BAAI BGE**: Optimized for semantic similarity\n",
    "- **MiniLM**: Efficient sentence transformers model\n",
    "- **TF-IDF**: Traditional bag-of-words approach\n",
    "\n",
    "These embeddings are used with SVM and XGBoost classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-04-24 16:54:00.771001950 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-24 16:54:00.771054357 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-04-24 16:54:01.560910484 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-24 16:54:01.560986724 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_finance = pkl.load(open(\"models/tfidf_finance.pkl\", \"rb\"))\n",
    "tfidf_healthcare = pkl.load(open(\"models/tfidf_healthcare.pkl\", \"rb\"))\n",
    "tfidf_law = pkl.load(open(\"models/tfidf_law.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Process\n",
    "\n",
    "For each dataset and model combination:\n",
    "1. Load pre-trained domain classifiers\n",
    "2. Process test samples through each domain classifier\n",
    "3. Combine predictions using OR logic (any domain=1 -> toxic=0)\n",
    "4. Calculate metrics:\n",
    "   - Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"ollama_chat/qwen2.5:14b\"\n",
    "\n",
    "# for domain, inference_df in datasets.items():\n",
    "#     try:\n",
    "#         llm_classifier_finance = LlmClassifier(\n",
    "#             api_key=\"\",\n",
    "#             api_base=\"http://localhost:11434\",\n",
    "#             model_name=model_name,\n",
    "#             domain=\"finance\",\n",
    "#             train_data=inference_df,\n",
    "#             test_data=inference_df,\n",
    "#         )\n",
    "\n",
    "#         llm_classifier_healthcare = LlmClassifier(\n",
    "#             api_key=\"\",\n",
    "#             api_base=\"http://localhost:11434\",\n",
    "#             model_name=model_name,\n",
    "#             domain=\"healthcare\",\n",
    "#             train_data=inference_df,\n",
    "#             test_data=inference_df,\n",
    "#         )\n",
    "\n",
    "#         llm_classifier_law = LlmClassifier(\n",
    "#             api_key=\"\",\n",
    "#             api_base=\"http://localhost:11434\",\n",
    "#             model_name=model_name,\n",
    "#             domain=\"law\",\n",
    "#             train_data=inference_df,\n",
    "#             test_data=inference_df,\n",
    "#         )\n",
    "\n",
    "#         # Load models\n",
    "#         llm_classifier_finance.load_model(\"models/qwen2.5:14b-finance.json\")\n",
    "#         llm_classifier_healthcare.load_model(\"models/qwen2.5:14b-healthcare.json\")\n",
    "#         llm_classifier_law.load_model(\"models/qwen2.5:14b-law.json\")\n",
    "\n",
    "#         predictions_llm = []\n",
    "#         prediction_times_llm = []\n",
    "#         actuals_llm = []\n",
    "\n",
    "#         # Get predictions for each prompt\n",
    "#         for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "#             start_time = time.perf_counter_ns()\n",
    "\n",
    "#             # Get predictions from all models\n",
    "#             pred_finance = llm_classifier_finance.predict_single(row[\"prompt\"])\n",
    "#             pred_healthcare = llm_classifier_healthcare.predict_single(row[\"prompt\"])\n",
    "#             pred_law = llm_classifier_law.predict_single(row[\"prompt\"])\n",
    "\n",
    "#             end_time = time.perf_counter_ns()\n",
    "#             prediction_times_llm.append(end_time - start_time)\n",
    "\n",
    "#             # If all models predict 0, final prediction is 0, otherwise 1\n",
    "#             predictions_llm.append(\n",
    "#                 0 if (pred_finance == 0 and pred_healthcare == 0 and pred_law == 0) else 1\n",
    "#             )\n",
    "#             actuals_llm.append(row[\"label\"])\n",
    "\n",
    "#         evaluate_run(\n",
    "#             predictions=predictions_llm,\n",
    "#             true_labels=actuals_llm,\n",
    "#             latency=statistics.mean(prediction_times_llm),\n",
    "#             domain=domain,\n",
    "#             embed_model=\"qwen2.5\",\n",
    "#             model_name=model_name,\n",
    "#             train_acc=0.0,\n",
    "#             cost=0.0,\n",
    "#             training=False,\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error running LLM model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_classifier_finance = ModernBERTNLI(domain=\"finance\")\n",
    "# bert_classifier_healthcare = ModernBERTNLI(domain=\"healthcare\")\n",
    "# bert_classifier_law = ModernBERTNLI(domain=\"law\")\n",
    "\n",
    "# try:\n",
    "#     # Move models to GPU\n",
    "#     bert_classifier_finance.classifier.model.to(\"cuda\")\n",
    "#     bert_classifier_healthcare.classifier.model.to(\"cuda\")\n",
    "#     bert_classifier_law.classifier.model.to(\"cuda\")\n",
    "\n",
    "#     for domain, inference_df in datasets.items():\n",
    "#         predictions_bert = []\n",
    "#         prediction_times_bert = []\n",
    "#         actuals_bert = []\n",
    "#         # Get predictions for each prompt\n",
    "#         for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "#             start_time = time.perf_counter_ns()\n",
    "\n",
    "#             # Get predictions from all models\n",
    "#             pred_finance = bert_classifier_finance.predict(row[\"prompt\"])\n",
    "#             pred_healthcare = bert_classifier_healthcare.predict(row[\"prompt\"])\n",
    "#             pred_law = bert_classifier_law.predict(row[\"prompt\"])\n",
    "\n",
    "#             end_time = time.perf_counter_ns()\n",
    "#             prediction_times_bert.append(end_time - start_time)\n",
    "\n",
    "#             # If all models predict 0, final prediction is 0, otherwise 1\n",
    "#             predictions_bert.append(\n",
    "#                 0 if (pred_finance == 0 and pred_healthcare == 0 and pred_law == 0) else 1\n",
    "#             )\n",
    "#             actuals_bert.append(row[\"label\"])\n",
    "\n",
    "#     evaluate_run(\n",
    "#         predictions=predictions_bert,\n",
    "#         true_labels=actuals_bert,\n",
    "#         latency=statistics.mean(prediction_times_bert),\n",
    "#         domain=domain,\n",
    "#         embed_model=\"BERT\",\n",
    "#         model_name=\"ModernBERT\",\n",
    "#         train_acc=0.0,\n",
    "#         cost=0.0,\n",
    "#         training=False,\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(f\"Error running ModernBERT models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fasttext_law = fasttext.load_model(\"models/fastText_law_fasttext.bin\")\n",
    "fasttext_finance = fasttext.load_model(\"models/fastText_finance_fasttext.bin\")\n",
    "fasttext_healthcare = fasttext.load_model(\"models/fastText_healthcare_fasttext.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_results = []\n",
    "try:\n",
    "    for batch_size in batch_sizes:\n",
    "        all_results = []\n",
    "        num_batches = 0\n",
    "        # create batches from batch_data\n",
    "        batches = [\n",
    "            batch_data[i : i + batch_size] for i in range(0, len(batch_data), batch_size)\n",
    "        ]\n",
    "        for batch in batches:\n",
    "            num_batches += 1\n",
    "            batch_metrics = {\n",
    "                \"time_taken_law\": 0,\n",
    "                \"time_taken_finance\": 0,  \n",
    "                \"time_taken_healthcare\": 0\n",
    "            }\n",
    "\n",
    "            batch_prompt = [prompt.replace(\"\\n\", \" \") for prompt in batch]\n",
    "            \n",
    "            # Time law predictions\n",
    "            start_time = time.perf_counter()\n",
    "            law_preds = fasttext_law.predict(batch_prompt)\n",
    "            batch_metrics['time_taken_law'] += time.perf_counter() - start_time\n",
    "\n",
    "            # Time finance predictions\n",
    "            start_time = time.perf_counter()\n",
    "            finance_preds = fasttext_finance.predict(batch_prompt)\n",
    "            batch_metrics['time_taken_finance'] += time.perf_counter() - start_time\n",
    "\n",
    "            # Time healthcare predictions\n",
    "            start_time = time.perf_counter()\n",
    "            health_preds = fasttext_healthcare.predict(batch_prompt)\n",
    "            batch_metrics['time_taken_healthcare'] += time.perf_counter() - start_time\n",
    "\n",
    "            results =[]\n",
    "            for prediction_finance, prediction_healthcare, prediction_law in zip(finance_preds[0], health_preds[0], law_preds[0]):\n",
    "                  results.append({\n",
    "                    'finance': 1 if prediction_finance[0] == \"__label__1\" else 0,\n",
    "                    'healthcare': 1 if prediction_healthcare[0] == \"__label__1\" else 0,\n",
    "                    'law': 1 if prediction_law[0] == \"__label__1\" else 0\n",
    "                })\n",
    "\n",
    "            batch_results.append({\n",
    "                \"batch_size\": batch_size,\n",
    "                \"time_taken_embed\": 0,\n",
    "                \"time_taken_law\": batch_metrics['time_taken_law'],\n",
    "                \"time_taken_finance\": batch_metrics['time_taken_finance'],\n",
    "                \"time_taken_healthcare\": batch_metrics['time_taken_healthcare'],\n",
    "                \"results\": results,\n",
    "                \"model_name\": \"fasttext\",\n",
    "                \"embedding_model\": \"fasttext\",\n",
    "                \"embedding\": False\n",
    "            })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FastText error: {e}\")\n",
    "pd.DataFrame(batch_results).to_csv(\"reports/batch_fasttext.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - SVM, XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:54:12] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cpu, while the input data is on: cuda:0.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "for embedding_model in [\"mini\", \"baai\", \"tf_idf\"]:    \n",
    "    svm_batch_results = []\n",
    "    xgb_batch_results = []\n",
    "    try:\n",
    "        xgb_law = XGBClassifier(\n",
    "            tree_method=\"hist\",\n",
    "            device=\"cuda\",\n",
    "            seed=22\n",
    "        )\n",
    "        xgb_finance = XGBClassifier(\n",
    "            tree_method=\"hist\",\n",
    "            device=\"cuda\",\n",
    "            seed=22\n",
    "        )\n",
    "        xgb_healthcare = XGBClassifier(\n",
    "            tree_method=\"hist\",\n",
    "            device=\"cuda\",\n",
    "            seed=22\n",
    "        )\n",
    "\n",
    "        xgb_law.load_model(f\"models/XGBoost_law_{embedding_model}.json\")\n",
    "        xgb_finance.load_model(f\"models/XGBoost_finance_{embedding_model}.json\")\n",
    "        xgb_healthcare.load_model(f\"models/XGBoost_healthcare_{embedding_model}.json\")\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "            svm_all_results = []\n",
    "            xgb_all_results = []\n",
    "            num_batches = 0\n",
    "\n",
    "            batches = [\n",
    "                batch_data[i : i + batch_size] for i in range(0, len(batch_data), batch_size)\n",
    "            ]\n",
    "            for batch in batches:\n",
    "                num_batches += 1\n",
    "\n",
    "                batch_metrics = {\n",
    "                    \"embed_time\": 0,\n",
    "                    \"svm_law_time\": 0,\n",
    "                    \"svm_finance_time\": 0,\n",
    "                    \"svm_health_time\": 0,\n",
    "                    \"xgb_law_time\": 0,\n",
    "                    \"xgb_finance_time\": 0,\n",
    "                    \"xgb_health_time\": 0\n",
    "                }\n",
    "\n",
    "                # Time embeddings\n",
    "                start_time = time.perf_counter()\n",
    "                if embedding_model == \"tf_idf\":\n",
    "                    embeds = tfidf_finance.transform(batch)\n",
    "                    embeds = embeds.toarray()\n",
    "                elif embedding_model == \"mini\":\n",
    "                    embeds = list(mini_embedding.embed(batch))\n",
    "                else: # baai\n",
    "                    embeds = list(baai_embedding.embed(batch))\n",
    "                batch_metrics['embed_time'] += time.perf_counter() - start_time\n",
    "\n",
    "                embeds = cp.array(embeds)\n",
    "\n",
    "                # XGB predictions\n",
    "                start_time = time.perf_counter()\n",
    "                xgb_law_preds = xgb_law.predict(embeds)\n",
    "                batch_metrics['xgb_law_time'] += time.perf_counter() - start_time\n",
    "\n",
    "                start_time = time.perf_counter()\n",
    "                xgb_finance_preds = xgb_finance.predict(embeds)\n",
    "                batch_metrics['xgb_finance_time'] += time.perf_counter() - start_time\n",
    "\n",
    "                start_time = time.perf_counter()\n",
    "                xgb_health_preds = xgb_healthcare.predict(embeds)\n",
    "                batch_metrics['xgb_health_time'] += time.perf_counter() - start_time\n",
    "\n",
    "                xgb_batch_preds = [1 if (l or f or h) else 0\n",
    "                                  for l,f,h in zip(xgb_law_preds, xgb_finance_preds, xgb_health_preds)]\n",
    "\n",
    "\n",
    "                xgb_batch_results.append({\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"time_taken_embed\": batch_metrics['embed_time'],\n",
    "                    \"time_taken_law\": batch_metrics['xgb_law_time'],\n",
    "                    \"time_taken_finance\": batch_metrics['xgb_finance_time'],\n",
    "                    \"time_taken_healthcare\": batch_metrics['xgb_health_time'],\n",
    "                    \"results\": xgb_batch_preds,\n",
    "                    \"model_name\": \"xgb\",\n",
    "                    \"embedding_model\": embedding_model,\n",
    "                    \"embedding\": True\n",
    "                })\n",
    "        pd.DataFrame(xgb_batch_results).to_csv(f\"reports/batch_xgb_{embedding_model}.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {embedding_model}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_model in [\"mini\", \"baai\", \"tf_idf\"]:    \n",
    "    svm_batch_results = []\n",
    "    xgb_batch_results = []\n",
    "\n",
    "    try:\n",
    "        # Load models\n",
    "        svm_law = pkl.load(open(f\"models/SVM_law_{embedding_model}.pkl\", \"rb\"))\n",
    "        svm_finance = pkl.load(open(f\"models/SVM_finance_{embedding_model}.pkl\", \"rb\"))\n",
    "        svm_healthcare = pkl.load(open(f\"models/SVM_healthcare_{embedding_model}.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "            \n",
    "            batches = [\n",
    "                batch_data[i : i + batch_size] for i in range(0, len(batch_data), batch_size)\n",
    "            ]\n",
    "            for batch in batches:\n",
    "                num_batches += 1\n",
    "\n",
    "                batch_metrics = {\n",
    "                    \"embed_time\": 0,\n",
    "                    \"svm_law_time\": 0,\n",
    "                    \"svm_finance_time\": 0,\n",
    "                    \"svm_health_time\": 0,\n",
    "                    \"xgb_law_time\": 0,\n",
    "                    \"xgb_finance_time\": 0,\n",
    "                    \"xgb_health_time\": 0\n",
    "                }\n",
    "\n",
    "                # Time embeddings\n",
    "                start_time = time.perf_counter()\n",
    "                if embedding_model == \"tf_idf\":\n",
    "                    embeds = tfidf_finance.transform(batch)\n",
    "                elif embedding_model == \"mini\":\n",
    "                    embeds = np.array(list(mini_embedding.embed(batch)))\n",
    "                else: # baai\n",
    "                    embeds = np.array(list(baai_embedding.embed(batch)))\n",
    "                batch_metrics['embed_time'] += time.perf_counter() - start_time\n",
    "\n",
    "                # Get all predictions and time them\n",
    "                start_time = time.perf_counter()\n",
    "                svm_law_preds = svm_law.predict(embeds)\n",
    "                batch_metrics['svm_law_time'] += time.perf_counter() - start_time\n",
    "\n",
    "                start_time = time.perf_counter()\n",
    "                svm_finance_preds = svm_finance.predict(embeds)\n",
    "                batch_metrics['svm_finance_time'] += time.perf_counter() - start_time\n",
    "\n",
    "                start_time = time.perf_counter() \n",
    "                svm_health_preds = svm_healthcare.predict(embeds)\n",
    "                batch_metrics['svm_health_time'] += time.perf_counter() - start_time\n",
    "                # Combine predictions - 0 only if all predict 0\n",
    "                svm_batch_preds = [1 if (l or f or h) else 0\n",
    "                                  for l,f,h in zip(svm_law_preds, svm_finance_preds, svm_health_preds)]\n",
    "               \n",
    "                # Record results for this batch size (averaged)\n",
    "                svm_batch_results.append({\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"time_taken_embed\": batch_metrics['embed_time'],\n",
    "                    \"time_taken_law\": batch_metrics['svm_law_time'],\n",
    "                    \"time_taken_finance\": batch_metrics['svm_finance_time'],\n",
    "                    \"time_taken_healthcare\": batch_metrics['svm_health_time'],\n",
    "                    \"results\": svm_batch_preds,\n",
    "                    \"model_name\": \"svm\",\n",
    "                    \"embedding_model\": embedding_model,\n",
    "                    \"embedding\": True\n",
    "                })\n",
    "\n",
    "        pd.DataFrame(svm_batch_results).to_csv(f\"reports/batch_svm_{embedding_model}.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {embedding_model}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tibor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer function for batch processing\n",
    "tokenizer_func = partial(\n",
    "    tokenizer, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load ONNX models\n",
    "    mlp_classifier = ort.InferenceSession(\n",
    "        \"models/text_classifier_optimized_int8.onnx\",\n",
    "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    )\n",
    "    print(\"Successfully loaded all ONNX models\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ONNX models: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_data = pd.read_csv(\"data/domain_eval.csv\")\n",
    "ood_data = pd.read_csv(\"data/ood_eval.csv\")\n",
    "\n",
    "data = {\"domain\": domain_data, \"ood\": ood_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, inference_df in data.items():\n",
    "    print(f\"\\nProcessing {domain} dataset...\")\n",
    "    predictions_mlp = []\n",
    "    prediction_times_mlp = []\n",
    "    actuals_mlp = []\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            # Tokenize input text\n",
    "            start_time = time.perf_counter_ns()\n",
    "            inputs = tokenizer_func(row[\"prompt\"])\n",
    "\n",
    "            # Convert to numpy arrays for ONNX\n",
    "            onnx_inputs = {\n",
    "                'input_ids': inputs['input_ids'].numpy(),\n",
    "                'attention_mask': inputs['attention_mask'].numpy(),\n",
    "            }\n",
    "\n",
    "            # Run inference\n",
    "            pred = mlp_classifier.run(None, onnx_inputs)[0]\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_mlp.append(end_time - start_time)\n",
    "            predictions_mlp.append(0 if np.argmax(pred) == 3 else 1)\n",
    "            \n",
    "\n",
    "        # Evaluate results\n",
    "        evaluate_run(\n",
    "            predictions=predictions_mlp,\n",
    "            true_labels=actuals_mlp,\n",
    "            latency=statistics.mean(prediction_times_mlp),\n",
    "            domain=domain,\n",
    "            embed_model=\"MiniLM_L12\",\n",
    "            model_name=\"MLP_ONNX\",\n",
    "            train_acc=0.0,\n",
    "            cost=0.0,\n",
    "            training=False,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {domain} dataset: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
