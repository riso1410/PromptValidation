{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Domain Generalization: Hate Speech Detection\n",
    "\n",
    "This notebook evaluates how well domain-specific classifiers generalize to out-of-domain hate speech detection.\n",
    "We test various model architectures trained on domain classification to see if they can effectively identify hate speech.\n",
    "\n",
    "Key aspects evaluated:\n",
    "- Few-shot transfer capabilities using DSPy\n",
    "- Model robustness across different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "This notebook evaluates several classification models:\n",
    "1. **LLM-based**: Using Qwen 2.5 for zero-shot classification\n",
    "2. **BERT-based**: ModernBERT with NLI approach\n",
    "3. **Traditional ML**: \n",
    "   - fastText for efficient text classification\n",
    "   - SVM and XGBoost with different embeddings\n",
    "\n",
    "Each model is evaluated on hate speech detection as an out-of-domain task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import dependencies and initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from prompt_classifier.metrics import evaluate_run\n",
    "from prompt_classifier.modeling.dspy_llm import LlmClassifier\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(22)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Datasets\n",
    "\n",
    "Load various hate speech datasets for evaluation:\n",
    "- Jigsaw Toxicity\n",
    "- OLID\n",
    "- HateXplain\n",
    "- TUKE Slovak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Jigsaw dataset\n",
    "jigsaw_splits = {\n",
    "    \"train\": \"train_dataset.csv\",\n",
    "    \"validation\": \"val_dataset.csv\",\n",
    "    \"test\": \"test_dataset.csv\",\n",
    "}\n",
    "jigsaw_df = pd.read_csv(\n",
    "    \"hf://datasets/Arsive/toxicity_classification_jigsaw/\" + jigsaw_splits[\"validation\"]\n",
    ")\n",
    "\n",
    "jigsaw_df = jigsaw_df[\n",
    "    (jigsaw_df[\"toxic\"] == 1)\n",
    "    | (jigsaw_df[\"severe_toxic\"] == 1)\n",
    "    | (jigsaw_df[\"obscene\"] == 1)\n",
    "    | (jigsaw_df[\"threat\"] == 1)\n",
    "    | (jigsaw_df[\"insult\"] == 1)\n",
    "    | (jigsaw_df[\"identity_hate\"] == 1)\n",
    "]\n",
    "\n",
    "jigsaw_df = jigsaw_df.rename(columns={\"comment_text\": \"prompt\"})\n",
    "jigsaw_df[\"label\"] = 0\n",
    "jigsaw_df = jigsaw_df[[\"prompt\", \"label\"]]\n",
    "jigsaw_df = jigsaw_df.dropna(subset=[\"prompt\"])\n",
    "jigsaw_df = jigsaw_df[jigsaw_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load OLID dataset\n",
    "olid_splits = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
    "olid_df = pd.read_csv(\"hf://datasets/christophsonntag/OLID/\" + olid_splits[\"train\"])\n",
    "olid_df = olid_df.rename(columns={\"cleaned_tweet\": \"prompt\"})\n",
    "olid_df[\"label\"] = 0\n",
    "olid_df = olid_df[[\"prompt\", \"label\"]]\n",
    "olid_df = olid_df.dropna(subset=[\"prompt\"])\n",
    "olid_df = olid_df[olid_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load hateXplain dataset\n",
    "hateXplain = pd.read_parquet(\n",
    "    \"hf://datasets/nirmalendu01/hateXplain_filtered/data/train-00000-of-00001.parquet\"\n",
    ")\n",
    "hateXplain = hateXplain.rename(columns={\"test_case\": \"prompt\"})\n",
    "hateXplain = hateXplain[(hateXplain[\"gold_label\"] == \"hateful\")]\n",
    "hateXplain = hateXplain[[\"prompt\", \"label\"]]\n",
    "hateXplain[\"label\"] = 0\n",
    "hateXplain = hateXplain.dropna(subset=[\"prompt\"])\n",
    "hateXplain = hateXplain[hateXplain[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load TUKE Slovak dataset\n",
    "tuke_sk_splits = {\"train\": \"train.json\", \"test\": \"test.json\"}\n",
    "tuke_sk_df = pd.read_json(\n",
    "    \"hf://datasets/TUKE-KEMT/hate_speech_slovak/\" + tuke_sk_splits[\"train\"], lines=True\n",
    ")\n",
    "tuke_sk_df = tuke_sk_df.rename(columns={\"text\": \"prompt\"})\n",
    "tuke_sk_df = tuke_sk_df[tuke_sk_df[\"label\"] == 0]\n",
    "tuke_sk_df = tuke_sk_df[[\"prompt\", \"label\"]]\n",
    "tuke_sk_df = tuke_sk_df.dropna(subset=[\"prompt\"])\n",
    "tuke_sk_df = tuke_sk_df[tuke_sk_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load DKK dataset\n",
    "dkk = pd.read_parquet(\"data/test-00000-of-00001.parquet\")\n",
    "dkk = dkk.rename(columns={\"text\": \"prompt\"})\n",
    "dkk = dkk[dkk[\"label\"] == \"OFF\"].reset_index(drop=True)\n",
    "dkk[\"label\"] = 0\n",
    "dkk = dkk.dropna(subset=[\"prompt\"])\n",
    "dkk = dkk[dkk[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "dkk_all = pd.read_parquet(\"data/test-00000-of-00001.parquet\")\n",
    "dkk_all = dkk_all.rename(columns={\"text\": \"prompt\"})\n",
    "dkk_all[\"label\"] = 0\n",
    "dkk_all = dkk_all.dropna(subset=[\"prompt\"])\n",
    "dkk_all = dkk_all[dkk_all[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "datasets = {\n",
    "    \"jigsaw\": jigsaw_df,\n",
    "    \"olid\": olid_df,\n",
    "    \"hate_xplain\": hateXplain,\n",
    "    \"tuke_sk\": tuke_sk_df,\n",
    "    \"dkk\": dkk,\n",
    "    \"dkk_all\": dkk_all,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(jigsaw_df.head())\n",
    "display(olid_df.head())\n",
    "display(hateXplain.head())\n",
    "display(tuke_sk_df.head())\n",
    "display(dkk.head())\n",
    "display(dkk_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Overview\n",
    "\n",
    "We use four major hate speech datasets:\n",
    "\n",
    "1. **Jigsaw Toxicity**\n",
    "   - Multi-label toxicity classification\n",
    "   - Includes toxic, severe_toxic, obscene, threat, insult, identity_hate labels\n",
    "\n",
    "2. **OLID (Offensive Language Identification Dataset)**\n",
    "   - Hierarchical labeling of offensive language\n",
    "   - Focuses on Twitter content\n",
    "\n",
    "3. **HateXplain**\n",
    "   - Annotated with rationales for hate speech\n",
    "   - Includes target community information\n",
    "\n",
    "4. **TUKE Slovak**\n",
    "   - Slovak language hate speech dataset\n",
    "   - Tests cross-lingual generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n",
    "Using multiple embedding approaches:\n",
    "- **BAAI BGE**: Optimized for semantic similarity\n",
    "- **MiniLM**: Efficient sentence transformers model\n",
    "- **TF-IDF**: Traditional bag-of-words approach\n",
    "\n",
    "These embeddings are used with SVM and XGBoost classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = os.getcwd()\n",
    "print(current_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_finance = pkl.load(open(\"models/tfidf_finance.pkl\", \"rb\"))\n",
    "tfidf_healthcare = pkl.load(open(\"models/tfidf_healthcare.pkl\", \"rb\"))\n",
    "tfidf_law = pkl.load(open(\"models/tfidf_law.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Process\n",
    "\n",
    "For each dataset and model combination:\n",
    "1. Load pre-trained domain classifiers\n",
    "2. Process test samples through each domain classifier\n",
    "3. Combine predictions using OR logic (any domain=1 -> toxic=0)\n",
    "4. Calculate metrics:\n",
    "   - Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ollama_chat/qwen2.5:14b\"\n",
    "\n",
    "for domain, inference_df in datasets.items():\n",
    "    try:\n",
    "        llm_classifier_finance = LlmClassifier(\n",
    "            api_key=\"\",\n",
    "            api_base=\"http://localhost:11434\",\n",
    "            model_name=model_name,\n",
    "            domain=\"finance\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        llm_classifier_healthcare = LlmClassifier(\n",
    "            api_key=\"\",\n",
    "            api_base=\"http://localhost:11434\",\n",
    "            model_name=model_name,\n",
    "            domain=\"healthcare\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        llm_classifier_law = LlmClassifier(\n",
    "            api_key=\"\",\n",
    "            api_base=\"http://localhost:11434\",\n",
    "            model_name=model_name,\n",
    "            domain=\"law\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        # Load models\n",
    "        llm_classifier_finance.load_model(\"models/qwen2.5:14b-finance.json\")\n",
    "        llm_classifier_healthcare.load_model(\"models/qwen2.5:14b-healthcare.json\")\n",
    "        llm_classifier_law.load_model(\"models/qwen2.5:14b-law.json\")\n",
    "\n",
    "        predictions_llm = []\n",
    "        prediction_times_llm = []\n",
    "        actuals_llm = []\n",
    "\n",
    "        # Get predictions for each prompt\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "\n",
    "            # Get predictions from all models\n",
    "            pred_finance = llm_classifier_finance.predict_single(row[\"prompt\"])\n",
    "            pred_healthcare = llm_classifier_healthcare.predict_single(row[\"prompt\"])\n",
    "            pred_law = llm_classifier_law.predict_single(row[\"prompt\"])\n",
    "\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_llm.append(end_time - start_time)\n",
    "\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_llm.append(\n",
    "                0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1\n",
    "            )\n",
    "            actuals_llm.append(row[\"label\"])\n",
    "\n",
    "        evaluate_run(\n",
    "            predictions=predictions_llm,\n",
    "            true_labels=actuals_llm,\n",
    "            latency=statistics.mean(prediction_times_llm),\n",
    "            domain=domain,\n",
    "            embed_model=\"qwen2.5\",\n",
    "            model_name=model_name,\n",
    "            train_acc=0.0,\n",
    "            cost=0.0,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error running LLM model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier_finance = ModernBERTNLI(domain=\"finance\")\n",
    "bert_classifier_healthcare = ModernBERTNLI(domain=\"healthcare\")\n",
    "bert_classifier_law = ModernBERTNLI(domain=\"law\")\n",
    "\n",
    "try:\n",
    "    # Move models to GPU\n",
    "    bert_classifier_finance.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_healthcare.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_law.classifier.model.to(\"cuda\")\n",
    "\n",
    "    for domain, inference_df in datasets.items():\n",
    "        predictions_bert = []\n",
    "        prediction_times_bert = []\n",
    "        actuals_bert = []\n",
    "        # Get predictions for each prompt\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "\n",
    "            # Get predictions from all models\n",
    "            pred_finance = bert_classifier_finance.predict(row[\"prompt\"])\n",
    "            pred_healthcare = bert_classifier_healthcare.predict(row[\"prompt\"])\n",
    "            pred_law = bert_classifier_law.predict(row[\"prompt\"])\n",
    "\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_bert.append(end_time - start_time)\n",
    "\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_bert.append(\n",
    "                0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1\n",
    "            )\n",
    "            actuals_bert.append(row[\"label\"])\n",
    "\n",
    "    evaluate_run(\n",
    "        predictions=predictions_bert,\n",
    "        true_labels=actuals_bert,\n",
    "        latency=statistics.mean(prediction_times_bert),\n",
    "        domain=domain,\n",
    "        embed_model=\"BERT\",\n",
    "        model_name=\"ModernBERT\",\n",
    "        train_acc=0.0,\n",
    "        cost=0.0,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error running ModernBERT models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastText\n",
    "for domain, inference_df in datasets.items():\n",
    "    actuals_ft = []\n",
    "    predictions_ft = []\n",
    "    prediction_times_ft = []\n",
    "    print(f\"Processing dataset {domain}...\")\n",
    "    try:\n",
    "        # Try to load model with proper error handling\n",
    "        try:\n",
    "            fasttext_classifier_finance = FastTextClassifier(\n",
    "                train_data=inference_df, test_data=inference_df\n",
    "            )\n",
    "            fasttext_classifier_finance.model = fasttext.load_model(\n",
    "                \"models/fastText_finance_fasttext.bin\"\n",
    "            )\n",
    "\n",
    "            fasttext_classifier_healthcare = FastTextClassifier(\n",
    "                train_data=inference_df, test_data=inference_df\n",
    "            )\n",
    "            fasttext_classifier_healthcare.model = fasttext.load_model(\n",
    "                \"models/fastText_healthcare_fasttext.bin\"\n",
    "            )\n",
    "\n",
    "            fasttext_classifier_law = FastTextClassifier(\n",
    "                train_data=inference_df, test_data=inference_df\n",
    "            )\n",
    "            fasttext_classifier_law.model = fasttext.load_model(\n",
    "                \"models/fastText_law_fasttext.bin\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fastText models: {e}\")\n",
    "            continue\n",
    "\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            text = str(row[\"prompt\"])\n",
    "            query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "            try:\n",
    "                start_time = time.perf_counter_ns()\n",
    "\n",
    "                # Predictions from all three classifiers\n",
    "                prediction_finance = fasttext_classifier_finance.model.predict(query)\n",
    "                prediction_healthcare = fasttext_classifier_healthcare.model.predict(\n",
    "                    query\n",
    "                )\n",
    "                prediction_law = fasttext_classifier_law.model.predict(query)\n",
    "\n",
    "                end_time = time.perf_counter_ns()\n",
    "                prediction_times_ft.append(end_time - start_time)\n",
    "\n",
    "                predictions_ft.append(\n",
    "                    0\n",
    "                    if (\n",
    "                        prediction_finance[0][0] == \"__label__1\"\n",
    "                        or prediction_healthcare[0][0] == \"__label__1\"\n",
    "                        or prediction_law[0][0] == \"__label__1\"\n",
    "                    )\n",
    "                    else 1\n",
    "                )\n",
    "                actuals_ft.append(row[\"label\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "        evaluate_run(\n",
    "            predictions=predictions_ft,\n",
    "            true_labels=actuals_ft,\n",
    "            latency=statistics.mean(prediction_times_ft),\n",
    "            domain=domain,\n",
    "            embed_model=\"fastText\",\n",
    "            model_name=\"fastText\",\n",
    "            train_acc=0.0,\n",
    "            cost=0.0,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset {domain}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - SVM, XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_names = [\"mini\", \"baai\", \"tf_idf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding test data\n",
    "for embedding_model in embedding_models_names:\n",
    "    for domain, inference_df in datasets.items():\n",
    "        # Get actual labels once\n",
    "        actuals_ml = inference_df[\"label\"].tolist()\n",
    "\n",
    "        # Get embeddings based on model type\n",
    "        if embedding_model == \"tf_idf\":\n",
    "            test_embeds = tfidf_finance.transform(inference_df[\"prompt\"])\n",
    "        else:\n",
    "            start_time = time.perf_counter_ns()\n",
    "            if embedding_model == \"mini\":\n",
    "                test_embeds = np.array(\n",
    "                    list(mini_embedding.embed(inference_df[\"prompt\"]))\n",
    "                )\n",
    "            else:  # baai\n",
    "                test_embeds = np.array(\n",
    "                    list(baai_embedding.embed(inference_df[\"prompt\"]))\n",
    "                )\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_times = end_time - start_time\n",
    "            mean_embed_time = embed_times / len(inference_df)\n",
    "\n",
    "        # Load models\n",
    "        try:\n",
    "            # Load SVM models\n",
    "            with open(f\"models/SVM_finance_{embedding_model}.pkl\", \"rb\") as f:\n",
    "                svm_classifier_finance = pkl.load(f)\n",
    "            with open(f\"models/SVM_healthcare_{embedding_model}.pkl\", \"rb\") as f:\n",
    "                svm_classifier_healthcare = pkl.load(f)\n",
    "            with open(f\"models/SVM_law_{embedding_model}.pkl\", \"rb\") as f:\n",
    "                svm_classifier_law = pkl.load(f)\n",
    "\n",
    "            # Load XGBoost models\n",
    "            xgb_classifier_finance = XGBClassifier()\n",
    "            xgb_classifier_healthcare = XGBClassifier()\n",
    "            xgb_classifier_law = XGBClassifier()\n",
    "\n",
    "            xgb_classifier_finance.load_model(\n",
    "                f\"models/XGBoost_finance_{embedding_model}.json\"\n",
    "            )\n",
    "            xgb_classifier_healthcare.load_model(\n",
    "                f\"models/XGBoost_healthcare_{embedding_model}.json\"\n",
    "            )\n",
    "            xgb_classifier_law.load_model(f\"models/XGBoost_law_{embedding_model}.json\")\n",
    "\n",
    "            predictions_xgb = []\n",
    "            predictions_svm = []\n",
    "            prediction_times_xgb = []\n",
    "            prediction_times_svm = []\n",
    "\n",
    "            # Make predictions\n",
    "            for test_embed in test_embeds:\n",
    "                test_embed = test_embed.reshape(1, -1)\n",
    "\n",
    "                # SVM predictions\n",
    "                start_time = time.perf_counter_ns()\n",
    "                pred_finance = svm_classifier_finance.predict(test_embed)\n",
    "                pred_healthcare = svm_classifier_healthcare.predict(test_embed)\n",
    "                pred_law = svm_classifier_law.predict(test_embed)\n",
    "                end_time = time.perf_counter_ns()\n",
    "\n",
    "                prediction_times_svm.append(end_time - start_time)\n",
    "                predictions_svm.append(\n",
    "                    0\n",
    "                    if (\n",
    "                        pred_finance[0] == 1\n",
    "                        or pred_healthcare[0] == 1\n",
    "                        or pred_law[0] == 1\n",
    "                    )\n",
    "                    else 1\n",
    "                )\n",
    "\n",
    "                # XGBoost predictions\n",
    "                start_time = time.perf_counter_ns()\n",
    "                pred_finance = xgb_classifier_finance.predict(test_embed)\n",
    "                pred_healthcare = xgb_classifier_healthcare.predict(test_embed)\n",
    "                pred_law = xgb_classifier_law.predict(test_embed)\n",
    "                end_time = time.perf_counter_ns()\n",
    "\n",
    "                prediction_times_xgb.append(end_time - start_time)\n",
    "                predictions_xgb.append(\n",
    "                    0\n",
    "                    if (\n",
    "                        pred_finance[0] == 1\n",
    "                        or pred_healthcare[0] == 1\n",
    "                        or pred_law[0] == 1\n",
    "                    )\n",
    "                    else 1\n",
    "                )\n",
    "\n",
    "            # Evaluate results\n",
    "            evaluate_run(\n",
    "                predictions=predictions_svm,\n",
    "                true_labels=actuals_ml,\n",
    "                latency=statistics.mean(prediction_times_svm),\n",
    "                domain=domain,\n",
    "                embed_model=embedding_model,\n",
    "                model_name=\"SVM\",\n",
    "                train_acc=0.0,\n",
    "                cost=0.0,\n",
    "            )\n",
    "            evaluate_run(\n",
    "                predictions=predictions_xgb,\n",
    "                true_labels=actuals_ml,\n",
    "                latency=statistics.mean(prediction_times_xgb),\n",
    "                domain=domain,\n",
    "                embed_model=embedding_model,\n",
    "                model_name=\"XGBoost\",\n",
    "                train_acc=0.0,\n",
    "                cost=0.0,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error processing {domain} dataset with {embedding_model} embeddings: {e}\"\n",
    "            )\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "tokenizer_func = partial(\n",
    "    tokenizer, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load ONNX models\n",
    "    mlp_classifier = ort.InferenceSession(\n",
    "        \"models/text_classifier_optimized_int8.onnx\",\n",
    "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    )\n",
    "    print(\"Successfully loaded all ONNX models\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ONNX models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, inference_df in datasets.items():\n",
    "    print(f\"\\nProcessing {domain} dataset...\")\n",
    "    predictions_mlp = []\n",
    "    prediction_times_mlp = []\n",
    "    actuals_mlp = []\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            # Tokenize input text\n",
    "            start_time = time.perf_counter_ns()\n",
    "            inputs = tokenizer_func(row[\"prompt\"])\n",
    "\n",
    "            # Convert to numpy arrays for ONNX\n",
    "            onnx_inputs = {\n",
    "                'input_ids': inputs['input_ids'].numpy(),\n",
    "                'attention_mask': inputs['attention_mask'].numpy(),\n",
    "            }\n",
    "\n",
    "            # Run inference\n",
    "            pred = mlp_classifier.run(None, onnx_inputs)[0]\n",
    "            end_time = time.perf_counter_ns()\n",
    "\n",
    "            prediction_times_mlp.append(end_time - start_time)\n",
    "            predictions_mlp.append(0 if np.argmax(pred) == 1 else 1)\n",
    "            actuals_mlp.append(row[\"label\"])\n",
    "\n",
    "        # Evaluate results\n",
    "        evaluate_run(\n",
    "            predictions=predictions_mlp,\n",
    "            true_labels=actuals_mlp,\n",
    "            latency=statistics.mean(prediction_times_mlp),\n",
    "            domain=domain,\n",
    "            embed_model=\"MiniLM-L12\",\n",
    "            model_name=\"MLP-ONNX\",\n",
    "            train_acc=0.0,\n",
    "            cost=0.0,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {domain} dataset: {e}\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-validation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
