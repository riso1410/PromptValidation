{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import time\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from prompt_classifier.modeling.dspy_llm import LlmClassifier\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "from prompt_classifier.metrics import evaluate\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Jigsaw dataset\n",
    "jigsaw_splits = {'train': 'train_dataset.csv', 'validation': 'val_dataset.csv', 'test': 'test_dataset.csv'}\n",
    "jigsaw_df = pd.read_csv(\"hf://datasets/Arsive/toxicity_classification_jigsaw/\" + jigsaw_splits[\"validation\"])\n",
    "\n",
    "jigsaw_df = jigsaw_df[(jigsaw_df[\"toxic\"] == 1) |\n",
    "                            (jigsaw_df[\"severe_toxic\"] == 1) |\n",
    "                            (jigsaw_df[\"obscene\"] == 1) |\n",
    "                            (jigsaw_df[\"threat\"] == 1) |\n",
    "                            (jigsaw_df[\"insult\"] == 1) |\n",
    "                            (jigsaw_df[\"identity_hate\"] == 1)]\n",
    "\n",
    "jigsaw_df = jigsaw_df.rename(columns={\"comment_text\": \"messages\"})\n",
    "jigsaw_df[\"label\"] = 0\n",
    "jigsaw_df = jigsaw_df.dropna(subset=[\"messages\"])\n",
    "jigsaw_df = jigsaw_df[[\"messages\", \"label\"]]\n",
    "\n",
    "# Load OLID dataset\n",
    "olid_splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "olid_df = pd.read_csv(\"hf://datasets/christophsonntag/OLID/\" + olid_splits[\"train\"])\n",
    "\n",
    "olid_df = olid_df.rename(columns={\"cleaned_tweet\": \"messages\"})\n",
    "olid_df[\"label\"] = 0\n",
    "olid_df = olid_df.dropna(subset=[\"messages\"])\n",
    "olid_df = olid_df[[\"messages\", \"label\"]]\n",
    "\n",
    "# Load hateXplain dataset\n",
    "hateXplain = pd.read_parquet(\"hf://datasets/nirmalendu01/hateXplain_filtered/data/train-00000-of-00001.parquet\")\n",
    "hateXplain = hateXplain.rename(columns={\"test_case\": \"messages\"})\n",
    "hateXplain = hateXplain[(hateXplain[\"gold_label\"] == \"hateful\")]\n",
    "hateXplain = hateXplain.rename(columns={\"gold_label\": \"label\"})\n",
    "hateXplain = hateXplain[[\"messages\", \"label\"]]\n",
    "hateXplain = hateXplain.dropna(subset=[\"messages\"])\n",
    "\n",
    "# Load TUKE Slovak dataset\n",
    "tuke_sk_splits = {'train': 'train.json', 'test': 'test.json'}\n",
    "tuke_sk_df = pd.read_json(\"hf://datasets/TUKE-KEMT/hate_speech_slovak/\" + tuke_sk_splits[\"train\"], lines=True)\n",
    "tuke_sk_df = tuke_sk_df.rename(columns={\"text\": \"messages\"})\n",
    "tuke_sk_df = tuke_sk_df[tuke_sk_df[\"label\"] == 0]\n",
    "tuke_sk_df = tuke_sk_df[[\"messages\", \"label\"]]\n",
    "tuke_sk_df = tuke_sk_df.dropna(subset=[\"messages\"])\n",
    "\n",
    "datasets = {\n",
    "    'jigsaw': jigsaw_df,\n",
    "    'olid': olid_df,\n",
    "    'hate_xplain': hateXplain,\n",
    "    'tuke_sk': tuke_sk_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-04-03 22:52:04.037329509 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-03 22:52:04.037387066 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-04-03 22:52:04.793673176 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-03 22:52:04.793713046 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_finance = pkl.load(open(\"./models/tfidf_finance.pkl\", \"rb\"))\n",
    "tfidf_healthcare = pkl.load(open(\"./models/tfidf_healthcare.pkl\", \"rb\"))\n",
    "tfidf_law = pkl.load(open(\"./models/tfidf_law.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Classifier -> zmenit na qwen2.5\n",
    "model_name = \"Qwen2.5:7b\"\n",
    "\n",
    "for domain, inference_df in datasets:\n",
    "    try:\n",
    "        llm_classifier_finance = LlmClassifier(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            api_base=os.getenv(\"PROXY_URL\"),\n",
    "            model_name=model_name,\n",
    "            domain=\"finance\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        llm_classifier_healthcare = LlmClassifier(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            api_base=os.getenv(\"PROXY_URL\"),\n",
    "            model_name=model_name,\n",
    "            domain=\"healthcare\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        llm_classifier_law = LlmClassifier(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            api_base=os.getenv(\"PROXY_URL\"),\n",
    "            model_name=model_name,\n",
    "            domain=\"law\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        # Load models\n",
    "        llm_classifier_finance.load_model(\"models/gpt-4o-mini-finance.json\")\n",
    "        llm_classifier_healthcare.load_model(\"models/gpt-4o-mini-healthcare.json\")\n",
    "        llm_classifier_law.load_model(\"models/gpt-4o-mini-law.json\")\n",
    "\n",
    "        predictions_llm = []\n",
    "        prediction_times_llm = []\n",
    "        actuals_llm = []\n",
    "\n",
    "        # Get predictions for each prompt\n",
    "        for _, row in tqdm(inference_df.head(1000).iterrows(), total=1000):\n",
    "            start_time = time.perf_counter_ns()\n",
    "\n",
    "            # Get predictions from all models\n",
    "            pred_finance = llm_classifier_finance.predict_single(row[\"prompt\"])\n",
    "            pred_healthcare = llm_classifier_healthcare.predict_single(row[\"prompt\"])\n",
    "            pred_law = llm_classifier_law.predict_single(row[\"prompt\"])\n",
    "\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_llm.append(end_time - start_time)\n",
    "\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_llm.append(0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1)\n",
    "            actuals_llm.append(row[\"label\"])\n",
    "        \n",
    "        evaluate(predictions=predictions_llm, true_labels=actuals_llm, latency=prediction_times_llm, domain=domain, embed_model=\"ada-002\", model_name=model_name, train_acc=0.0, cost=0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running GPT model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier_finance = ModernBERTNLI(domain=\"finance\")\n",
    "bert_classifier_healthcare = ModernBERTNLI(domain=\"healthcare\")\n",
    "bert_classifier_law = ModernBERTNLI(domain=\"law\")\n",
    "\n",
    "predictions_bert = []\n",
    "prediction_times_bert = []\n",
    "actuals_bert = []\n",
    "\n",
    "try:\n",
    "    # Move models to GPU\n",
    "    bert_classifier_finance.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_healthcare.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_law.classifier.model.to(\"cuda\")\n",
    "\n",
    "    for inference_df in datasets:\n",
    "        # Get predictions for each prompt\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "\n",
    "            # Get predictions from all models\n",
    "            pred_finance = bert_classifier_finance.predict(row[\"prompt\"])\n",
    "            pred_healthcare = bert_classifier_healthcare.predict(row[\"prompt\"])\n",
    "            pred_law = bert_classifier_law.predict(row[\"prompt\"])\n",
    "\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_bert.append(end_time - start_time)\n",
    "\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_bert.append(0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1)\n",
    "            actuals_bert.append(row[\"label\"])\n",
    "\n",
    "    evaluate(predictions=predictions_bert, true_labels=actuals_bert, latency=prediction_times_bert, domain=domain, embed_model=\"BERT\", model_name=\"ModernBERT\", train_acc=0.0, cost=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"Error running ModernBERT models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_ft = []\n",
    "predictions_ft = []\n",
    "prediction_times_ft = []\n",
    "\n",
    "# fastText\n",
    "for inference_df in datasets:\n",
    "    fasttext_classifier_finance = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "    fasttext_classifier_finance.model = fasttext.load_model(\"models/fastText_finance_fasttext.bin\")\n",
    "\n",
    "    fasttext_classifier_healthcare = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "    fasttext_classifier_healthcare.model = fasttext.load_model(\"models/fastText_healthcare_fasttext.bin\")\n",
    "\n",
    "    fasttext_classifier_law = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "    fasttext_classifier_law.model = fasttext.load_model(\"models/fastText_law_fasttext.bin\")\n",
    "\n",
    "    for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "        text = str(row[\"prompt\"])\n",
    "        query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "        start_time = time.perf_counter_ns()\n",
    "\n",
    "        # Predictions from all three classifiers\n",
    "        prediction_finance = fasttext_classifier_finance.model.predict(query)\n",
    "        prediction_healthcare = fasttext_classifier_healthcare.model.predict(query)\n",
    "        prediction_law = fasttext_classifier_law.model.predict(query)\n",
    "\n",
    "        end_time = time.perf_counter_ns()\n",
    "        prediction_times_ft.append(end_time - start_time)\n",
    "\n",
    "        predictions_ft.append(0 if (prediction_finance[0][0] == \"__label__1\" or prediction_healthcare[0][0] == \"__label__1\" or prediction_law[0][0] == \"__label__1\") else 1)\n",
    "        actuals_ft.append(row[\"label\"])\n",
    "\n",
    "        evaluate(predictions=predictions_ft, true_labels=actuals_ft, latency=prediction_times_ft, domain=domain, embed_model=\"fastText\", model_name=\"fastText\", train_acc=0.0, cost=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding test data\n",
    "for inference_df in datasets:\n",
    "    start_time = time.perf_counter_ns()\n",
    "    test_embeds = np.array(list(baai_embedding.embed(inference_df[\"prompt\"])))\n",
    "    end_time = time.perf_counter_ns()\n",
    "    embed_times = end_time - start_time\n",
    "\n",
    "    mean_embed_time = embed_times / len(inference_df)\n",
    "\n",
    "    with open(\"models/SVM_finance_baai.pkl\", \"rb\") as svm_file:\n",
    "        svm_classifier_finance = pkl.load(svm_file)\n",
    "\n",
    "    with open(\"models/SVM_healthcare_baai.pkl\", \"rb\") as svm_file:\n",
    "        svm_classifier_healthcare = pkl.load(svm_file)\n",
    "\n",
    "    with open(\"models/SVM_law_baai.pkl\", \"rb\") as svm_file:\n",
    "        svm_classifier_law = pkl.load(svm_file)\n",
    "\n",
    "    xgb_classifier_finance = XGBClassifier()\n",
    "    xgb_classifier_healthcare = XGBClassifier()\n",
    "    xgb_classifier_law = XGBClassifier()\n",
    "\n",
    "    xgb_classifier_finance.load_model(\"models/XGBoost_finance_baai.json\")\n",
    "    xgb_classifier_healthcare.load_model(\"models/XGBoost_healthcare_baai.json\")\n",
    "    xgb_classifier_law.load_model(\"models/XGBoost_law_baai.json\")\n",
    "\n",
    "    predictions_xgb = []\n",
    "    predictions_svm = []\n",
    "\n",
    "    prediction_times_xgb = []\n",
    "    prediction_times_svm = []\n",
    "\n",
    "    actuals_ml = []\n",
    "\n",
    "    for test_embed in test_embeds:\n",
    "        # SVM predictions\n",
    "        start_time = time.perf_counter_ns()\n",
    "        pred_finance = svm_classifier_finance.predict(test_embed.reshape(1, -1))\n",
    "        pred_healthcare = svm_classifier_healthcare.predict(test_embed.reshape(1, -1))\n",
    "        pred_law = svm_classifier_law.predict(test_embed.reshape(1, -1))\n",
    "        end_time = time.perf_counter_ns()\n",
    "\n",
    "        prediction_times_svm.append(end_time - start_time)\n",
    "        # If any model predicts 1, final prediction is 0\n",
    "        predictions_svm.append(0 if (pred_finance[0] == 1 or pred_healthcare[0] == 1 or pred_law[0] == 1) else 1)\n",
    "\n",
    "        # XGBoost predictions\n",
    "        start_time = time.perf_counter_ns()\n",
    "        pred_finance = xgb_classifier_finance.predict(test_embed.reshape(1, -1))\n",
    "        pred_healthcare = xgb_classifier_healthcare.predict(test_embed.reshape(1, -1))\n",
    "        pred_law = xgb_classifier_law.predict(test_embed.reshape(1, -1))\n",
    "        end_time = time.perf_counter_ns()\n",
    "\n",
    "        prediction_times_xgb.append(end_time - start_time)\n",
    "        # If any model predicts 1, final prediction is 0\n",
    "        predictions_xgb.append(0 if (pred_finance[0] == 1 or pred_healthcare[0] == 1 or pred_law[0] == 1) else 1)\n",
    "        actuals_ml.append(row[\"label\"])\n",
    "\n",
    "    \n",
    "    evaluate(predictions=predictions_svm, true_labels=actuals_ml, latency=prediction_times_svm, domain=domain, embed_model=\"\", model_name=\"fastText\", train_acc=0.0, cost=0.0)\n",
    "    evaluate(predictions=predictions_xgb, true_labels=actuals_ml, latency=prediction_times_xgb, domain=domain, embed_model=\"\", model_name=\"XGBoost\", train_acc=0.0, cost=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
