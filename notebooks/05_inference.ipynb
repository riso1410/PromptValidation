{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import time\n",
    "\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from prompt_classifier.modeling.dspy_llm import LlmClassifier\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'train_dataset.csv', 'validation': 'val_dataset.csv', 'test': 'test_dataset.csv'}\n",
    "inference_df = pd.read_csv(\"hf://datasets/Arsive/toxicity_classification_jigsaw/\" + splits[\"validation\"])\n",
    "inference_df = inference_df[(inference_df[\"toxic\"] == 1) | \n",
    "                            (inference_df[\"severe_toxic\"] == 1) | \n",
    "                            (inference_df[\"obscene\"] == 1) | \n",
    "                            (inference_df[\"threat\"] == 1) | \n",
    "                            (inference_df[\"insult\"] == 1) | \n",
    "                            (inference_df[\"identity_hate\"] == 1)]\n",
    "inference_df = inference_df.rename(columns={\"comment_text\": \"prompt\"})\n",
    "inference_df[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-03-28 20:31:13.813905722 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-03-28 20:31:13.813942844 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Classifier\n",
    "llm_classifier = LlmClassifier(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    proxy_url=os.getenv(\"PROXY_URL\"),\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    domain=\"finance\",\n",
    "    train_data=inference_df,\n",
    "    test_data=inference_df,\n",
    ")\n",
    "\n",
    "try:\n",
    "    llm_classifier.load_model(\"models/gpt-4o-mini-finance.json\")\n",
    "\n",
    "    test_predictions, test_actuals, test_latency = llm_classifier.predict()\n",
    "    print(f\"Test Accuracy: {metrics.accuracy_score(test_actuals, test_predictions)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error running GPT model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error running ModernBERT model: Failed to import transformers.models.modernbert.modeling_modernbert because of the following error (look up to see its traceback):\n",
      "partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # ModernBERT Classifier\n",
    "    bert_classifier = ModernBERTNLI(domain=\"finance\")\n",
    "    bert_classifier.classifier.model.to(\"cuda\")\n",
    "\n",
    "    # Test predictions\n",
    "    test_predictions = []\n",
    "    test_times = []\n",
    "    for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "        start_time = time.perf_counter_ns()\n",
    "        pred = bert_classifier.predict(row[\"prompt\"])\n",
    "        test_predictions.append(pred)\n",
    "        test_times.append(time.perf_counter_ns() - start_time)\n",
    "\n",
    "    test_acc = metrics.accuracy_score(inference_df[\"label\"], test_predictions)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error running ModernBERT model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 3214/3214 [00:00<00:00, 6290.89it/s]\n"
     ]
    }
   ],
   "source": [
    "actuals_ft = []\n",
    "predictions_ft = []\n",
    "prediction_times_ft = []\n",
    "\n",
    "# fastText\n",
    "fasttext_classifier = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "fasttext_classifier.model = fasttext.load_model(\"models/fastText_finance_fasttext.bin\")\n",
    "\n",
    "for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "    text = str(row[\"prompt\"])\n",
    "    query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "    start_time = time.perf_counter_ns()\n",
    "    prediction = fasttext_classifier.model.predict(query)\n",
    "    end_time = time.perf_counter_ns()\n",
    "\n",
    "    prediction_times_ft.append(end_time - start_time)\n",
    "\n",
    "    if prediction[0][0] == \"__label__1\":\n",
    "        predictions_ft.append(1)\n",
    "    else:\n",
    "        predictions_ft.append(0)\n",
    "\n",
    "    actuals_ft.append(row[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedding test data\n",
    "start_time = time.perf_counter_ns()\n",
    "test_embeds = np.array(list(baai_embedding.embed(inference_df[\"prompt\"])))\n",
    "end_time = time.perf_counter_ns()\n",
    "embed_times = end_time - start_time\n",
    "\n",
    "mean_embed_time = embed_times / len(inference_df)\n",
    "\n",
    "with open(\"models/SVM_finance_baai.pkl\", \"rb\") as svm_file:\n",
    "    svm_classifier = pkl.load(svm_file)\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.load_model(\"models/XGBoost_finance_baai.json\")\n",
    "\n",
    "predictions_xgb = []\n",
    "predictions_svm = []\n",
    "\n",
    "prediction_times_xgb = []\n",
    "prediction_times_svm = []\n",
    "\n",
    "for _, test_embed in enumerate(test_embeds):\n",
    "    start_time = time.perf_counter_ns()\n",
    "    prediction = svm_classifier.predict(test_embed.reshape(1, -1))\n",
    "    end_time = time.perf_counter_ns()\n",
    "\n",
    "    prediction_times_svm.append(end_time - start_time)\n",
    "    predictions_svm.append(prediction[0])\n",
    "\n",
    "\n",
    "for _, test_embed in enumerate(test_embeds):\n",
    "    start_time = time.perf_counter_ns()\n",
    "    prediction = xgb_classifier.predict(test_embed.reshape(1, -1))\n",
    "    end_time = time.perf_counter_ns()\n",
    "\n",
    "    prediction_times_xgb.append(end_time - start_time)\n",
    "    predictions_xgb.append(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_ms = {\n",
    "    'fastText': np.array(prediction_times_ft) / 1_000_000,\n",
    "    'XGBoost': np.array(prediction_times_xgb) / 1_000_000,\n",
    "    'SVM': np.array(prediction_times_svm) / 1_000_000\n",
    "}\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'Model': [k for k,v in times_ms.items() for _ in v],\n",
    "    'Latency (ms)': [x for v in times_ms.values() for x in v]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastText Accuracy: 0.7078406969508401\n",
      "SVM Accuracy: 0.9499066583696328\n",
      "XGBoost Accuracy: 0.924082140634723\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy for each model\n",
    "accuracy_ft = metrics.accuracy_score(actuals_ft, predictions_ft)\n",
    "accuracy_svm = metrics.accuracy_score(actuals_ft, predictions_svm)\n",
    "accuracy_xgb = metrics.accuracy_score(actuals_ft, predictions_xgb)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"fastText Accuracy: {accuracy_ft}\")\n",
    "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
