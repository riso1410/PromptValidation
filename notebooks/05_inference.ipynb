{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Domain Generalization: Hate Speech Detection\n",
    "\n",
    "This notebook evaluates how well domain-specific classifiers generalize to out-of-domain hate speech detection.\n",
    "We test various model architectures trained on domain classification to see if they can effectively identify hate speech.\n",
    "\n",
    "Key aspects evaluated:\n",
    "- Few-shot transfer capabilities using DSPy\n",
    "- Model robustness across different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "This notebook evaluates several classification models:\n",
    "1. **LLM-based**: Using Qwen 2.5 for zero-shot classification\n",
    "2. **BERT-based**: ModernBERT with NLI approach\n",
    "3. **Traditional ML**: \n",
    "   - fastText for efficient text classification\n",
    "   - SVM and XGBoost with different embeddings\n",
    "\n",
    "Each model is evaluated on hate speech detection as an out-of-domain task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import dependencies and initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import time\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from prompt_classifier.modeling.dspy_llm import LlmClassifier\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "from prompt_classifier.metrics import evaluate\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Datasets\n",
    "\n",
    "Load various hate speech datasets for evaluation:\n",
    "- Jigsaw Toxicity\n",
    "- OLID\n",
    "- HateXplain\n",
    "- TUKE Slovak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Jigsaw dataset\n",
    "jigsaw_splits = {'train': 'train_dataset.csv', 'validation': 'val_dataset.csv', 'test': 'test_dataset.csv'}\n",
    "jigsaw_df = pd.read_csv(\"hf://datasets/Arsive/toxicity_classification_jigsaw/\" + jigsaw_splits[\"validation\"])\n",
    "\n",
    "jigsaw_df = jigsaw_df[(jigsaw_df[\"toxic\"] == 1) |\n",
    "                            (jigsaw_df[\"severe_toxic\"] == 1) |\n",
    "                            (jigsaw_df[\"obscene\"] == 1) |\n",
    "                            (jigsaw_df[\"threat\"] == 1) |\n",
    "                            (jigsaw_df[\"insult\"] == 1) |\n",
    "                            (jigsaw_df[\"identity_hate\"] == 1)]\n",
    "\n",
    "jigsaw_df = jigsaw_df.rename(columns={\"comment_text\": \"messages\"})\n",
    "jigsaw_df[\"label\"] = 0\n",
    "jigsaw_df = jigsaw_df.dropna(subset=[\"messages\"])\n",
    "jigsaw_df = jigsaw_df[[\"messages\", \"label\"]]\n",
    "\n",
    "# Load OLID dataset\n",
    "olid_splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "olid_df = pd.read_csv(\"hf://datasets/christophsonntag/OLID/\" + olid_splits[\"train\"])\n",
    "\n",
    "olid_df = olid_df.rename(columns={\"cleaned_tweet\": \"messages\"})\n",
    "olid_df[\"label\"] = 0\n",
    "olid_df = olid_df.dropna(subset=[\"messages\"])\n",
    "olid_df = olid_df[[\"messages\", \"label\"]]\n",
    "\n",
    "# Load hateXplain dataset\n",
    "hateXplain = pd.read_parquet(\"hf://datasets/nirmalendu01/hateXplain_filtered/data/train-00000-of-00001.parquet\")\n",
    "hateXplain = hateXplain.rename(columns={\"test_case\": \"messages\"})\n",
    "hateXplain = hateXplain[(hateXplain[\"gold_label\"] == \"hateful\")]\n",
    "hateXplain = hateXplain.rename(columns={\"gold_label\": \"label\"})\n",
    "hateXplain = hateXplain[[\"messages\", \"label\"]]\n",
    "hateXplain = hateXplain.dropna(subset=[\"messages\"])\n",
    "\n",
    "# Load TUKE Slovak dataset\n",
    "tuke_sk_splits = {'train': 'train.json', 'test': 'test.json'}\n",
    "tuke_sk_df = pd.read_json(\"hf://datasets/TUKE-KEMT/hate_speech_slovak/\" + tuke_sk_splits[\"train\"], lines=True)\n",
    "tuke_sk_df = tuke_sk_df.rename(columns={\"text\": \"messages\"})\n",
    "tuke_sk_df = tuke_sk_df[tuke_sk_df[\"label\"] == 0]\n",
    "tuke_sk_df = tuke_sk_df[[\"messages\", \"label\"]]\n",
    "tuke_sk_df = tuke_sk_df.dropna(subset=[\"messages\"])\n",
    "\n",
    "datasets = {\n",
    "    'jigsaw': jigsaw_df,\n",
    "    'olid': olid_df,\n",
    "    'hate_xplain': hateXplain,\n",
    "    'tuke_sk': tuke_sk_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Overview\n",
    "\n",
    "We use four major hate speech datasets:\n",
    "\n",
    "1. **Jigsaw Toxicity**\n",
    "   - Multi-label toxicity classification\n",
    "   - Includes toxic, severe_toxic, obscene, threat, insult, identity_hate labels\n",
    "\n",
    "2. **OLID (Offensive Language Identification Dataset)**\n",
    "   - Hierarchical labeling of offensive language\n",
    "   - Focuses on Twitter content\n",
    "\n",
    "3. **HateXplain**\n",
    "   - Annotated with rationales for hate speech\n",
    "   - Includes target community information\n",
    "\n",
    "4. **TUKE Slovak**\n",
    "   - Slovak language hate speech dataset\n",
    "   - Tests cross-lingual generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n",
    "Using multiple embedding approaches:\n",
    "- **BAAI BGE**: Optimized for semantic similarity\n",
    "- **MiniLM**: Efficient sentence transformers model\n",
    "- **TF-IDF**: Traditional bag-of-words approach\n",
    "\n",
    "These embeddings are used with SVM and XGBoost classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-04-03 22:52:04.037329509 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-03 22:52:04.037387066 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-04-03 22:52:04.793673176 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-03 22:52:04.793713046 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_finance = pkl.load(open(\"./models/tfidf_finance.pkl\", \"rb\"))\n",
    "tfidf_healthcare = pkl.load(open(\"./models/tfidf_healthcare.pkl\", \"rb\"))\n",
    "tfidf_law = pkl.load(open(\"./models/tfidf_law.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Process\n",
    "\n",
    "For each dataset and model combination:\n",
    "1. Load pre-trained domain classifiers\n",
    "2. Process test samples through each domain classifier\n",
    "3. Combine predictions using OR logic (any domain=1 -> toxic=0)\n",
    "4. Calculate metrics:\n",
    "   - Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Classifier -> zmenit na qwen2.5\n",
    "model_name = \"Qwen2.5:7b\"\n",
    "\n",
    "for domain, inference_df in datasets.items():\n",
    "    try:\n",
    "        llm_classifier_finance = LlmClassifier(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            api_base=os.getenv(\"PROXY_URL\"),\n",
    "            model_name=model_name,\n",
    "            domain=\"finance\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        llm_classifier_healthcare = LlmClassifier(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            api_base=os.getenv(\"PROXY_URL\"),\n",
    "            model_name=model_name,\n",
    "            domain=\"healthcare\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        llm_classifier_law = LlmClassifier(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            api_base=os.getenv(\"PROXY_URL\"),\n",
    "            model_name=model_name,\n",
    "            domain=\"law\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        # Load models\n",
    "        llm_classifier_finance.load_model(\"models/gpt-4o-mini-finance.json\")\n",
    "        llm_classifier_healthcare.load_model(\"models/gpt-4o-mini-healthcare.json\")\n",
    "        llm_classifier_law.load_model(\"models/gpt-4o-mini-law.json\")\n",
    "\n",
    "        predictions_llm = []\n",
    "        prediction_times_llm = []\n",
    "        actuals_llm = []\n",
    "\n",
    "        # Get predictions for each prompt\n",
    "        for _, row in tqdm(inference_df.head(1000).iterrows(), total=1000):\n",
    "            start_time = time.perf_counter_ns()\n",
    "\n",
    "            # Get predictions from all models\n",
    "            pred_finance = llm_classifier_finance.predict_single(row[\"messages\"])\n",
    "            pred_healthcare = llm_classifier_healthcare.predict_single(row[\"messages\"])\n",
    "            pred_law = llm_classifier_law.predict_single(row[\"messages\"])\n",
    "\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_llm.append(end_time - start_time)\n",
    "\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_llm.append(0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1)\n",
    "            actuals_llm.append(row[\"label\"])\n",
    "        \n",
    "        evaluate(predictions=predictions_llm, true_labels=actuals_llm, latency=prediction_times_llm, domain=domain, embed_model=\"ada-002\", model_name=model_name, train_acc=0.0, cost=0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running GPT model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier_finance = ModernBERTNLI(domain=\"finance\")\n",
    "bert_classifier_healthcare = ModernBERTNLI(domain=\"healthcare\")\n",
    "bert_classifier_law = ModernBERTNLI(domain=\"law\")\n",
    "\n",
    "predictions_bert = []\n",
    "prediction_times_bert = []\n",
    "actuals_bert = []\n",
    "\n",
    "try:\n",
    "    # Move models to GPU\n",
    "    bert_classifier_finance.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_healthcare.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_law.classifier.model.to(\"cuda\")\n",
    "\n",
    "    for domain, inference_df in datasets.items():\n",
    "        # Get predictions for each prompt\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "\n",
    "            # Get predictions from all models\n",
    "            pred_finance = bert_classifier_finance.predict(row[\"messages\"])\n",
    "            pred_healthcare = bert_classifier_healthcare.predict(row[\"messages\"])\n",
    "            pred_law = bert_classifier_law.predict(row[\"messages\"])\n",
    "\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_bert.append(end_time - start_time)\n",
    "\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_bert.append(0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1)\n",
    "            actuals_bert.append(row[\"label\"])\n",
    "\n",
    "    evaluate(predictions=predictions_bert, true_labels=actuals_bert, latency=prediction_times_bert, domain=domain, embed_model=\"BERT\", model_name=\"ModernBERT\", train_acc=0.0, cost=0.0)\n",
    "except Exception as e:\n",
    "    print(f\"Error running ModernBERT models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_ft = []\n",
    "predictions_ft = []\n",
    "prediction_times_ft = []\n",
    "\n",
    "# fastText\n",
    "for domain, inference_df in datasets.items():\n",
    "    fasttext_classifier_finance = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "    fasttext_classifier_finance.model = fasttext.load_model(\"models/fastText_finance_fasttext.bin\")\n",
    "\n",
    "    fasttext_classifier_healthcare = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "    fasttext_classifier_healthcare.model = fasttext.load_model(\"models/fastText_healthcare_fasttext.bin\")\n",
    "\n",
    "    fasttext_classifier_law = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "    fasttext_classifier_law.model = fasttext.load_model(\"models/fastText_law_fasttext.bin\")\n",
    "\n",
    "    for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "        text = str(row[\"messages\"])\n",
    "        query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "        start_time = time.perf_counter_ns()\n",
    "\n",
    "        # Predictions from all three classifiers\n",
    "        prediction_finance = fasttext_classifier_finance.model.predict(query)\n",
    "        prediction_healthcare = fasttext_classifier_healthcare.model.predict(query)\n",
    "        prediction_law = fasttext_classifier_law.model.predict(query)\n",
    "\n",
    "        end_time = time.perf_counter_ns()\n",
    "        prediction_times_ft.append(end_time - start_time)\n",
    "\n",
    "        predictions_ft.append(0 if (prediction_finance[0][0] == \"__label__1\" or prediction_healthcare[0][0] == \"__label__1\" or prediction_law[0][0] == \"__label__1\") else 1)\n",
    "        actuals_ft.append(row[\"label\"])\n",
    "\n",
    "        evaluate(predictions=predictions_ft, true_labels=actuals_ft, latency=prediction_times_ft, domain=domain, embed_model=\"fastText\", model_name=\"fastText\", train_acc=0.0, cost=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_names = [\"mini\", \"baai\", \"tfidf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding test data\n",
    "for embedding_model in embedding_models_names:\n",
    "    for domain, inference_df in datasets.items():\n",
    "        start_time = time.perf_counter_ns()\n",
    "        test_embeds = np.array(list(baai_embedding.embed(inference_df[\"messages\"])))\n",
    "        end_time = time.perf_counter_ns()\n",
    "        embed_times = end_time - start_time\n",
    "\n",
    "        mean_embed_time = embed_times / len(inference_df)\n",
    "\n",
    "        with open(f\"models/SVM_finance_{embedding_model}.pkl\", \"rb\") as svm_file:\n",
    "            svm_classifier_finance = pkl.load(svm_file)\n",
    "\n",
    "        with open(f\"models/SVM_healthcare_{embedding_model}.pkl\", \"rb\") as svm_file:\n",
    "            svm_classifier_healthcare = pkl.load(svm_file)\n",
    "\n",
    "        with open(f\"models/SVM_law_{embedding_model}.pkl\", \"rb\") as svm_file:\n",
    "            svm_classifier_law = pkl.load(svm_file)\n",
    "\n",
    "        xgb_classifier_finance = XGBClassifier()\n",
    "        xgb_classifier_healthcare = XGBClassifier()\n",
    "        xgb_classifier_law = XGBClassifier()\n",
    "\n",
    "        xgb_classifier_finance.load_model(f\"models/XGBoost_finance_{embedding_model}.json\")\n",
    "        xgb_classifier_healthcare.load_model(f\"models/XGBoost_healthcare_{embedding_model}.json\")\n",
    "        xgb_classifier_law.load_model(f\"models/XGBoost_law_{embedding_model}.json\")\n",
    "\n",
    "        predictions_xgb = []\n",
    "        predictions_svm = []\n",
    "\n",
    "        prediction_times_xgb = []\n",
    "        prediction_times_svm = []\n",
    "\n",
    "        actuals_ml = []\n",
    "\n",
    "        for test_embed in test_embeds:\n",
    "            if embedding_model == \"tfidf\":\n",
    "                test_embed = test_embed.reshape(1, -1)\n",
    "                test_embed = tfidf_finance.transform(test_embed).toarray()[0]\n",
    "            else:\n",
    "                test_embed = test_embed.reshape(1, -1)\n",
    "            # SVM predictions\n",
    "            start_time = time.perf_counter_ns()\n",
    "            pred_finance = svm_classifier_finance.predict(test_embed)\n",
    "            pred_healthcare = svm_classifier_healthcare.predict(test_embed)\n",
    "            pred_law = svm_classifier_law.predict(test_embed)\n",
    "            end_time = time.perf_counter_ns()\n",
    "\n",
    "            prediction_times_svm.append(end_time - start_time)\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_svm.append(0 if (pred_finance[0] == 1 or pred_healthcare[0] == 1 or pred_law[0] == 1) else 1)\n",
    "\n",
    "            # XGBoost predictions\n",
    "            start_time = time.perf_counter_ns()\n",
    "            pred_finance = xgb_classifier_finance.predict(test_embed)\n",
    "            pred_healthcare = xgb_classifier_healthcare.predict(test_embed)\n",
    "            pred_law = xgb_classifier_law.predict(test_embed)\n",
    "            end_time = time.perf_counter_ns()\n",
    "\n",
    "            prediction_times_xgb.append(end_time - start_time)\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_xgb.append(0 if (pred_finance[0] == 1 or pred_healthcare[0] == 1 or pred_law[0] == 1) else 1)\n",
    "            actuals_ml.append(row[\"label\"])\n",
    "\n",
    "        \n",
    "        evaluate(predictions=predictions_svm, true_labels=actuals_ml, latency=prediction_times_svm, domain=domain, embed_model=embedding_model, model_name=f\"fastText_{embedding_model}\", train_acc=0.0, cost=0.0)\n",
    "        evaluate(predictions=predictions_xgb, true_labels=actuals_ml, latency=prediction_times_xgb, domain=domain, embed_model=embedding_model, model_name=f\"XGBoost_{embedding_model}\", train_acc=0.0, cost=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
