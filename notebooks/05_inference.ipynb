{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Domain Generalization: Hate Speech Detection\n",
    "\n",
    "This notebook evaluates how well domain-specific classifiers generalize to out-of-domain hate speech detection.\n",
    "We test various model architectures trained on domain classification to see if they can effectively identify hate speech.\n",
    "\n",
    "Key aspects evaluated:\n",
    "- Few-shot transfer capabilities using DSPy\n",
    "- Model robustness across different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "This notebook evaluates several classification models:\n",
    "1. **LLM-based**: Using Qwen 2.5 for zero-shot classification\n",
    "2. **BERT-based**: ModernBERT with NLI approach\n",
    "3. **Traditional ML**: \n",
    "   - fastText for efficient text classification\n",
    "   - SVM and XGBoost with different embeddings\n",
    "\n",
    "Each model is evaluated on hate speech detection as an out-of-domain task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import dependencies and initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from prompt_classifier.metrics import evaluate_run\n",
    "from prompt_classifier.modeling.dspy_llm import LlmClassifier\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(22)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Datasets\n",
    "\n",
    "Load various hate speech datasets for evaluation:\n",
    "- Jigsaw Toxicity\n",
    "- OLID\n",
    "- HateXplain\n",
    "- TUKE Slovak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw_splits = {\n",
    "    \"train\": \"train_dataset.csv\",\n",
    "    \"validation\": \"val_dataset.csv\",\n",
    "    \"test\": \"test_dataset.csv\",\n",
    "}\n",
    "jigsaw_df = pd.read_csv(\n",
    "    \"hf://datasets/Arsive/toxicity_classification_jigsaw/\"\n",
    "    + jigsaw_splits[\"validation\"]\n",
    ")\n",
    "\n",
    "jigsaw_df = jigsaw_df[\n",
    "    (jigsaw_df[\"toxic\"] == 1)\n",
    "    | (jigsaw_df[\"severe_toxic\"] == 1)\n",
    "    | (jigsaw_df[\"obscene\"] == 1)\n",
    "    | (jigsaw_df[\"threat\"] == 1)\n",
    "    | (jigsaw_df[\"insult\"] == 1)\n",
    "    | (jigsaw_df[\"identity_hate\"] == 1)\n",
    "]\n",
    "\n",
    "jigsaw_df = jigsaw_df.rename(columns={\"comment_text\": \"prompt\"})\n",
    "jigsaw_df[\"label\"] = 0\n",
    "jigsaw_df = jigsaw_df[[\"prompt\", \"label\"]]\n",
    "jigsaw_df = jigsaw_df.dropna(subset=[\"prompt\"])\n",
    "jigsaw_df = jigsaw_df[jigsaw_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load OLID dataset\n",
    "olid_splits = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
    "olid_df = pd.read_csv(\"hf://datasets/christophsonntag/OLID/\" + olid_splits[\"test\"])\n",
    "olid_df = olid_df.rename(columns={\"cleaned_tweet\": \"prompt\"})\n",
    "olid_df[\"label\"] = 0\n",
    "olid_df = olid_df[[\"prompt\", \"label\"]]\n",
    "olid_df = olid_df.dropna(subset=[\"prompt\"])\n",
    "olid_df = olid_df[olid_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load hateXplain dataset\n",
    "hate_xplain = pd.read_parquet(\n",
    "    \"hf://datasets/nirmalendu01/hateXplain_filtered/data/train-00000-of-00001.parquet\"\n",
    ")\n",
    "hate_xplain = hate_xplain.rename(columns={\"test_case\": \"prompt\"})\n",
    "hate_xplain = hate_xplain[(hate_xplain[\"gold_label\"] == \"hateful\")]\n",
    "hate_xplain = hate_xplain[[\"prompt\", \"label\"]]\n",
    "hate_xplain[\"label\"] = 0\n",
    "hate_xplain = hate_xplain.dropna(subset=[\"prompt\"])\n",
    "hate_xplain = hate_xplain[hate_xplain[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load TUKE Slovak dataset\n",
    "tuke_sk_splits = {\"train\": \"train.json\", \"test\": \"test.json\"}\n",
    "tuke_sk_df = pd.read_json(\n",
    "    \"hf://datasets/TUKE-KEMT/hate_speech_slovak/\" + tuke_sk_splits[\"test\"],\n",
    "    lines=True,\n",
    ")\n",
    "tuke_sk_df = tuke_sk_df.rename(columns={\"text\": \"prompt\"})\n",
    "tuke_sk_df = tuke_sk_df[tuke_sk_df[\"label\"] == 0]\n",
    "tuke_sk_df = tuke_sk_df[[\"prompt\", \"label\"]]\n",
    "tuke_sk_df = tuke_sk_df.dropna(subset=[\"prompt\"])\n",
    "tuke_sk_df = tuke_sk_df[tuke_sk_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "\n",
    "dkk_all = pd.read_parquet(\"data/test-00000-of-00001.parquet\")\n",
    "dkk_all = dkk_all.rename(columns={\"text\": \"prompt\"})\n",
    "dkk_all[\"label\"] = 0\n",
    "dkk_all = dkk_all.dropna(subset=[\"prompt\"])\n",
    "dkk_all = dkk_all[dkk_all[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "web_questions = pd.read_parquet(\"hf://datasets/Stanford/web_questions/\" + splits[\"test\"])\n",
    "\n",
    "web_questions['prompt'] = web_questions['question']\n",
    "web_questions['label'] = 0\n",
    "web_questions['dataset'] = 'web_questions'\n",
    "web_questions = web_questions[['prompt', 'label']]\n",
    "\n",
    "splits = {'train': 'data/train-00000-of-00001-7ebb9cdef03dd950.parquet', 'test': 'data/test-00000-of-00001-fbd3905b045b12b8.parquet'}\n",
    "ml_questions = pd.read_parquet(\"hf://datasets/mjphayes/machine_learning_questions/\" + splits[\"test\"])\n",
    "\n",
    "ml_questions['prompt'] = ml_questions['question']\n",
    "ml_questions['label'] = 0\n",
    "ml_questions['dataset'] = 'machine_learning_questions'\n",
    "ml_questions = ml_questions[['prompt', 'label']]\n",
    "\n",
    "datasets = {\n",
    "    \"jigsaw\": jigsaw_df,\n",
    "    \"olid\": olid_df,\n",
    "    \"hate_xplain\": hate_xplain,\n",
    "    \"tuke_sk\": tuke_sk_df,\n",
    "    \"dkk\": dkk_all,\n",
    "    \"web_questions\": web_questions,\n",
    "    \"ml_questions\": ml_questions,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [1, 16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(jigsaw_df.head())\n",
    "display(olid_df.head())\n",
    "display(hate_xplain.head())\n",
    "display(tuke_sk_df.head())\n",
    "display(dkk_all.head())\n",
    "display(web_questions.head())\n",
    "display(ml_questions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Overview\n",
    "\n",
    "We use four major hate speech datasets:\n",
    "\n",
    "1. **Jigsaw Toxicity**\n",
    "   - Multi-label toxicity classification\n",
    "   - Includes toxic, severe_toxic, obscene, threat, insult, identity_hate labels\n",
    "\n",
    "2. **OLID (Offensive Language Identification Dataset)**\n",
    "   - Hierarchical labeling of offensive language\n",
    "   - Focuses on Twitter content\n",
    "\n",
    "3. **HateXplain**\n",
    "   - Annotated with rationales for hate speech\n",
    "   - Includes target community information\n",
    "\n",
    "4. **TUKE Slovak**\n",
    "   - Slovak language hate speech dataset\n",
    "   - Tests cross-lingual generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n",
    "Using multiple embedding approaches:\n",
    "- **BAAI BGE**: Optimized for semantic similarity\n",
    "- **MiniLM**: Efficient sentence transformers model\n",
    "- **TF-IDF**: Traditional bag-of-words approach\n",
    "\n",
    "These embeddings are used with SVM and XGBoost classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = os.getcwd()\n",
    "print(current_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_finance = pkl.load(open(\"models/tfidf_finance.pkl\", \"rb\"))\n",
    "tfidf_healthcare = pkl.load(open(\"models/tfidf_healthcare.pkl\", \"rb\"))\n",
    "tfidf_law = pkl.load(open(\"models/tfidf_law.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Process\n",
    "\n",
    "For each dataset and model combination:\n",
    "1. Load pre-trained domain classifiers\n",
    "2. Process test samples through each domain classifier\n",
    "3. Combine predictions using OR logic (any domain=1 -> toxic=0)\n",
    "4. Calculate metrics:\n",
    "   - Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ollama_chat/qwen2.5:14b\"\n",
    "\n",
    "for domain, inference_df in datasets.items():\n",
    "    try:\n",
    "        llm_classifier_finance = LlmClassifier(\n",
    "            api_key=\"\",\n",
    "            api_base=\"http://localhost:11434\",\n",
    "            model_name=model_name,\n",
    "            domain=\"finance\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        llm_classifier_healthcare = LlmClassifier(\n",
    "            api_key=\"\",\n",
    "            api_base=\"http://localhost:11434\",\n",
    "            model_name=model_name,\n",
    "            domain=\"healthcare\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        llm_classifier_law = LlmClassifier(\n",
    "            api_key=\"\",\n",
    "            api_base=\"http://localhost:11434\",\n",
    "            model_name=model_name,\n",
    "            domain=\"law\",\n",
    "            train_data=inference_df,\n",
    "            test_data=inference_df,\n",
    "        )\n",
    "\n",
    "        # Load models\n",
    "        llm_classifier_finance.load_model(\"models/qwen2.5:14b-finance.json\")\n",
    "        llm_classifier_healthcare.load_model(\"models/qwen2.5:14b-healthcare.json\")\n",
    "        llm_classifier_law.load_model(\"models/qwen2.5:14b-law.json\")\n",
    "\n",
    "        predictions_llm = []\n",
    "        prediction_times_llm = []\n",
    "        actuals_llm = []\n",
    "\n",
    "        # Get predictions for each prompt\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "\n",
    "            # Get predictions from all models\n",
    "            pred_finance = llm_classifier_finance.predict_single(row[\"prompt\"])\n",
    "            pred_healthcare = llm_classifier_healthcare.predict_single(row[\"prompt\"])\n",
    "            pred_law = llm_classifier_law.predict_single(row[\"prompt\"])\n",
    "\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_llm.append(end_time - start_time)\n",
    "\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_llm.append(\n",
    "                0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1\n",
    "            )\n",
    "            actuals_llm.append(row[\"label\"])\n",
    "\n",
    "        evaluate_run(\n",
    "            predictions=predictions_llm,\n",
    "            true_labels=actuals_llm,\n",
    "            latency=statistics.mean(prediction_times_llm),\n",
    "            domain=domain,\n",
    "            embed_model=\"qwen2.5\",\n",
    "            model_name=model_name,\n",
    "            train_acc=0.0,\n",
    "            cost=0.0,\n",
    "            training=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error running LLM model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier_finance = ModernBERTNLI(domain=\"finance\")\n",
    "bert_classifier_healthcare = ModernBERTNLI(domain=\"healthcare\")\n",
    "bert_classifier_law = ModernBERTNLI(domain=\"law\")\n",
    "\n",
    "try:\n",
    "    # Move models to GPU\n",
    "    bert_classifier_finance.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_healthcare.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_law.classifier.model.to(\"cuda\")\n",
    "\n",
    "    for domain, inference_df in datasets.items():\n",
    "        predictions_bert = []\n",
    "        prediction_times_bert = []\n",
    "        actuals_bert = []\n",
    "        # Get predictions for each prompt\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "\n",
    "            # Get predictions from all models\n",
    "            pred_finance = bert_classifier_finance.predict(row[\"prompt\"])\n",
    "            pred_healthcare = bert_classifier_healthcare.predict(row[\"prompt\"])\n",
    "            pred_law = bert_classifier_law.predict(row[\"prompt\"])\n",
    "\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_bert.append(end_time - start_time)\n",
    "\n",
    "            # If any model predicts 1, final prediction is 0\n",
    "            predictions_bert.append(\n",
    "                0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1\n",
    "            )\n",
    "            actuals_bert.append(row[\"label\"])\n",
    "\n",
    "    evaluate_run(\n",
    "        predictions=predictions_bert,\n",
    "        true_labels=actuals_bert,\n",
    "        latency=statistics.mean(prediction_times_bert),\n",
    "        domain=domain,\n",
    "        embed_model=\"BERT\",\n",
    "        model_name=\"ModernBERT\",\n",
    "        train_acc=0.0,\n",
    "        cost=0.0,\n",
    "        training=False,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error running ModernBERT models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastText\n",
    "for domain, inference_df in datasets.items():\n",
    "    actuals_ft = []\n",
    "    predictions_ft = []\n",
    "    prediction_times_ft = []\n",
    "    print(f\"Processing dataset {domain}...\")\n",
    "    try:\n",
    "        # Try to load model with proper error handling\n",
    "        try:\n",
    "            fasttext_classifier_finance = FastTextClassifier(\n",
    "                train_data=inference_df, test_data=inference_df\n",
    "            )\n",
    "            fasttext_classifier_finance.model = fasttext.load_model(\n",
    "                \"models/fastText_finance_fasttext.bin\"\n",
    "            )\n",
    "\n",
    "            fasttext_classifier_healthcare = FastTextClassifier(\n",
    "                train_data=inference_df, test_data=inference_df\n",
    "            )\n",
    "            fasttext_classifier_healthcare.model = fasttext.load_model(\n",
    "                \"models/fastText_healthcare_fasttext.bin\"\n",
    "            )\n",
    "\n",
    "            fasttext_classifier_law = FastTextClassifier(\n",
    "                train_data=inference_df, test_data=inference_df\n",
    "            )\n",
    "            fasttext_classifier_law.model = fasttext.load_model(\n",
    "                \"models/fastText_law_fasttext.bin\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fastText models: {e}\")\n",
    "            continue\n",
    "\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            text = str(row[\"prompt\"])\n",
    "            query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "            try:\n",
    "                start_time = time.perf_counter_ns()\n",
    "\n",
    "                # Predictions from all three classifiers\n",
    "                prediction_finance = fasttext_classifier_finance.model.predict(query)\n",
    "                prediction_healthcare = fasttext_classifier_healthcare.model.predict(\n",
    "                    query\n",
    "                )\n",
    "                prediction_law = fasttext_classifier_law.model.predict(query)\n",
    "\n",
    "                end_time = time.perf_counter_ns()\n",
    "                prediction_times_ft.append(end_time - start_time)\n",
    "\n",
    "                predictions_ft.append(\n",
    "                    0\n",
    "                    if (\n",
    "                        prediction_finance[0][0] == \"__label__1\"\n",
    "                        or prediction_healthcare[0][0] == \"__label__1\"\n",
    "                        or prediction_law[0][0] == \"__label__1\"\n",
    "                    )\n",
    "                    else 1\n",
    "                )\n",
    "                actuals_ft.append(row[\"label\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "        evaluate_run(\n",
    "            predictions=predictions_ft,\n",
    "            true_labels=actuals_ft,\n",
    "            latency=statistics.mean(prediction_times_ft),\n",
    "            domain=domain,\n",
    "            embed_model=\"fastText\",\n",
    "            model_name=\"fastText\",\n",
    "            train_acc=0.0,\n",
    "            cost=0.0,\n",
    "            training=False,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset {domain}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - SVM, XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_names = [\"mini\", \"baai\", \"tf_idf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding test data\n",
    "for embedding_model in embedding_models_names:\n",
    "    for domain, inference_df in datasets.items():\n",
    "        print(f\"Processing {domain} dataset with {embedding_model} embeddings...\")\n",
    "        actuals_ml = inference_df[\"label\"].tolist()\n",
    "\n",
    "        # Get embeddings once per dataset\n",
    "        try:\n",
    "            if embedding_model == \"tf_idf\":\n",
    "                test_embeds = tfidf_finance.transform(inference_df[\"prompt\"])\n",
    "            else:\n",
    "                start_time = time.perf_counter_ns()\n",
    "                if embedding_model == \"mini\":\n",
    "                    test_embeds = np.array(\n",
    "                        list(mini_embedding.embed(inference_df[\"prompt\"]))\n",
    "                    )\n",
    "                else:  # baai\n",
    "                    test_embeds = np.array(\n",
    "                        list(baai_embedding.embed(inference_df[\"prompt\"]))\n",
    "                    )\n",
    "                end_time = time.perf_counter_ns()\n",
    "                embed_times = end_time - start_time\n",
    "                mean_embed_time = embed_times / len(inference_df)\n",
    "\n",
    "            # Load models once per embedding type\n",
    "            # Load SVM models\n",
    "            with open(f\"models/SVM_finance_{embedding_model}.pkl\", \"rb\") as f:\n",
    "                svm_classifier_finance = pkl.load(f)\n",
    "            with open(f\"models/SVM_healthcare_{embedding_model}.pkl\", \"rb\") as f:\n",
    "                svm_classifier_healthcare = pkl.load(f)\n",
    "            with open(f\"models/SVM_law_{embedding_model}.pkl\", \"rb\") as f:\n",
    "                svm_classifier_law = pkl.load(f)\n",
    "\n",
    "            # Load XGBoost models\n",
    "            xgb_classifier_finance = XGBClassifier()\n",
    "            xgb_classifier_healthcare = XGBClassifier()\n",
    "            xgb_classifier_law = XGBClassifier()\n",
    "\n",
    "            xgb_classifier_finance.load_model(\n",
    "                f\"models/XGBoost_finance_{embedding_model}.json\"\n",
    "            )\n",
    "            xgb_classifier_healthcare.load_model(\n",
    "                f\"models/XGBoost_healthcare_{embedding_model}.json\"\n",
    "            )\n",
    "            xgb_classifier_law.load_model(f\"models/XGBoost_law_{embedding_model}.json\")\n",
    "\n",
    "            # Process different batch sizes\n",
    "            for batch in batch_size:\n",
    "                predictions_xgb = []\n",
    "                predictions_svm = []\n",
    "                prediction_times_xgb = []\n",
    "                prediction_times_svm = []\n",
    "\n",
    "                # Process embeddings in batches\n",
    "                for i in range(0, test_embeds.shape[0], batch):\n",
    "                    batch_embeds = test_embeds[i:i + batch]\n",
    "                    # SVM predictions\n",
    "                    start_time = time.perf_counter_ns()\n",
    "                    pred_finance = svm_classifier_finance.predict(batch_embeds)\n",
    "                    pred_healthcare = svm_classifier_healthcare.predict(batch_embeds)\n",
    "                    pred_law = svm_classifier_law.predict(batch_embeds)\n",
    "                    end_time = time.perf_counter_ns()\n",
    "                    prediction_times_svm.extend([end_time - start_time])\n",
    "\n",
    "                    # Combine predictions for the batch\n",
    "                    batch_preds = np.zeros(batch_embeds.shape[0])\n",
    "                    batch_preds[(pred_finance == 1) | (pred_healthcare == 1) | (pred_law == 1)] = 0\n",
    "                    batch_preds[(pred_finance != 1) & (pred_healthcare != 1) & (pred_law != 1)] = 1\n",
    "                    predictions_svm.extend(batch_preds)\n",
    "\n",
    "                    # XGBoost predictions\n",
    "                    start_time = time.perf_counter_ns()\n",
    "                    pred_finance = xgb_classifier_finance.predict(batch_embeds)\n",
    "                    pred_healthcare = xgb_classifier_healthcare.predict(batch_embeds)\n",
    "                    pred_law = xgb_classifier_law.predict(batch_embeds)\n",
    "                    end_time = time.perf_counter_ns()\n",
    "                    prediction_times_xgb.extend([end_time - start_time])\n",
    "\n",
    "                    # Combine predictions for the batch\n",
    "                    batch_preds = np.zeros(batch_embeds.shape[0])\n",
    "                    batch_preds[(pred_finance == 1) | (pred_healthcare == 1) | (pred_law == 1)] = 0\n",
    "                    batch_preds[(pred_finance != 1) & (pred_healthcare != 1) & (pred_law != 1)] = 1\n",
    "                    predictions_xgb.extend(batch_preds)\n",
    "\n",
    "                # Evaluate results for this batch size\n",
    "                evaluate_run(\n",
    "                    predictions=predictions_svm,\n",
    "                    true_labels=actuals_ml,\n",
    "                    latency=statistics.mean(prediction_times_svm),\n",
    "                    domain=domain,\n",
    "                    embed_model=f'{embedding_model}-{batch}',\n",
    "                    model_name=\"SVM\",\n",
    "                    train_acc=0.0,\n",
    "                    cost=0.0,\n",
    "                    training=False,\n",
    "                )\n",
    "                evaluate_run(\n",
    "                    predictions=predictions_xgb,\n",
    "                    true_labels=actuals_ml,\n",
    "                    latency=statistics.mean(prediction_times_xgb),\n",
    "                    domain=domain,\n",
    "                    embed_model=f'{embedding_model}-{batch}',\n",
    "                    model_name=\"XGBoost\",\n",
    "                    train_acc=0.0,\n",
    "                    cost=0.0,\n",
    "                    training=False,\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error processing {domain} dataset with {embedding_model} embeddings: {e}\"\n",
    "            )\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
