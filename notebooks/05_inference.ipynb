{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import time\n",
    "\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from prompt_classifier.modeling.dspy_llm import LlmClassifier\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw_splits = {'train': 'train_dataset.csv', 'validation': 'val_dataset.csv', 'test': 'test_dataset.csv'}\n",
    "inference_df = pd.read_csv(\"hf://datasets/Arsive/toxicity_classification_jigsaw/\" + jigsaw_splits[\"validation\"])\n",
    "\n",
    "olid_splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "olid_df = pd.read_csv(\"hf://datasets/christophsonntag/OLID/\" + olid_splits[\"train\"])\n",
    "\n",
    "inference_df = inference_df[(inference_df[\"toxic\"] == 1) | \n",
    "                            (inference_df[\"severe_toxic\"] == 1) | \n",
    "                            (inference_df[\"obscene\"] == 1) | \n",
    "                            (inference_df[\"threat\"] == 1) | \n",
    "                            (inference_df[\"insult\"] == 1) | \n",
    "                            (inference_df[\"identity_hate\"] == 1)]\n",
    "\n",
    "olid_df = olid_df.rename(columns={\"cleaned_tweet\": \"prompt\"})\n",
    "olid_df[\"label\"] = 0\n",
    "\n",
    "inference_df = inference_df.rename(columns={\"comment_text\": \"prompt\"})\n",
    "inference_df[\"label\"] = 0\n",
    "\n",
    "inference_df = pd.concat([inference_df, olid_df], ignore_index=True)\n",
    "inference_df = inference_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = inference_df[[\"prompt\", \"label\"]]\n",
    "inference_df = inference_df.dropna(subset=['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-03-31 14:26:10.285378521 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-03-31 14:26:10.285426626 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT Classifier\n",
    "llm_classifier = LlmClassifier(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_base=os.getenv(\"PROXY_URL\"),\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    domain=\"finance or healthcare or law\",\n",
    "    train_data=inference_df,\n",
    "    test_data=inference_df,\n",
    ")\n",
    "\n",
    "try:\n",
    "    llm_classifier.load_model(\"models/gpt-4o-mini-finance.json\")\n",
    "\n",
    "    test_predictions, test_actuals, test_latency = llm_classifier.predict()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error running GPT model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier_finance = ModernBERTNLI(domain=\"finance\")\n",
    "bert_classifier_healthcare = ModernBERTNLI(domain=\"healthcare\") \n",
    "bert_classifier_law = ModernBERTNLI(domain=\"law\")\n",
    "\n",
    "predictions_bert = []\n",
    "prediction_times_bert = []\n",
    "\n",
    "try:\n",
    "    # Move models to GPU\n",
    "    bert_classifier_finance.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_healthcare.classifier.model.to(\"cuda\")\n",
    "    bert_classifier_law.classifier.model.to(\"cuda\")\n",
    "\n",
    "    # Get predictions for each prompt\n",
    "    for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "        start_time = time.perf_counter_ns()\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        pred_finance = bert_classifier_finance.predict(row[\"prompt\"])\n",
    "        pred_healthcare = bert_classifier_healthcare.predict(row[\"prompt\"])\n",
    "        pred_law = bert_classifier_law.predict(row[\"prompt\"])\n",
    "        \n",
    "        end_time = time.perf_counter_ns()\n",
    "        prediction_times_bert.append(end_time - start_time)\n",
    "        \n",
    "        # If any model predicts 1, final prediction is 0\n",
    "        predictions_bert.append(0 if (pred_finance == 1 or pred_healthcare == 1 or pred_law == 1) else 1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error running ModernBERT models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 16384/16384 [00:01<00:00, 12847.39it/s]\n"
     ]
    }
   ],
   "source": [
    "actuals_ft = []\n",
    "predictions_ft = []\n",
    "prediction_times_ft = []\n",
    "\n",
    "# fastText\n",
    "fasttext_classifier_finance = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "fasttext_classifier_finance.model = fasttext.load_model(\"models/fastText_finance_fasttext.bin\")\n",
    "\n",
    "fasttext_classifier_healthcare = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "fasttext_classifier_healthcare.model = fasttext.load_model(\"models/fastText_healthcare_fasttext.bin\")\n",
    "\n",
    "fasttext_classifier_law = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "fasttext_classifier_law.model = fasttext.load_model(\"models/fastText_law_fasttext.bin\")\n",
    "\n",
    "for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "    text = str(row[\"prompt\"])\n",
    "    query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "    start_time = time.perf_counter_ns()\n",
    "    \n",
    "    # Predictions from all three classifiers\n",
    "    prediction_finance = fasttext_classifier_finance.model.predict(query)\n",
    "    prediction_healthcare = fasttext_classifier_healthcare.model.predict(query)\n",
    "    prediction_law = fasttext_classifier_law.model.predict(query)\n",
    "    \n",
    "    end_time = time.perf_counter_ns()\n",
    "    prediction_times_ft.append(end_time - start_time)\n",
    "    \n",
    "    predictions_ft.append(0 if (prediction_finance[0][0] == \"__label__1\" or prediction_healthcare[0][0] == \"__label__1\" or prediction_law[0][0] == \"__label__1\") else 1)\n",
    "\n",
    "    actuals_ft.append(row[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding test data\n",
    "start_time = time.perf_counter_ns()\n",
    "test_embeds = np.array(list(baai_embedding.embed(inference_df[\"prompt\"])))\n",
    "end_time = time.perf_counter_ns()\n",
    "embed_times = end_time - start_time\n",
    "\n",
    "mean_embed_time = embed_times / len(inference_df)\n",
    "\n",
    "with open(\"models/SVM_finance_baai.pkl\", \"rb\") as svm_file:\n",
    "    svm_classifier_finance = pkl.load(svm_file)\n",
    "\n",
    "with open(\"models/SVM_healthcare_baai.pkl\", \"rb\") as svm_file:\n",
    "    svm_classifier_healthcare = pkl.load(svm_file)\n",
    "\n",
    "with open(\"models/SVM_law_baai.pkl\", \"rb\") as svm_file:\n",
    "    svm_classifier_law = pkl.load(svm_file)\n",
    "\n",
    "xgb_classifier_finance = XGBClassifier()\n",
    "xgb_classifier_healthcare = XGBClassifier()\n",
    "xgb_classifier_law = XGBClassifier()\n",
    "\n",
    "xgb_classifier_finance.load_model(\"models/XGBoost_finance_baai.json\")\n",
    "xgb_classifier_healthcare.load_model(\"models/XGBoost_healthcare_baai.json\")\n",
    "xgb_classifier_law.load_model(\"models/XGBoost_law_baai.json\")\n",
    "\n",
    "predictions_xgb = []\n",
    "predictions_svm = []\n",
    "\n",
    "prediction_times_xgb = []\n",
    "prediction_times_svm = []\n",
    "\n",
    "for test_embed in test_embeds:\n",
    "    # SVM predictions\n",
    "    start_time = time.perf_counter_ns()\n",
    "    pred_finance = svm_classifier_finance.predict(test_embed.reshape(1, -1))\n",
    "    pred_healthcare = svm_classifier_healthcare.predict(test_embed.reshape(1, -1))\n",
    "    pred_law = svm_classifier_law.predict(test_embed.reshape(1, -1))\n",
    "    end_time = time.perf_counter_ns()\n",
    "    \n",
    "    prediction_times_svm.append(end_time - start_time)\n",
    "    # If any model predicts 1, final prediction is 0\n",
    "    predictions_svm.append(0 if (pred_finance[0] == 1 or pred_healthcare[0] == 1 or pred_law[0] == 1) else 1)\n",
    "\n",
    "    # XGBoost predictions\n",
    "    start_time = time.perf_counter_ns()\n",
    "    pred_finance = xgb_classifier_finance.predict(test_embed.reshape(1, -1))\n",
    "    pred_healthcare = xgb_classifier_healthcare.predict(test_embed.reshape(1, -1))\n",
    "    pred_law = xgb_classifier_law.predict(test_embed.reshape(1, -1))\n",
    "    end_time = time.perf_counter_ns()\n",
    "    \n",
    "    prediction_times_xgb.append(end_time - start_time)\n",
    "    # If any model predicts 1, final prediction is 0\n",
    "    predictions_xgb.append(0 if (pred_finance[0] == 1 or pred_healthcare[0] == 1 or pred_law[0] == 1) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_ms = {\n",
    "    'fastText': np.array(prediction_times_ft) / 1_000_000,\n",
    "    'XGBoost': np.array(prediction_times_xgb) / 1_000_000,\n",
    "    'SVM': np.array(prediction_times_svm) / 1_000_000,\n",
    "}\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'Model': [k for k,v in times_ms.items() for _ in v],\n",
    "    'Latency (ms)': [x for v in times_ms.values() for x in v]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastText Accuracy: 0.15362548828125\n",
      "SVM Accuracy: 0.778564453125\n",
      "XGBoost Accuracy: 0.725341796875\n"
     ]
    }
   ],
   "source": [
    "accuracy_ft = metrics.accuracy_score(actuals_ft, predictions_ft)\n",
    "accuracy_svm = metrics.accuracy_score(actuals_ft, predictions_svm)\n",
    "accuracy_xgb = metrics.accuracy_score(actuals_ft, predictions_xgb)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"fastText Accuracy: {accuracy_ft}\")\n",
    "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
