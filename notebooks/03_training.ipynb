{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Classification Model Training\n",
    "\n",
    "This notebook implements the training pipeline for multiple classification models to identify domain-specific prompts. We compare different approaches:\n",
    "\n",
    "## Models Evaluated\n",
    "- **GPT-based classifier**: Using LLM for zero/few-shot classification\n",
    "- **ModernBERT NLI**: Fine-tuned BERT model for Natural Language Inference\n",
    "- **SVM**: Using different text embeddings (BAAI-BGE, MiniLM, TF-IDF)\n",
    "- **XGBoost**: Using different text embeddings\n",
    "- **FastText**: Specialized text classification model\n",
    "\n",
    "## Domains\n",
    "- Law\n",
    "- Healthcare\n",
    "- Finance\n",
    "\n",
    "## Evaluation Metrics\n",
    "- Accuracy\n",
    "- Latency\n",
    "- Cost (for LLM-based approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from prompt_classifier.metrics import evaluate\n",
    "from prompt_classifier.modeling.dspy_llm import LlmClassifier\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "from prompt_classifier.util import create_domain_dataset, train_and_evaluate_model\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Acceleration\n",
    "\n",
    "Check available ONNX Runtime providers for hardware acceleration (CPU/CUDA).\n",
    "This affects the performance of embedding models and ModernBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available ONNX Runtime providers (CPU, CUDA etc.)\n",
    "providers = ort.get_available_providers()\n",
    "\n",
    "print(providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "### Training Data Types\n",
    "1. **Processed Data**: Clean, filtered dataset used for traditional ML models\n",
    "2. **Interim Data**: Raw/intermediate data used for LLM experiments\n",
    "\n",
    "Each domain dataset is balanced with positive samples from target domain and negative samples from other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "law_prompts = pd.read_csv(\"data/processed/law_prompts.csv\")\n",
    "healthcare_prompts = pd.read_csv(\"data/processed/healthcare_prompts.csv\")\n",
    "finance_prompts = pd.read_csv(\"data/processed/finance_prompts.csv\")\n",
    "\n",
    "law_dataset = create_domain_dataset(\n",
    "    law_prompts,\n",
    "    [healthcare_prompts, finance_prompts]\n",
    ")\n",
    "\n",
    "healthcare_dataset = create_domain_dataset(\n",
    "    healthcare_prompts,\n",
    "    [law_prompts, finance_prompts]\n",
    ")\n",
    "\n",
    "finance_dataset = create_domain_dataset(\n",
    "    finance_prompts,\n",
    "    [law_prompts, healthcare_prompts]\n",
    ")\n",
    "\n",
    "datasets = {\n",
    "    \"law\": law_dataset,\n",
    "    \"healthcare\": healthcare_dataset,\n",
    "    \"finance\": finance_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interim datasets\n",
    "law_prompts_interim = pd.read_csv(\"data/interim/law_prompts.csv\")\n",
    "healthcare_prompts_interim = pd.read_csv(\"data/interim/healthcare_prompts.csv\")\n",
    "finance_prompts_interim = pd.read_csv(\"data/interim/finance_prompts.csv\")\n",
    "\n",
    "law_dataset_interim = create_domain_dataset(\n",
    "    law_prompts_interim,\n",
    "    [healthcare_prompts_interim, finance_prompts_interim]\n",
    ")\n",
    "\n",
    "healthcare_dataset_interim = create_domain_dataset(\n",
    "    healthcare_prompts_interim,\n",
    "    [law_prompts_interim, finance_prompts_interim]\n",
    ")\n",
    "\n",
    "finance_dataset_interim = create_domain_dataset(\n",
    "    finance_prompts_interim,\n",
    "    [law_prompts_interim, healthcare_prompts_interim]\n",
    ")\n",
    "\n",
    "datasets_interim = {\n",
    "    \"law\": law_dataset_interim,\n",
    "    \"healthcare\": healthcare_dataset_interim,\n",
    "    \"finance\": finance_dataset_interim,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Embedding Models\n",
    "\n",
    "Set up text embedding models:\n",
    "- BAAI BGE Small\n",
    "- MiniLM\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "tfidf_embedding = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BAAI-BGE available providers: {baai_embedding.model.model.get_providers()}\")\n",
    "print(f\"MiniLM available providers: {mini_embedding.model.model.get_providers()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True # Suppresses warnings in ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "### LLM-based Models\n",
    "First, we evaluate GPT and ModernBERT models using interim data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets_interim.items():\n",
    "    # Split data\n",
    "    train_data = dataset.sample(n=800)\n",
    "    test_data = dataset.drop(train_data.index).sample(n=4000)\n",
    "\n",
    "    # GPT Classifier\n",
    "    \"\"\"\n",
    "    llm_classifier = LlmClassifier(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        api_base=os.getenv(\"PROXY_URL\"),\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        domain=domain,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "    )\n",
    "    \"\"\"\n",
    "    llm_classifier = LlmClassifier(\n",
    "        api_key='',\n",
    "        api_base='http://localhost:11434',\n",
    "        model_name='ollama_chat/llama3.2:3b',\n",
    "        domain=domain,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "    )\n",
    "    try:\n",
    "        # DSPy optimization\n",
    "        llm_classifier.optimize_model()\n",
    "\n",
    "        # Get predictions and metrics for test data\n",
    "        test_predictions, test_actuals, test_latency = llm_classifier.predict()\n",
    "        test_latency = statistics.mean(test_latency)\n",
    "\n",
    "        test_acc = metrics.accuracy_score(test_actuals, test_predictions)\n",
    "\n",
    "        # Evaluate and save model\n",
    "        evaluate(\n",
    "            predictions=test_predictions,\n",
    "            true_labels=test_actuals,\n",
    "            domain=domain,\n",
    "            model_name=\"gpt4o-mini\",\n",
    "            embed_model=\"ada-002\",\n",
    "            cost=llm_classifier.cost,\n",
    "            latency=test_latency,\n",
    "            train_acc=test_acc\n",
    "        )\n",
    "\n",
    "        llm_classifier.save_model(f\"models/gpt-4o-mini-{domain}.json\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running GPT model: {e}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        test_data = dataset.sample(n=30_000)\n",
    "        # ModernBERT Classifier\n",
    "        bert_classifier = ModernBERTNLI(domain=domain)\n",
    "        bert_classifier.classifier.model.to(\"cuda\")\n",
    "\n",
    "        # Test predictions\n",
    "        test_predictions = []\n",
    "        test_times = []\n",
    "        for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "            pred = bert_classifier.predict(row[\"prompt\"])\n",
    "            test_predictions.append(pred)\n",
    "            test_times.append(time.perf_counter_ns() - start_time)\n",
    "\n",
    "        print(test_predictions)\n",
    "        test_acc = metrics.accuracy_score(test_data[\"label\"], test_predictions)\n",
    "        mean_prediction_time = statistics.mean(test_times)\n",
    "\n",
    "        # Evaluate ModernBERT\n",
    "        evaluate(\n",
    "            predictions=test_predictions,\n",
    "            true_labels=test_data[\"label\"],\n",
    "            domain=domain,\n",
    "            model_name=\"modernbert\",\n",
    "            embed_model=\"bert-base\",\n",
    "            latency=mean_prediction_time,\n",
    "            train_acc=test_acc\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ModernBERT model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML Models\n",
    "\n",
    "Evaluate SVM, XGBoost and FastText using processed data with different embedding approaches:\n",
    "- BAAI-BGE: Dense semantic embeddings\n",
    "- MiniLM: Lightweight sentence embeddings\n",
    "- TF-IDF: Sparse word frequency embeddings\n",
    "- FastText: Custom subword embeddings\n",
    "\n",
    "Models are trained on 70% of data and evaluated on remaining 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tf_idf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BAAI-BGE available providers: {baai_embedding.model.model.get_providers()}\")\n",
    "print(f\"MiniLM available providers: {mini_embedding.model.model.get_providers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets.items():\n",
    "    train_data = dataset.sample(frac=0.7).reset_index(drop=True)\n",
    "    test_data = dataset.drop(train_data.index).reset_index(drop=True)\n",
    "\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    prediction_times = []\n",
    "\n",
    "    # fastText\n",
    "    try:\n",
    "        fasttext_classifier = FastTextClassifier(train_data=train_data, test_data=test_data)\n",
    "        fasttext_classifier.train()\n",
    "\n",
    "        train_predictions = []\n",
    "        for _, row in train_data.iterrows():\n",
    "            query = str(row[\"prompt\"]).replace(\"\\n\", \"\")\n",
    "            prediction = fasttext_classifier.model.predict(query)\n",
    "            train_predictions.append(1 if prediction[0][0] == \"__label__1\" else 0)\n",
    "\n",
    "        train_acc = metrics.accuracy_score(train_data[\"label\"], train_predictions)\n",
    "\n",
    "        for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "            text = str(row[\"prompt\"])\n",
    "            query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "            start_time = time.perf_counter_ns()\n",
    "            prediction = fasttext_classifier.model.predict(query)\n",
    "            end_time = time.perf_counter_ns()\n",
    "\n",
    "            prediction_times.append(end_time - start_time)\n",
    "\n",
    "            if prediction[0][0] == \"__label__1\":\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "\n",
    "            actuals.append(row[\"label\"])\n",
    "\n",
    "        mean_prediction_time = statistics.mean(prediction_times)\n",
    "\n",
    "        evaluate(\n",
    "            predictions,\n",
    "            true_labels=actuals,\n",
    "            domain=domain,\n",
    "            model_name=\"fastText\",\n",
    "            embed_model=\"fastText\",\n",
    "            latency=mean_prediction_time,\n",
    "            train_acc=train_acc,\n",
    "        )\n",
    "\n",
    "        fasttext_classifier.model.save_model(f\"models/fastText_{domain}_fasttext.bin\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running fastText model: {e}\")\n",
    "\n",
    "    for model_name, embedding_model in embedding_models.items():\n",
    "        embed_times: float = None\n",
    "\n",
    "        # Add timing for embedding creation\n",
    "        if model_name == \"tf_idf\":\n",
    "            embedding_model.fit(train_data[\"prompt\"])\n",
    "\n",
    "            with open(f\"models/tfidf_{domain}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(embedding_model, f)\n",
    "\n",
    "            # Time the transform process for training data\n",
    "            start_time = time.perf_counter_ns()\n",
    "            train_embeds = embedding_model.transform(train_data[\"prompt\"])\n",
    "            test_embeds = embedding_model.transform(test_data[\"prompt\"])\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_times = end_time - start_time\n",
    "        else:\n",
    "            # Time the embedding process for training data\n",
    "            start_time = time.perf_counter_ns()\n",
    "            train_embeds = np.array(list(embedding_model.embed(train_data[\"prompt\"])))\n",
    "            test_embeds = np.array(list(embedding_model.embed(test_data[\"prompt\"])))\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_times = end_time - start_time\n",
    "\n",
    "        mean_embed_time = embed_times / len(train_data + test_data)\n",
    "\n",
    "        try:\n",
    "            # Train and evaluate SVM model\n",
    "            train_and_evaluate_model(\n",
    "                model_name=\"SVM\",\n",
    "                train_embeds=train_embeds,\n",
    "                test_embeds=test_embeds,\n",
    "                train_labels=train_data[\"label\"],\n",
    "                test_labels=test_data[\"label\"],\n",
    "                domain=domain,\n",
    "                embed_model=model_name,\n",
    "                save_path=f\"models/SVM_{domain}_{model_name}.pkl\",\n",
    "                embedding_time=mean_embed_time  # Pass the embedding time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error running SVM model: {e}\")\n",
    "\n",
    "        try:\n",
    "            # Train and evaluate XGBoost model\n",
    "            train_and_evaluate_model(\n",
    "                model_name=\"XGBoost\",\n",
    "                train_embeds=train_embeds,\n",
    "                test_embeds=test_embeds,\n",
    "                train_labels=train_data[\"label\"],\n",
    "                test_labels=test_data[\"label\"],\n",
    "                domain=domain,\n",
    "                embed_model=model_name,\n",
    "                save_path=f\"models/XGBoost_{domain}_{model_name}.json\",\n",
    "                embedding_time=mean_embed_time  # Pass the embedding time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error running XGBoost model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
