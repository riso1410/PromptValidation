{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Classification Model Training\n",
    "\n",
    "This notebook implements the training pipeline for multiple classification models to identify domain-specific prompts. We compare different approaches:\n",
    "\n",
    "## Models Evaluated\n",
    "- **GPT-based classifier**: Using LLM for zero/few-shot classification\n",
    "- **ModernBERT NLI**: Fine-tuned BERT model for Natural Language Inference\n",
    "- **SVM**: Using different text embeddings (BAAI-BGE, MiniLM, TF-IDF)\n",
    "- **XGBoost**: Using different text embeddings\n",
    "- **FastText**: Specialized text classification model\n",
    "\n",
    "## Domains\n",
    "- Law\n",
    "- Healthcare\n",
    "- Finance\n",
    "\n",
    "## Evaluation Metrics\n",
    "- Accuracy\n",
    "- Latency\n",
    "- Cost (for LLM-based approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "import torch\n",
    "from accelerate.data_loader import DataLoader\n",
    "from datasets import ClassLabel, Dataset\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.chdir('..')\n",
    "from prompt_classifier.metrics import evaluate_run\n",
    "from prompt_classifier.modeling.dspy_llm import LlmClassifier\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "from prompt_classifier.modeling.widemlp import MLP, prepare_inputs\n",
    "from prompt_classifier.util import create_domain_dataset, train_and_evaluate_model\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Acceleration\n",
    "\n",
    "Check available ONNX Runtime providers for hardware acceleration (CPU/CUDA).\n",
    "This affects the performance of embedding models and ModernBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory tracking\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "        print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available ONNX Runtime providers (CPU, CUDA etc.)\n",
    "providers = ort.get_available_providers()\n",
    "\n",
    "print(providers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "### Training Data Types\n",
    "1. **Processed Data**: Clean, filtered dataset used for traditional ML models\n",
    "2. **Interim Data**: Raw/intermediate data used for LLM experiments\n",
    "\n",
    "Each domain dataset is balanced with positive samples from target domain and negative samples from other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "law_prompts = pd.read_csv(\"data/processed/law_prompts.csv\")\n",
    "healthcare_prompts = pd.read_csv(\"data/processed/healthcare_prompts.csv\")\n",
    "finance_prompts = pd.read_csv(\"data/processed/finance_prompts.csv\")\n",
    "\n",
    "law_dataset = create_domain_dataset(law_prompts, [healthcare_prompts, finance_prompts])\n",
    "\n",
    "healthcare_dataset = create_domain_dataset(\n",
    "    healthcare_prompts, [law_prompts, finance_prompts]\n",
    ")\n",
    "\n",
    "finance_dataset = create_domain_dataset(\n",
    "    finance_prompts, [law_prompts, healthcare_prompts]\n",
    ")\n",
    "\n",
    "datasets = {\n",
    "    \"law\": law_dataset,\n",
    "    \"healthcare\": healthcare_dataset,\n",
    "    \"finance\": finance_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0(Finance) 1(Law) 2(Health) 3(Uncertain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_prompts[\"label\"] = 0\n",
    "law_prompts[\"label\"] = 1\n",
    "healthcare_prompts[\"label\"] = 2\n",
    "\n",
    "combined_df = pd.concat([finance_prompts, healthcare_prompts, law_prompts], ignore_index=True)\n",
    "combined_dataset = Dataset.from_pandas(combined_df)\n",
    "combined_dataset = combined_dataset.cast_column(\n",
    "    \"label\", ClassLabel(num_classes=3, names=[\"finance\", \"health\", \"law\"])\n",
    ")\n",
    "combined_dataset = combined_dataset.train_test_split(\n",
    "    test_size=0.15, stratify_by_column=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = combined_dataset[\"train\"]\n",
    "test_dataset = combined_dataset[\"test\"]\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle class imbalance\n",
    "labels = combined_dataset[\"train\"][\"label\"]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "total_samples = len(labels)\n",
    "\n",
    "# Compute weights inversely proportional to class frequencies\n",
    "class_weights = total_samples / (len(unique) * counts)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Class weights:\")\n",
    "print(f\"Class 0 (finance): {class_weights[0]:.4f}\")\n",
    "print(f\"Class 1 (health): {class_weights[1]:.4f}\")\n",
    "print(f\"Class 2 (law): {class_weights[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interim datasets\n",
    "law_prompts_interim = pd.read_csv(\"data/interim/law_prompts.csv\")\n",
    "healthcare_prompts_interim = pd.read_csv(\"data/interim/healthcare_prompts.csv\")\n",
    "finance_prompts_interim = pd.read_csv(\"data/interim/finance_prompts.csv\")\n",
    "\n",
    "law_dataset_interim = create_domain_dataset(\n",
    "    law_prompts_interim, [healthcare_prompts_interim, finance_prompts_interim]\n",
    ")\n",
    "\n",
    "healthcare_dataset_interim = create_domain_dataset(\n",
    "    healthcare_prompts_interim, [law_prompts_interim, finance_prompts_interim]\n",
    ")\n",
    "\n",
    "finance_dataset_interim = create_domain_dataset(\n",
    "    finance_prompts_interim, [law_prompts_interim, healthcare_prompts_interim]\n",
    ")\n",
    "\n",
    "datasets_interim = {\n",
    "    \"law\": law_dataset_interim,\n",
    "    \"healthcare\": healthcare_dataset_interim,\n",
    "    \"finance\": finance_dataset_interim,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Embedding Models\n",
    "\n",
    "Set up text embedding models:\n",
    "- BAAI BGE Small\n",
    "- MiniLM\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "tfidf_embedding = TfidfVectorizer(\n",
    "    max_features=20_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BAAI-BGE available providers: {baai_embedding.model.model.get_providers()}\")\n",
    "print(f\"MiniLM available providers: {mini_embedding.model.model.get_providers()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True  # Suppresses warnings in ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "### LLM-based Models\n",
    "First, we evaluate GPT and ModernBERT models using interim data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets_interim.items():\n",
    "    # GPT Classifier\n",
    "    train_data = dataset.sample(n=800)\n",
    "    test_data = dataset.drop(train_data.index).sample(n=4000)\n",
    "    \"\"\"\n",
    "    llm_classifier = LlmClassifier(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        api_base=os.getenv(\"PROXY_URL\"),\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        domain=domain,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "    )\n",
    "    \"\"\"\n",
    "    llm_classifier = LlmClassifier(\n",
    "        api_key=\"\",\n",
    "        api_base=\"http://localhost:11434\",\n",
    "        model_name=\"ollama_chat/qwen2.5:14b\",\n",
    "        domain=domain,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "    )\n",
    "    try:\n",
    "        # DSPy optimization\n",
    "        llm_classifier.optimize_model()\n",
    "\n",
    "        # Get predictions and metrics for test data\n",
    "        test_predictions, test_actuals, test_latency = llm_classifier.predict()\n",
    "        test_latency = statistics.mean(test_latency)\n",
    "\n",
    "        test_acc = metrics.accuracy_score(test_actuals, test_predictions)\n",
    "\n",
    "        # evaluate_run and save model\n",
    "        evaluate_run(\n",
    "            predictions=test_predictions,\n",
    "            true_labels=test_actuals,\n",
    "            domain=domain,\n",
    "            model_name=\"qwen2.5:14b\",\n",
    "            embed_model=\"qwen-base\",\n",
    "            cost=llm_classifier.cost,\n",
    "            latency=test_latency,\n",
    "            train_acc=test_acc,\n",
    "            training=True,\n",
    "        )\n",
    "\n",
    "        llm_classifier.save_model(f\"models/qwen2.5:14b_{domain}.json\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running LLM model: {e}\")\n",
    "\n",
    "    # ModernBERT Classifier\n",
    "    try:\n",
    "        test_data = dataset.sample(n=30_000)\n",
    "        bert_classifier = ModernBERTNLI(domain=domain)\n",
    "        bert_classifier.classifier.model.to(\"cuda\")\n",
    "\n",
    "        # Test predictions\n",
    "        test_predictions = []\n",
    "        test_times = []\n",
    "        for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "            pred = bert_classifier.predict(row[\"prompt\"])\n",
    "            test_predictions.append(pred)\n",
    "            test_times.append(time.perf_counter_ns() - start_time)\n",
    "\n",
    "        print(test_predictions)\n",
    "        test_acc = metrics.accuracy_score(test_data[\"label\"], test_predictions)\n",
    "        mean_prediction_time = statistics.mean(test_times)\n",
    "\n",
    "        # evaluate_run ModernBERT\n",
    "        evaluate_run(\n",
    "            predictions=test_predictions,\n",
    "            true_labels=test_data[\"label\"],\n",
    "            domain=domain,\n",
    "            model_name=\"modernbert\",\n",
    "            embed_model=\"bert-base\",\n",
    "            latency=mean_prediction_time,\n",
    "            train_acc=test_acc,\n",
    "            training=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ModernBERT model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML Models\n",
    "\n",
    "Evaluate SVM, XGBoost and FastText using processed data with different embedding approaches:\n",
    "- BAAI-BGE: Dense semantic embeddings\n",
    "- MiniLM: Lightweight sentence embeddings\n",
    "- TF-IDF: Sparse word frequency embeddings\n",
    "- FastText: Custom subword embeddings\n",
    "\n",
    "Models are trained on 70% of data and evaluated on remaining 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tf_idf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BAAI-BGE available providers: {baai_embedding.model.model.get_providers()}\")\n",
    "print(f\"MiniLM available providers: {mini_embedding.model.model.get_providers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets.items():\n",
    "    train_data = dataset.sample(frac=0.7).reset_index(drop=True)\n",
    "    test_data = dataset.drop(train_data.index).reset_index(drop=True)\n",
    "\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    prediction_times = []\n",
    "\n",
    "    for model_name, embedding_model in embedding_models.items():\n",
    "        embed_times: float = None\n",
    "\n",
    "        # Add timing for embedding creation\n",
    "        if model_name == \"tf_idf\":\n",
    "            # Fit on training data only\n",
    "            embedding_model.fit(train_data[\"prompt\"])\n",
    "\n",
    "            with open(f\"models/tfidf_{domain}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(embedding_model, f)\n",
    "\n",
    "            start_time = time.perf_counter_ns()\n",
    "            # Convert sparse matrices to dense for consistency\n",
    "            train_embeds = embedding_model.transform(train_data[\"prompt\"])\n",
    "            test_embeds = embedding_model.transform(test_data[\"prompt\"])\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_times = end_time - start_time\n",
    "        else:\n",
    "            # Time the embedding process for training data\n",
    "            start_time = time.perf_counter_ns()\n",
    "            train_embeds = np.array(list(embedding_model.embed(train_data[\"prompt\"])))\n",
    "            test_embeds = np.array(list(embedding_model.embed(test_data[\"prompt\"])))\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_times = end_time - start_time\n",
    "\n",
    "        mean_embed_time = embed_times / len(train_data + test_data)\n",
    "\n",
    "        # Verify shapes\n",
    "        print(f\"Training {model_name} embeddings on {domain} domain\")\n",
    "        print(f\"Train shape: {train_embeds.shape}, Test shape: {test_embeds.shape}\")\n",
    "        print(type(train_embeds))\n",
    "\n",
    "        try:\n",
    "            # Train and evaluate SVM model\n",
    "            train_and_evaluate_model(\n",
    "                model_name=\"SVM\",\n",
    "                train_embeds=train_embeds,\n",
    "                test_embeds=test_embeds,\n",
    "                train_labels=train_data[\"label\"],\n",
    "                test_labels=test_data[\"label\"],\n",
    "                domain=domain,\n",
    "                embed_model=model_name,\n",
    "                save_path=f\"models/SVM_{domain}_{model_name}.pkl\",\n",
    "                embedding_time=mean_embed_time,\n",
    "                training=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error running SVM model: {e}\")\n",
    "\n",
    "        try:\n",
    "            # Train and evaluate XGBoost model\n",
    "            train_and_evaluate_model(\n",
    "                model_name=\"XGBoost\",\n",
    "                train_embeds=train_embeds,\n",
    "                test_embeds=test_embeds,\n",
    "                train_labels=train_data[\"label\"],\n",
    "                test_labels=test_data[\"label\"],\n",
    "                domain=domain,\n",
    "                embed_model=model_name,\n",
    "                save_path=f\"models/XGBoost_{domain}_{model_name}.json\",\n",
    "                embedding_time=mean_embed_time,\n",
    "                training=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error running XGBoost model: {e}\")\n",
    "\n",
    "    # fastText\n",
    "    try:\n",
    "        fasttext_classifier = FastTextClassifier(\n",
    "            train_data=train_data, test_data=test_data\n",
    "        )\n",
    "        fasttext_classifier.train()\n",
    "\n",
    "        train_predictions = []\n",
    "        for _, row in train_data.iterrows():\n",
    "            query = str(row[\"prompt\"]).replace(\"\\n\", \"\")\n",
    "            prediction = fasttext_classifier.model.predict(query)\n",
    "            train_predictions.append(1 if prediction[0][0] == \"__label__1\" else 0)\n",
    "\n",
    "        train_acc = metrics.accuracy_score(train_data[\"label\"], train_predictions)\n",
    "\n",
    "        for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "            text = str(row[\"prompt\"])\n",
    "            query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "            start_time = time.perf_counter_ns()\n",
    "            prediction = fasttext_classifier.model.predict(query)\n",
    "            end_time = time.perf_counter_ns()\n",
    "\n",
    "            prediction_times.append(end_time - start_time)\n",
    "\n",
    "            if prediction[0][0] == \"__label__1\":\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "\n",
    "            actuals.append(row[\"label\"])\n",
    "\n",
    "        mean_prediction_time = statistics.mean(prediction_times)\n",
    "\n",
    "        evaluate_run(\n",
    "            predictions,\n",
    "            true_labels=actuals,\n",
    "            domain=domain,\n",
    "            model_name=\"fastText\",\n",
    "            embed_model=\"fastText\",\n",
    "            latency=mean_prediction_time,\n",
    "            train_acc=train_acc,\n",
    "            training=True,\n",
    "        )\n",
    "\n",
    "        fasttext_classifier.model.save_model(f\"models/fastText_{domain}_fasttext.bin\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running fastText model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "vocab_size = 20_000\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def evaluate_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            texts = batch['prompt']\n",
    "            labels = torch.tensor(batch['label'], device=device)\n",
    "\n",
    "            # Tokenize and prepare inputs\n",
    "            encoded = tokenizer(texts, padding=True, truncation=True)\n",
    "            input_ids = torch.tensor(encoded['input_ids'], device=device)\n",
    "            flat_inputs, offsets = prepare_inputs(input_ids, device)\n",
    "\n",
    "            loss, outputs = model(flat_inputs, offsets, labels)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actuals.extend(labels.cpu().tolist())\n",
    "\n",
    "    metrics_dict = {\n",
    "        'loss': running_loss / len(data_loader),\n",
    "        'accuracy': metrics.accuracy_score(actuals, predictions),\n",
    "        'macro_f1': metrics.f1_score(actuals, predictions, average='macro'),\n",
    "        'weighted_f1': metrics.f1_score(actuals, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metrics(metrics_dict):\n",
    "    return ' | '.join([f\"{k}: {v:.4f}\" for k,v in metrics_dict.items()])\n",
    "\n",
    "# Initialize model with 3 classes\n",
    "model = MLP(vocab_size=tokenizer.vocab_size, num_classes=3, problem_type=\"multi_label_classification\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Training phase\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        for i, batch in pbar:\n",
    "            texts = batch['prompt']\n",
    "            labels = torch.tensor(batch['label'], device=device)\n",
    "\n",
    "            # Tokenize and prepare inputs\n",
    "            encoded = tokenizer(texts, padding=True, truncation=True)\n",
    "            input_ids = torch.tensor(encoded['input_ids'], device=device)\n",
    "            flat_inputs, offsets = prepare_inputs(input_ids, device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, _ = model(flat_inputs, offsets, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Evaluation phase\n",
    "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    train_metrics = evaluate_epoch(model, train_dataloader, device)\n",
    "    print(f\"Train | {format_metrics(train_metrics)}\")\n",
    "    val_metrics = evaluate_epoch(model, test_dataloader, device)\n",
    "    print(f\"Test  | {format_metrics(val_metrics)}\\n\")\n",
    "\n",
    "    scheduler.step(val_metrics['loss'])\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Learning rate: {current_lr:.2e}\\n\")\n",
    "\n",
    "    # Save model if it's the best so far based on validation loss\n",
    "    if not hasattr(model, 'best_val_loss') or val_metrics['loss'] < model.best_val_loss:\n",
    "        model.best_val_loss = val_metrics['loss']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_metrics': val_metrics,\n",
    "        }, 'models_5/mlp_best_model.pt')\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(\"Best model saved to: models/mlp_best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-validation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
