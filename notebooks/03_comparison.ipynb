{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "pridat timer aj pri embeddingu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from prompt_classifier.metrics import evaluate\n",
    "from prompt_classifier.modeling.dspy_gpt import GPT4oMini\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = ort.get_available_providers()\n",
    "\n",
    "print(providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_prompts = pd.read_csv(\"data/processed/law_prompts.csv\")\n",
    "general_prompts = pd.read_csv(\"data/processed/general_prompts.csv\")\n",
    "healthcare_prompts = pd.read_csv(\"data/processed/healthcare_prompts.csv\")\n",
    "finance_prompts = pd.read_csv(\"data/processed/finance_prompts.csv\")\n",
    "\n",
    "law_dataset = (\n",
    "    pd.concat([law_prompts, general_prompts]).sample(frac=1).reset_index(drop=True)\n",
    ")\n",
    "healthcare_dataset = (\n",
    "    pd.concat([healthcare_prompts, general_prompts])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "finance_dataset = (\n",
    "    pd.concat([finance_prompts, general_prompts]).sample(frac=1).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "datasets = {\n",
    "    \"law\": law_dataset,\n",
    "    \"healthcare\": healthcare_dataset,\n",
    "    \"finance\": finance_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_prompts_interim = pd.read_csv(\"data/interim/law_prompts.csv\")\n",
    "general_prompts_interim = pd.read_csv(\"data/interim/general_prompts.csv\")\n",
    "healthcare_prompts_interim = pd.read_csv(\"data/interim/healthcare_prompts.csv\")\n",
    "finance_prompts_interim = pd.read_csv(\"data/interim/finance_prompts.csv\")\n",
    "\n",
    "law_dataset_interim = (\n",
    "    pd.concat([law_prompts_interim, general_prompts_interim])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "healthcare_dataset_interim = (\n",
    "    pd.concat([healthcare_prompts_interim, general_prompts_interim])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "finance_dataset_interim = (\n",
    "    pd.concat([finance_prompts_interim, general_prompts_interim])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "datasets_interim = {\n",
    "    \"law\": law_dataset_interim,\n",
    "    \"healthcare\": healthcare_dataset_interim,\n",
    "    \"finance\": finance_dataset_interim,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "tfidf_embedding = TfidfVectorizer()\n",
    "\n",
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tfidf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BAAI-BGE available providers: {baai_embedding.model.model.get_providers()}\")\n",
    "print(f\"MiniLM available providers: {mini_embedding.model.model.get_providers()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True # Suppresses warnings in ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT and ModernBERT loop using interim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets_interim.items():\n",
    "    # Split data\n",
    "    train_data = dataset.sample(n=800)\n",
    "    test_data = dataset.drop(train_data.index).sample(n=4000)\n",
    "\n",
    "    # GPT Classifier\n",
    "    gpt_classifier = GPT4oMini(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        proxy_url=os.getenv(\"PROXY_URL\"),\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        domain=domain,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # DSPy optimization\n",
    "        gpt_classifier.optimize_model()\n",
    "        \n",
    "        # Get predictions and metrics for test data\n",
    "        test_predictions, test_actuals, test_latency = gpt_classifier.predict()\n",
    "\n",
    "        test_predictions = [int(pred) for pred in test_predictions]\n",
    "        test_actuals = [int(actual) for actual in test_actuals]\n",
    "        test_acc = metrics.accuracy_score(test_actuals, test_predictions)\n",
    "\n",
    "        # Evaluate and save model\n",
    "        evaluate(\n",
    "            predictions=test_predictions,\n",
    "            true_labels=test_actuals,\n",
    "            domain=domain,\n",
    "            model_name=\"gpt4o-mini\",\n",
    "            embed_model=\"ada-002\",\n",
    "            cost=gpt_classifier.cost,\n",
    "            latency=test_latency,\n",
    "            train_acc=test_acc\n",
    "        )\n",
    "\n",
    "        gpt_classifier.save_model(f\"models/gpt-4o-mini-{domain}.json\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running GPT model: {e}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        test_data = dataset.sample(n=30_000)\n",
    "        # ModernBERT Classifier\n",
    "        bert_classifier = ModernBERTNLI(domain=domain)\n",
    "        bert_classifier.classifier.model.to(\"cuda\")\n",
    "        \n",
    "        # Test predictions\n",
    "        test_predictions = []\n",
    "        test_times = []\n",
    "        for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "            start_time = time.perf_counter_ns()\n",
    "            pred = bert_classifier.predict(row[\"prompt\"])\n",
    "            test_predictions.append(pred)\n",
    "            test_times.append(time.perf_counter_ns() - start_time)\n",
    "\n",
    "        print(test_predictions)\n",
    "        test_acc = metrics.accuracy_score(test_data[\"label\"], test_predictions)\n",
    "        mean_prediction_time = statistics.mean(test_times)\n",
    "\n",
    "        # Evaluate ModernBERT\n",
    "        evaluate(\n",
    "            predictions=test_predictions,\n",
    "            true_labels=test_data[\"label\"],\n",
    "            domain=domain,\n",
    "            model_name=\"modernbert\",\n",
    "            embed_model=\"bert-base\",\n",
    "            latency=mean_prediction_time,\n",
    "            train_acc=test_acc\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error running ModernBERT model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM, fastText and XGBoost loop using processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model_name: str,\n",
    "    train_embeds: np.ndarray,\n",
    "    test_embeds: np.ndarray,\n",
    "    train_labels: pd.Series,\n",
    "    test_labels: pd.Series,\n",
    "    domain: str,\n",
    "    embed_model: str,\n",
    "    save_path: str,\n",
    "    embedding_time: float = 0.0,\n",
    ") -> None:\n",
    "\n",
    "    # Initialize the classifier\n",
    "    if model_name == \"SVM\":\n",
    "        classifier = SVC(probability=True)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        classifier = XGBClassifier(n_jobs=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_name. Choose 'SVM' or 'XGBoost'.\")\n",
    "\n",
    "    print(f\"Training {embed_model} embeddings on {domain} domain using {model_name}\")\n",
    "\n",
    "    # Train the model\n",
    "    classifier.fit(train_embeds, train_labels)\n",
    "\n",
    "    train_predictions = classifier.predict(train_embeds)\n",
    "    train_acc = metrics.accuracy_score(train_labels, train_predictions)\n",
    "\n",
    "    predictions = []\n",
    "    prediction_times = []\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    for _, test_embed in enumerate(\n",
    "        tqdm(test_embeds, desc=f\"Evaluating {model_name} on {domain}\")\n",
    "    ):\n",
    "        start_time = time.perf_counter_ns()\n",
    "        prediction = classifier.predict(test_embed.reshape(1, -1))\n",
    "        end_time = time.perf_counter_ns()\n",
    "\n",
    "        prediction_times.append(end_time - start_time)\n",
    "        predictions.append(prediction[0])\n",
    "\n",
    "    mean_prediction_time = statistics.mean(prediction_times)\n",
    "    total_latency = mean_prediction_time + (embedding_time / len(test_embeds))\n",
    "\n",
    "    # Save the model\n",
    "    try:\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            pickle.dump(classifier, file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "    # Evaluate the predictions\n",
    "    evaluate(\n",
    "        predictions,\n",
    "        test_labels,\n",
    "        domain,\n",
    "        model_name=model_name,\n",
    "        embed_model=embed_model,\n",
    "        latency=total_latency,\n",
    "        train_acc=train_acc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tf_idf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BAAI-BGE available providers: {baai_embedding.model.model.get_providers()}\")\n",
    "print(f\"MiniLM available providers: {mini_embedding.model.model.get_providers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets.items():\n",
    "    train_data = dataset.sample(frac=0.7).reset_index(drop=True)\n",
    "    test_data = dataset.drop(train_data.index).reset_index(drop=True)\n",
    "\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    prediction_times = []\n",
    "\n",
    "    # fastText\n",
    "    try:\n",
    "        fasttext_classifier = FastTextClassifier(train_data=train_data, test_data=test_data)\n",
    "        fasttext_classifier.train()\n",
    "\n",
    "        train_predictions = []\n",
    "        for _, row in train_data.iterrows():\n",
    "            query = str(row[\"prompt\"]).replace(\"\\n\", \"\")\n",
    "            prediction = fasttext_classifier.model.predict(query)\n",
    "            train_predictions.append(1 if prediction[0][0] == \"__label__1\" else 0)\n",
    "\n",
    "        train_acc = metrics.accuracy_score(train_data[\"label\"], train_predictions)\n",
    "\n",
    "        for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "            text = str(row[\"prompt\"])\n",
    "            query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "            start_time = time.perf_counter_ns()\n",
    "            prediction = fasttext_classifier.model.predict(query)\n",
    "            end_time = time.perf_counter_ns()\n",
    "\n",
    "            prediction_times.append(end_time - start_time)\n",
    "\n",
    "            if prediction[0][0] == \"__label__1\":\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "\n",
    "            actuals.append(row[\"label\"])\n",
    "\n",
    "        mean_prediction_time = statistics.mean(prediction_times)\n",
    "\n",
    "        evaluate(\n",
    "            predictions,\n",
    "            true_labels=actuals,\n",
    "            domain=domain,\n",
    "            model_name=\"fastText\",\n",
    "            embed_model=\"fastText\",\n",
    "            latency=mean_prediction_time,\n",
    "            train_acc=train_acc,\n",
    "        )\n",
    "        \n",
    "        fasttext_classifier.model.save_model(f\"models/fastText_{domain}_fasttext.bin\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error running fastText model: {e}\")\n",
    "\n",
    "    for model_name, embedding_model in embedding_models.items():\n",
    "        embed_train_times = []\n",
    "        embed_test_times = []\n",
    "        \n",
    "        # Add timing for embedding creation\n",
    "        if model_name == \"tf_idf\":\n",
    "            # Time the fitting process\n",
    "            start_time = time.perf_counter_ns()\n",
    "            embedding_model.fit(train_data[\"prompt\"])\n",
    "            end_time = time.perf_counter_ns()\n",
    "            fit_time = end_time - start_time\n",
    "            print(f\"TF-IDF fitting time: {fit_time/1e9:.2f} seconds\")\n",
    "            \n",
    "            with open(f\"models/tfidf_{domain}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(embedding_model, f)\n",
    "            \n",
    "            # Time the transform process for training data\n",
    "            start_time = time.perf_counter_ns()\n",
    "            train_embeds = embedding_model.transform(train_data[\"prompt\"])\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_train_times.append(end_time - start_time)\n",
    "            \n",
    "            # Time the transform process for test data\n",
    "            start_time = time.perf_counter_ns()\n",
    "            test_embeds = embedding_model.transform(test_data[\"prompt\"])\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_test_times.append(end_time - start_time)\n",
    "        else:\n",
    "            # Time the embedding process for training data\n",
    "            start_time = time.perf_counter_ns()\n",
    "            train_embeds = np.array(list(embedding_model.embed(train_data[\"prompt\"])))\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_train_times.append(end_time - start_time)\n",
    "            \n",
    "            # Time the embedding process for test data\n",
    "            start_time = time.perf_counter_ns()\n",
    "            test_embeds = np.array(list(embedding_model.embed(test_data[\"prompt\"])))\n",
    "            end_time = time.perf_counter_ns()\n",
    "            embed_test_times.append(end_time - start_time)\n",
    "        \n",
    "        # Calculate average embedding times\n",
    "        mean_train_embed_time = statistics.mean(embed_train_times)\n",
    "        mean_test_embed_time = statistics.mean(embed_test_times)\n",
    "        print(f\"{model_name} embedding time - Train: {mean_train_embed_time/1e9:.2f}s, Test: {mean_test_embed_time/1e9:.2f}s\")\n",
    "        \n",
    "        # Continue with model training and evaluation\n",
    "        try:\n",
    "            # Train and evaluate SVM model\n",
    "            train_and_evaluate_model(\n",
    "                model_name=\"SVM\",\n",
    "                train_embeds=train_embeds,\n",
    "                test_embeds=test_embeds,\n",
    "                train_labels=train_data[\"label\"],\n",
    "                test_labels=test_data[\"label\"],\n",
    "                domain=domain,\n",
    "                embed_model=model_name,\n",
    "                save_path=f\"models/SVM_{domain}_{model_name}.pkl\",\n",
    "                embedding_time=mean_test_embed_time  # Pass the embedding time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error running SVM model: {e}\")\n",
    "\n",
    "        try:\n",
    "            # Train and evaluate XGBoost model\n",
    "            train_and_evaluate_model(\n",
    "                model_name=\"XGBoost\",\n",
    "                train_embeds=train_embeds,\n",
    "                test_embeds=test_embeds,\n",
    "                train_labels=train_data[\"label\"],\n",
    "                test_labels=test_data[\"label\"],\n",
    "                domain=domain,\n",
    "                embed_model=model_name,\n",
    "                save_path=f\"models/XGBoost_{domain}_{model_name}.json\",\n",
    "                embedding_time=mean_test_embed_time  # Pass the embedding time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error running XGBoost model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
