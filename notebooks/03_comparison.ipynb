{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risko\\Desktop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from prompt_classifier.metrics import evaluate\n",
    "from prompt_classifier.modeling.dspy_gpt import GPT4oMini\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "from prompt_classifier.modeling.nli_modernbert import ModernBERTNLI\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "providers = ort.get_available_providers()\n",
    "\n",
    "print(providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/processed/law_prompts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m law_prompts \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/processed/law_prompts.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m general_prompts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed/general_prompts.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m healthcare_prompts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed/healthcare_prompts.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\risko\\miniconda3\\envs\\prompt-classification\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\risko\\miniconda3\\envs\\prompt-classification\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\risko\\miniconda3\\envs\\prompt-classification\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\risko\\miniconda3\\envs\\prompt-classification\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\risko\\miniconda3\\envs\\prompt-classification\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/processed/law_prompts.csv'"
     ]
    }
   ],
   "source": [
    "law_prompts = pd.read_csv(\"data/processed/law_prompts.csv\")\n",
    "general_prompts = pd.read_csv(\"data/processed/general_prompts.csv\")\n",
    "healthcare_prompts = pd.read_csv(\"data/processed/healthcare_prompts.csv\")\n",
    "finance_prompts = pd.read_csv(\"data/processed/finance_prompts.csv\")\n",
    "\n",
    "law_dataset = (\n",
    "    pd.concat([law_prompts, general_prompts]).sample(frac=1).reset_index(drop=True)\n",
    ")\n",
    "healthcare_dataset = (\n",
    "    pd.concat([healthcare_prompts, general_prompts])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "finance_dataset = (\n",
    "    pd.concat([finance_prompts, general_prompts]).sample(frac=1).reset_index(drop=True)\n",
    ")\n",
    "\n",
    "datasets = {\n",
    "    \"law\": law_dataset,\n",
    "    \"healthcare\": healthcare_dataset,\n",
    "    \"finance\": finance_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_prompts_interim = pd.read_csv(\"data/interim/law_prompts.csv\")\n",
    "general_prompts_interim = pd.read_csv(\"data/interim/general_prompts.csv\")\n",
    "healthcare_prompts_interim = pd.read_csv(\"data/interim/healthcare_prompts.csv\")\n",
    "finance_prompts_interim = pd.read_csv(\"data/interim/finance_prompts.csv\")\n",
    "\n",
    "law_dataset_interim = (\n",
    "    pd.concat([law_prompts_interim, general_prompts_interim])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "healthcare_dataset_interim = (\n",
    "    pd.concat([healthcare_prompts_interim, general_prompts_interim])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "finance_dataset_interim = (\n",
    "    pd.concat([finance_prompts_interim, general_prompts_interim])\n",
    "    .sample(frac=1)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "datasets_interim = {\n",
    "    \"law\": law_dataset_interim,\n",
    "    \"healthcare\": healthcare_dataset_interim,\n",
    "    \"finance\": finance_dataset_interim,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "tfidf_embedding = TfidfVectorizer()\n",
    "\n",
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tfidf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAAI-BGE available providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "MiniLM available providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"BAAI-BGE available providers: {baai_embedding.model.model.get_providers()}\")\n",
    "print(f\"MiniLM available providers: {mini_embedding.model.model.get_providers()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True # Suppresses warnings in ModernBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT and ModernBERT loop using interim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets_interim.items():\n",
    "    # Split data\n",
    "    train_data = dataset.sample(frac=0.00025)\n",
    "    test_data = dataset.drop(train_data.index).head(100)\n",
    "\n",
    "    # GPT Classifier\n",
    "    gpt_classifier = GPT4oMini(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        proxy_url=os.getenv(\"PROXY_URL\"),\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        domain=domain,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "    )\n",
    "\n",
    "    # DSPy optimization\n",
    "    gpt_classifier.optimize_model()\n",
    "\n",
    "    # Get predictions and metrics for train data\n",
    "    train_predictions, train_actuals, train_latency = gpt_classifier.predict(train_data)\n",
    "    train_acc = metrics.accuracy_score(train_actuals, train_predictions)\n",
    "\n",
    "    # Get predictions and metrics for test data\n",
    "    test_predictions, test_actuals, test_latency = gpt_classifier.predict(test_data)\n",
    "    \n",
    "    # Evaluate and save model\n",
    "    evaluate(\n",
    "        predictions=test_predictions,\n",
    "        true_labels=test_actuals,\n",
    "        domain=domain,\n",
    "        model_name=\"gpt4o-mini\",\n",
    "        embed_model=\"ada-002\",\n",
    "        cost=gpt_classifier.cost,\n",
    "        latency=test_latency,\n",
    "        train_acc=train_acc\n",
    "    )\n",
    "\n",
    "    gpt_classifier.save_model(f\"models/gpt-4o-mini-{domain}.json\")\n",
    "\n",
    "    # ModernBERT Classifier\n",
    "    bert_classifier = ModernBERTNLI(domain=domain)\n",
    "\n",
    "    # Train predictions\n",
    "    train_predictions = []\n",
    "    train_times = []\n",
    "    for _, row in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "        start_time = time.perf_counter_ns()\n",
    "        pred = bert_classifier.predict(row[\"prompt\"])\n",
    "        train_predictions.append(pred)\n",
    "        train_times.append(time.perf_counter_ns() - start_time)\n",
    "\n",
    "    train_acc = metrics.accuracy_score(train_data[\"label\"], train_predictions)\n",
    "\n",
    "    # Test predictions\n",
    "    test_predictions = []\n",
    "    test_times = []\n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        start_time = time.perf_counter_ns()\n",
    "        pred = bert_classifier.predict(row[\"prompt\"])\n",
    "        test_predictions.append(pred)\n",
    "        test_times.append(time.perf_counter_ns() - start_time)\n",
    "\n",
    "    mean_prediction_time = statistics.mean(test_times)\n",
    "\n",
    "    # Evaluate ModernBERT\n",
    "    evaluate(\n",
    "        predictions=test_predictions,\n",
    "        true_labels=test_data[\"label\"],\n",
    "        domain=domain,\n",
    "        model_name=\"modernbert\",\n",
    "        embed_model=\"bert-base\",\n",
    "        latency=mean_prediction_time,\n",
    "        train_acc=train_acc\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM, fastText and XGBoost loop using processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model_name: str,\n",
    "    train_embeds: np.ndarray,\n",
    "    test_embeds: np.ndarray,\n",
    "    train_labels: pd.Series,\n",
    "    test_labels: pd.Series,\n",
    "    domain: str,\n",
    "    embed_model: str,\n",
    "    save_path: str,\n",
    ") -> None:\n",
    "\n",
    "    # Initialize the classifier\n",
    "    if model_name == \"SVM\":\n",
    "        classifier = SVC(probability=True)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        classifier = XGBClassifier(n_jobs=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_name. Choose 'SVM' or 'XGBoost'.\")\n",
    "\n",
    "    print(f\"Training {embed_model} embeddings on {domain} domain using {model_name}\")\n",
    "\n",
    "    # Train the model\n",
    "    classifier.fit(train_embeds, train_labels)\n",
    "\n",
    "    train_predictions = classifier.predict(train_embeds)\n",
    "    train_acc = metrics.accuracy_score(train_labels, train_predictions)\n",
    "\n",
    "    predictions = []\n",
    "    prediction_times = []\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    for _, test_embed in enumerate(\n",
    "        tqdm(test_embeds, desc=f\"Evaluating {model_name} on {domain}\")\n",
    "    ):\n",
    "        start_time = time.perf_counter_ns()\n",
    "        prediction = classifier.predict(test_embed.reshape(1, -1))\n",
    "        end_time = time.perf_counter_ns()\n",
    "\n",
    "        prediction_times.append(end_time - start_time)\n",
    "        predictions.append(prediction[0])\n",
    "\n",
    "    mean_prediction_time = statistics.mean(prediction_times)\n",
    "\n",
    "    # Save the model\n",
    "    try:\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            pickle.dump(classifier, file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "    # Evaluate the predictions\n",
    "    evaluate(\n",
    "        predictions,\n",
    "        test_labels,\n",
    "        domain,\n",
    "        model_name=model_name,\n",
    "        embed_model=embed_model,\n",
    "        latency=mean_prediction_time,\n",
    "        train_acc=train_acc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tf_idf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BAAI-BGE available providers: {baai_embedding.model.model.get_providers()}\")\n",
    "print(f\"MiniLM available providers: {mini_embedding.model.model.get_providers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets.items():\n",
    "    train_data = dataset.sample(frac=0.7).reset_index(drop=True)\n",
    "    test_data = dataset.drop(train_data.index).reset_index(drop=True)\n",
    "\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    prediction_times = []\n",
    "\n",
    "    # fastText\n",
    "    fasttext_classifier = FastTextClassifier(train_data=train_data, test_data=test_data)\n",
    "    fasttext_classifier.train()\n",
    "\n",
    "    train_predictions = []\n",
    "    for _, row in train_data.iterrows():\n",
    "        query = str(row[\"prompt\"]).replace(\"\\n\", \"\")\n",
    "        prediction = fasttext_classifier.model.predict(query)\n",
    "        train_predictions.append(1 if prediction[0][0] == \"__label__1\" else 0)\n",
    "\n",
    "    train_acc = metrics.accuracy_score(train_data[\"label\"], train_predictions)\n",
    "\n",
    "    for _, row in tqdm(test_data.iterrows(), total=len(test_data)):\n",
    "        text = str(row[\"prompt\"])\n",
    "        query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "        start_time = time.perf_counter_ns()\n",
    "        prediction = fasttext_classifier.model.predict(query)\n",
    "        end_time = time.perf_counter_ns()\n",
    "\n",
    "        prediction_times.append(end_time - start_time)\n",
    "\n",
    "        if prediction[0][0] == \"__label__1\":\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "\n",
    "        actuals.append(row[\"label\"])\n",
    "\n",
    "    mean_prediction_time = statistics.mean(prediction_times)\n",
    "\n",
    "    evaluate(\n",
    "        predictions,\n",
    "        true_labels=actuals,\n",
    "        domain=domain,\n",
    "        model_name=\"fastText\",\n",
    "        embed_model=\"fastText\",\n",
    "        latency=mean_prediction_time,\n",
    "        train_acc=train_acc,\n",
    "    )\n",
    "    \n",
    "    fasttext_classifier.model.save_model(f\"models/fastText_{domain}_fasttext.bin\")\n",
    "\n",
    "    for model_name, embedding_model in embedding_models.items():\n",
    "        if model_name == \"tf_idf\":\n",
    "            embedding_model.fit(train_data[\"prompt\"])\n",
    "            with open(f\"models/tfidf_{domain}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(embedding_model, f)\n",
    "            train_embeds = embedding_model.transform(train_data[\"prompt\"])\n",
    "            test_embeds = embedding_model.transform(test_data[\"prompt\"])\n",
    "\n",
    "\n",
    "        else:\n",
    "            train_embeds = np.array(list(embedding_model.embed(train_data[\"prompt\"])))\n",
    "            test_embeds = np.array(list(embedding_model.embed(test_data[\"prompt\"])))\n",
    "\n",
    "    \n",
    "        # Train and evaluate SVM model\n",
    "        train_and_evaluate_model(\n",
    "            model_name=\"SVM\",\n",
    "            train_embeds=train_embeds,\n",
    "            test_embeds=test_embeds,\n",
    "            train_labels=train_data[\"label\"],\n",
    "            test_labels=test_data[\"label\"],\n",
    "            domain=domain,\n",
    "            embed_model=model_name,\n",
    "            save_path=f\"models/SVM_{domain}_{model_name}.pkl\",\n",
    "        )\n",
    "\n",
    "        # Train and evaluate XGBoost model\n",
    "        train_and_evaluate_model(\n",
    "            model_name=\"XGBoost\",\n",
    "            train_embeds=train_embeds,\n",
    "            test_embeds=test_embeds,\n",
    "            train_labels=train_data[\"label\"],\n",
    "            test_labels=test_data[\"label\"],\n",
    "            domain=domain,\n",
    "            embed_model=model_name,\n",
    "            save_path=f\"models/XGBoost_{domain}_{model_name}.json\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
