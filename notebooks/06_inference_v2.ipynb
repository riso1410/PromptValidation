{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as ort\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from prompt_classifier.modeling.fasttext import FastTextClassifier\n",
    "\n",
    "load_dotenv()\n",
    "random.seed(22)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Datasets\n",
    "\n",
    "Load various hate speech datasets for evaluation:\n",
    "- Jigsaw Toxicity\n",
    "- OLID\n",
    "- HateXplain\n",
    "- TUKE Slovak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_data = pd.read_csv(\"data/domain_eval.csv\")\n",
    "ood_data = pd.read_csv(\"data/ood_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-04-21 10:15:29.693512774 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-21 10:15:29.693564931 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-04-21 10:15:30.261584792 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-21 10:15:30.261627592 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-04-21 10:15:30.261584792 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-04-21 10:15:30.261627592 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", \n",
    "    providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_finance = pkl.load(open(\"models/tfidf_finance.pkl\", \"rb\"))\n",
    "tfidf_healthcare = pkl.load(open(\"models/tfidf_healthcare.pkl\", \"rb\"))\n",
    "tfidf_law = pkl.load(open(\"models/tfidf_law.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, dataset_name, model_name, embed_type=None, domain=None):\n",
    "    filename = f'data/{dataset_name}_eval.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Create column name with model_embed format\n",
    "    if embed_type:\n",
    "        model_part = f'{model_name}_{embed_type}'\n",
    "    else:\n",
    "        model_part = model_name\n",
    "    \n",
    "    # Add domain with hyphen separator\n",
    "    col_name = f'pred-{model_part}-{domain}' if domain else f'pred-{model_part}'\n",
    "    \n",
    "    df[col_name] = predictions\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain dataset...\n",
      "Processing dataset domain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "  0%|          | 0/9000 [00:00<?, ?it/s]Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 9000/9000 [00:03<00:00, 2840.14it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ood dataset...\n",
      "Processing dataset ood...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "  0%|          | 0/13457 [00:00<?, ?it/s]Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 13457/13457 [00:02<00:00, 6516.84it/s]\n",
      "100%|██████████| 13457/13457 [00:02<00:00, 6516.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# FastText inference\n",
    "for dataset_name, inference_df in {'ood': ood_data}.items():\n",
    "    predictions_ft = []\n",
    "    print(f\"Processing {dataset_name} dataset...\")\n",
    "    try:\n",
    "        actuals_ft = []\n",
    "        prediction_times_ft = []\n",
    "        print(f\"Processing dataset {dataset_name}...\")\n",
    "        try:\n",
    "            # Try to load model with proper error handling\n",
    "            try:\n",
    "                fasttext_classifier_finance = FastTextClassifier(\n",
    "                    train_data=inference_df, test_data=inference_df\n",
    "                )\n",
    "                fasttext_classifier_finance.model = fasttext.load_model(\n",
    "                    \"models/fastText_finance_fasttext.bin\"\n",
    "                )\n",
    "\n",
    "                fasttext_classifier_healthcare = FastTextClassifier(\n",
    "                    train_data=inference_df, test_data=inference_df\n",
    "                )\n",
    "                fasttext_classifier_healthcare.model = fasttext.load_model(\n",
    "                    \"models/fastText_healthcare_fasttext.bin\"\n",
    "                )\n",
    "\n",
    "                fasttext_classifier_law = FastTextClassifier(\n",
    "                    train_data=inference_df, test_data=inference_df\n",
    "                )\n",
    "                fasttext_classifier_law.model = fasttext.load_model(\n",
    "                    \"models/fastText_law_fasttext.bin\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading fastText models: {e}\")\n",
    "                continue\n",
    "\n",
    "            for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "                text = str(row[\"prompt\"])\n",
    "                query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "                try:\n",
    "                    start_time = time.perf_counter_ns()\n",
    "\n",
    "                    # Predictions from all three classifiers\n",
    "                    prediction_finance = fasttext_classifier_finance.model.predict(query)\n",
    "                    prediction_healthcare = fasttext_classifier_healthcare.model.predict(\n",
    "                        query\n",
    "                    )\n",
    "                    prediction_law = fasttext_classifier_law.model.predict(query)\n",
    "\n",
    "                    end_time = time.perf_counter_ns()\n",
    "                    prediction_times_ft.append(end_time - start_time)\n",
    "\n",
    "                    predictions_ft.append(\n",
    "                        0\n",
    "                        if (\n",
    "                            prediction_finance[0][0] == \"__label__1\"\n",
    "                            or prediction_healthcare[0][0] == \"__label__1\"\n",
    "                            or prediction_law[0][0] == \"__label__1\"\n",
    "                        )\n",
    "                        else 1\n",
    "                    )\n",
    "                    actuals_ft.append(row[\"label\"])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {e}\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "            save_predictions(predictions_ft, dataset_name, 'fasttext')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading fastText models: {e}\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset {dataset_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - SVM, XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_names = [\"mini\", \"baai\", \"tf_idf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain dataset with mini embeddings...\n",
      "Processing ood dataset with mini embeddings...\n",
      "Processing ood dataset with mini embeddings...\n",
      "Processing domain dataset with baai embeddings...\n",
      "Processing domain dataset with baai embeddings...\n",
      "Processing ood dataset with baai embeddings...\n",
      "Processing ood dataset with baai embeddings...\n",
      "Processing domain dataset with tf_idf embeddings...\n",
      "Processing domain dataset with tf_idf embeddings...\n",
      "Processing ood dataset with tf_idf embeddings...\n",
      "Processing ood dataset with tf_idf embeddings...\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(embedding_model, texts):\n",
    "    if embedding_model == \"tf_idf\":\n",
    "        return tfidf_finance.transform(texts)\n",
    "    elif embedding_model == \"mini\":\n",
    "        return np.array(list(mini_embedding.embed(texts)))\n",
    "    else:  # baai\n",
    "        return np.array(list(baai_embedding.embed(texts)))\n",
    "\n",
    "def predict_batch(texts, embedding_model, classifiers, batch_size):\n",
    "    predictions = []\n",
    "    embeddings = get_embeddings(embedding_model, texts)\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_embeds = embeddings[i:i + batch_size]\n",
    "        \n",
    "        # Get predictions from all three domains\n",
    "        preds = [clf.predict(batch_embeds) for clf in classifiers]\n",
    "        \n",
    "        # Combine predictions for the batch\n",
    "        batch_preds = np.zeros(batch_embeds.shape[0])\n",
    "        batch_preds[(preds[0] == 1) | (preds[1] == 1) | (preds[2] == 1)] = 0\n",
    "        batch_preds[(preds[0] != 1) & (preds[1] != 1) & (preds[2] != 1)] = 1\n",
    "        \n",
    "        predictions.extend(batch_preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# ML Models inference\n",
    "for embedding_model in embedding_models_names:\n",
    "    for dataset_name, inference_df in {'domain': domain_data, 'ood': ood_data}.items():\n",
    "        print(f\"Processing {dataset_name} dataset with {embedding_model} embeddings...\")\n",
    "        try:\n",
    "            # Load SVM models\n",
    "            svm_classifiers = [\n",
    "                pkl.load(open(f\"models/SVM_{domain}_{embedding_model}.pkl\", \"rb\"))\n",
    "                for domain in [\"finance\", \"healthcare\", \"law\"]\n",
    "            ]\n",
    "            \n",
    "            # Load XGBoost models\n",
    "            xgb_classifiers = []\n",
    "            for domain in [\"finance\", \"healthcare\", \"law\"]:\n",
    "                clf = XGBClassifier()\n",
    "                clf.load_model(f\"models/XGBoost_{domain}_{embedding_model}.json\")\n",
    "                xgb_classifiers.append(clf)\n",
    "            \n",
    "            # Get predictions using batch processing\n",
    "            predictions_svm = predict_batch(\n",
    "                inference_df[\"prompt\"].tolist(), \n",
    "                embedding_model, \n",
    "                svm_classifiers, \n",
    "                batch_size=128\n",
    "            )\n",
    "            \n",
    "            predictions_xgb = predict_batch(\n",
    "                inference_df[\"prompt\"].tolist(), \n",
    "                embedding_model, \n",
    "                xgb_classifiers, \n",
    "                batch_size=128\n",
    "            )\n",
    "            \n",
    "            # Save predictions\n",
    "            save_predictions(predictions_svm, dataset_name, 'svm', embedding_model)\n",
    "            save_predictions(predictions_xgb, dataset_name, 'xgb', embedding_model)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_name} dataset with {embedding_model} embeddings: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tibor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer function for batch processing\n",
    "tokenizer_func = partial(\n",
    "    tokenizer, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded all ONNX models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-04-21 10:22:50.388985667 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 141 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load ONNX models\n",
    "    mlp_classifier = ort.InferenceSession(\n",
    "        \"models/text_classifier_optimized_int8.onnx\",\n",
    "        providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "    )\n",
    "    print(\"Successfully loaded all ONNX models\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ONNX models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_data = pd.read_csv(\"data/domain_eval.csv\")\n",
    "ood_data = pd.read_csv(\"data/ood_eval.csv\")\n",
    "\n",
    "data = {\"domain\": domain_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing domain dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [06:00<00:00, 25.00it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for domain, inference_df in data.items():\n",
    "    print(f\"\\nProcessing {domain} dataset...\")\n",
    "    predictions_mlp = []\n",
    "    prediction_times_mlp = []\n",
    "    actuals_mlp = []\n",
    "\n",
    "    try:\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            # Tokenize input text\n",
    "            start_time = time.perf_counter_ns()\n",
    "            inputs = tokenizer_func(row[\"prompt\"])\n",
    "\n",
    "            # Convert to numpy arrays for ONNX\n",
    "            onnx_inputs = {\n",
    "                'input_ids': inputs['input_ids'].numpy(),\n",
    "                'attention_mask': inputs['attention_mask'].numpy(),\n",
    "            }\n",
    "\n",
    "            # Run inference\n",
    "            pred = mlp_classifier.run(None, onnx_inputs)[0]\n",
    "            end_time = time.perf_counter_ns()\n",
    "            prediction_times_mlp.append(end_time - start_time)\n",
    "            predictions_mlp.append(np.argmax(pred))\n",
    "            \n",
    "\n",
    "        save_predictions(predictions_mlp, domain, 'mlp_onnx')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {domain} dataset: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "  0%|          | 0/9000 [00:00<?, ?it/s]Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 9000/9000 [00:02<00:00, 3078.62it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain dataset with mini embeddings...\n",
      "Processing domain dataset with baai embeddings...\n",
      "Processing domain dataset with baai embeddings...\n",
      "Processing domain dataset with tf_idf embeddings...\n",
      "Processing domain dataset with tf_idf embeddings...\n"
     ]
    }
   ],
   "source": [
    "# FastText domain inference\n",
    "predictions_ft = []\n",
    "for domain, inference_df in {'domain': domain_data}.items():\n",
    "    print(f\"Processing {domain} dataset...\")\n",
    "    try:\n",
    "        actuals_ft = []\n",
    "        prediction_times_ft = []\n",
    "        \n",
    "        # Load domain-specific models\n",
    "        fasttext_classifier_finance = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "        fasttext_classifier_finance.model = fasttext.load_model(\"models/fastText_finance_fasttext.bin\")\n",
    "\n",
    "        fasttext_classifier_healthcare = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "        fasttext_classifier_healthcare.model = fasttext.load_model(\"models/fastText_healthcare_fasttext.bin\")\n",
    "        \n",
    "        fasttext_classifier_law = FastTextClassifier(train_data=inference_df, test_data=inference_df)\n",
    "        fasttext_classifier_law.model = fasttext.load_model(\"models/fastText_law_fasttext.bin\")\n",
    "\n",
    "        for _, row in tqdm(inference_df.iterrows(), total=len(inference_df)):\n",
    "            text = str(row[\"prompt\"])\n",
    "            query = text.replace(\"\\n\", \"\")\n",
    "\n",
    "            try:\n",
    "                start_time = time.perf_counter_ns()\n",
    "                \n",
    "                # Get predictions for each domain\n",
    "                prediction_finance = fasttext_classifier_finance.model.predict(query)\n",
    "                prediction_healthcare = fasttext_classifier_healthcare.model.predict(query)\n",
    "                prediction_law = fasttext_classifier_law.model.predict(query)\n",
    "                \n",
    "                end_time = time.perf_counter_ns()\n",
    "                prediction_times_ft.append(end_time - start_time)\n",
    "\n",
    "                # Store predictions by domain\n",
    "                predictions_ft.append({\n",
    "                    'finance': 1 if prediction_finance[0][0] == \"__label__1\" else 0,\n",
    "                    'healthcare': 1 if prediction_healthcare[0][0] == \"__label__1\" else 0,\n",
    "                    'law': 1 if prediction_law[0][0] == \"__label__1\" else 0\n",
    "                })\n",
    "                actuals_ft.append(row[\"label\"])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Save predictions with domain suffixes\n",
    "        for domain_name in ['finance', 'healthcare', 'law']:\n",
    "            domain_preds = [p[domain_name] for p in predictions_ft]\n",
    "            save_predictions(domain_preds, 'domain', 'fasttext', domain=domain_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dataset: {e}\")\n",
    "        continue\n",
    "\n",
    "# ML Models domain inference \n",
    "for embedding_model in embedding_models_names:\n",
    "    print(f\"Processing domain dataset with {embedding_model} embeddings...\")\n",
    "    predictions_xgb = []\n",
    "    predictions_svm = []\n",
    "\n",
    "    try:\n",
    "        # Get embeddings once per dataset\n",
    "        if embedding_model == \"tf_idf\":\n",
    "            test_embeds = tfidf_finance.transform(domain_data[\"prompt\"])\n",
    "        else:\n",
    "            if embedding_model == \"mini\":\n",
    "                test_embeds = np.array(list(mini_embedding.embed(domain_data[\"prompt\"])))\n",
    "            else:  # baai\n",
    "                test_embeds = np.array(list(baai_embedding.embed(domain_data[\"prompt\"])))\n",
    "\n",
    "        # Load domain-specific models\n",
    "        for model_domain in ['finance', 'healthcare', 'law']:\n",
    "            # Load SVM models\n",
    "            with open(f\"models/SVM_{model_domain}_{embedding_model}.pkl\", \"rb\") as f:\n",
    "                svm_model = pkl.load(f)\n",
    "\n",
    "            # Load XGBoost models\n",
    "            xgb_model = XGBClassifier()\n",
    "            xgb_model.load_model(f\"models/XGBoost_{model_domain}_{embedding_model}.json\")\n",
    "\n",
    "            # Get predictions\n",
    "            svm_preds = svm_model.predict(test_embeds)\n",
    "            xgb_preds = xgb_model.predict(test_embeds)\n",
    "\n",
    "            # Save predictions with domain suffixes\n",
    "            save_predictions(svm_preds, 'domain', 'svm', embedding_model, domain=model_domain)\n",
    "            save_predictions(xgb_preds, 'domain', 'xgb', embedding_model, domain=model_domain)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing embeddings {embedding_model}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
