{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5786d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import dataloader\n",
    "import util\n",
    "\n",
    "util.set_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dataloader.get_domain_data()\n",
    "eval_datasets = dataloader.get_eval_datasets()\n",
    "batch_data = dataloader.get_batch_data()\n",
    "\n",
    "batch_sizes = [1, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "tfidf_embedding = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset = next(iter(datasets.values()))[\"prompt\"]\n",
    "train_prompts = first_dataset.sample(frac=0.8, random_state=22)\n",
    "\n",
    "tfidf_embedding.fit(train_prompts)\n",
    "\n",
    "with open(\"models/tfidf.pkl\", \"wb\") as f:\n",
    "    pkl.dump(tfidf_embedding, f)\n",
    "\n",
    "# Create embedding cache directory\n",
    "os.makedirs(\"cache/embeddings\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tf_idf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7d2b8",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_embeddings(\n",
    "    texts, model_name, domain, cache_dir=\"cache/embeddings\", force_recompute=False\n",
    "):\n",
    "    \"\"\"Get embeddings from cache if available, otherwise compute and cache them.\n",
    "\n",
    "    Args:\n",
    "        texts: The texts to embed\n",
    "        model_name: The name of the embedding model to use\n",
    "        domain: The domain identifier for the cache\n",
    "        cache_dir: Directory to store/retrieve cached embeddings\n",
    "        force_recompute: If True, ignore cache and recompute embeddings\n",
    "\n",
    "    Returns:\n",
    "        The embeddings matrix\n",
    "    \"\"\"\n",
    "    cache_file = f\"{cache_dir}/{domain}_{model_name}_embeddings.pkl\"\n",
    "\n",
    "    # Check if cache exists and we're not forcing recomputation\n",
    "    if os.path.exists(cache_file) and not force_recompute:\n",
    "        print(f\"Loading cached embeddings for {domain} using {model_name}\")\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            return pkl.load(f)\n",
    "\n",
    "    # Cache doesn't exist or forced recomputation\n",
    "    if force_recompute:\n",
    "        print(f\"Force recomputing embeddings for {domain} using {model_name}...\")\n",
    "    else:\n",
    "        print(f\"Computing embeddings for {domain} using {model_name}...\")\n",
    "\n",
    "    if model_name == \"tf_idf\":\n",
    "        embeddings = tfidf_embedding.transform(texts)\n",
    "    else:\n",
    "        # Get the appropriate embedding model\n",
    "        embed_model = embedding_models[model_name]\n",
    "\n",
    "        # Process in batches for better memory efficiency\n",
    "        batch_size = 1  # Adjust based on available RAM\n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            batch_embeddings = list(embed_model.embed(batch_texts))\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        embeddings = np.array(all_embeddings)\n",
    "\n",
    "    # Cache the results\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pkl.dump(embeddings, f)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4defc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets.items():\n",
    "    train_data = dataset.sample(frac=0.8, random_state=22).reset_index(drop=True)\n",
    "    test_data = dataset.drop(train_data.index).reset_index(drop=True)\n",
    "\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    prediction_times = []\n",
    "\n",
    "    for model_name, embedding_model in embedding_models.items():\n",
    "        start_time = time.perf_counter_ns()\n",
    "\n",
    "        # Get cached or compute new embeddings\n",
    "        train_embeds = get_cached_embeddings(\n",
    "            train_data[\"prompt\"], model_name, f\"{domain}_train\"\n",
    "        )\n",
    "        test_embeds = get_cached_embeddings(\n",
    "            test_data[\"prompt\"], model_name, f\"{domain}_test\"\n",
    "        )\n",
    "\n",
    "        end_time = time.perf_counter_ns()\n",
    "        embed_times = end_time - start_time\n",
    "        mean_embed_time = embed_times / len(train_data + test_data)\n",
    "\n",
    "        print(f\"Embedding time for {model_name}: {mean_embed_time} ns\")\n",
    "\n",
    "        # Train and evaluate XGBoost model\n",
    "        util.train_and_evaluate_model(\n",
    "            model_name=\"XGBoost\",\n",
    "            train_embeds=train_embeds,\n",
    "            test_embeds=test_embeds,\n",
    "            train_labels=train_data[\"label\"],\n",
    "            test_labels=test_data[\"label\"],\n",
    "            domain=domain,\n",
    "            embed_model=model_name,\n",
    "            save_path=f\"models/XGBoost_{domain}_{model_name}.json\",\n",
    "            embedding_time=mean_embed_time,\n",
    "            training=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758447d",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf426782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF model\n",
    "with open(\"models/tfidf.pkl\", \"rb\") as f:\n",
    "    tfidf_embedding = pkl.load(f)\n",
    "\n",
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tf_idf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed_model_name, embedding_model in embedding_models.items():\n",
    "    xgb_law = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    xgb_finance = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    xgb_healthcare = XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    xgb_law.load_model(f\"models/XGBoost_law_{embed_model_name}.json\")\n",
    "    xgb_finance.load_model(f\"models/XGBoost_finance_{embed_model_name}.json\")\n",
    "    xgb_healthcare.load_model(f\"models/XGBoost_healthcare_{embed_model_name}.json\")\n",
    "\n",
    "    for domain, inference_df in eval_datasets.items():\n",
    "        actuals_ml = inference_df[\"label\"].tolist()\n",
    "\n",
    "        # Use cached embeddings or compute new ones\n",
    "        embeds = get_cached_embeddings(\n",
    "            inference_df[\"prompt\"], embed_model_name, f\"{domain}_eval\"\n",
    "        )\n",
    "\n",
    "        predictions_xgb = []\n",
    "        prediction_times_xgb = []\n",
    "\n",
    "        # Batch prediction for better performance\n",
    "        start_time = time.perf_counter_ns()\n",
    "        pred_finance = xgb_finance.predict(embeds)\n",
    "        pred_healthcare = xgb_healthcare.predict(embeds)\n",
    "        pred_law = xgb_law.predict(embeds)\n",
    "        end_time = time.perf_counter_ns()\n",
    "\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_times_xgb = [prediction_time / embeds.shape[0]] * embeds.shape[0]\n",
    "\n",
    "        # Combine predictions\n",
    "        predictions_xgb = [\n",
    "            0 if (f == 1 or h == 1 or l == 1) else 1\n",
    "            for f, h, l in zip(pred_finance, pred_healthcare, pred_law, strict=True)\n",
    "        ]\n",
    "\n",
    "        util.evaluate_run(\n",
    "            predictions=predictions_xgb,\n",
    "            true_labels=actuals_ml,\n",
    "            latency=statistics.mean(prediction_times_xgb),\n",
    "            domain=domain,\n",
    "            embed_model=embed_model_name,\n",
    "            model_name=\"XGBoost\",\n",
    "            train_acc=0.0,\n",
    "            cost=0.0,\n",
    "            training=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c96c0",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_model_name in [\"mini\", \"baai\", \"tf_idf\"]:\n",
    "    # Load models\n",
    "    xgb_law = XGBClassifier(tree_method=\"hist\", device=\"cuda\")\n",
    "    xgb_finance = XGBClassifier(tree_method=\"hist\", device=\"cuda\")\n",
    "    xgb_healthcare = XGBClassifier(tree_method=\"hist\", device=\"cuda\")\n",
    "\n",
    "    xgb_law.load_model(f\"models/XGBoost_law_{embedding_model_name}.json\")\n",
    "    xgb_finance.load_model(f\"models/XGBoost_finance_{embedding_model_name}.json\")\n",
    "    xgb_healthcare.load_model(f\"models/XGBoost_healthcare_{embedding_model_name}.json\")\n",
    "\n",
    "    xgb_batch_results = []\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(\n",
    "            f\"Processing batch size {batch_size} with {embedding_model_name} embeddings\"\n",
    "        )\n",
    "        batches = [\n",
    "            batch_data[i : i + batch_size]\n",
    "            for i in range(0, len(batch_data), batch_size)\n",
    "        ]\n",
    "        for batch in tqdm(batches):\n",
    "            batch_metrics = {\n",
    "                \"embed_time\": 0,\n",
    "                \"xgb_law_time\": 0,\n",
    "                \"xgb_finance_time\": 0,\n",
    "                \"xgb_health_time\": 0,\n",
    "            }\n",
    "\n",
    "            # Time embeddings\n",
    "            start_time = time.perf_counter()\n",
    "            embedding_model = embedding_models[embedding_model_name]\n",
    "            if embedding_model_name == \"tf_idf\":\n",
    "                embeds = embedding_model.transform(batch)\n",
    "            else:\n",
    "                embeds = np.array(list(embedding_model.embed(batch)))\n",
    "            batch_metrics[\"embed_time\"] += time.perf_counter() - start_time\n",
    "\n",
    "            # XGB predictions\n",
    "            start_time = time.perf_counter()\n",
    "            xgb_law_preds = xgb_law.predict(embeds)\n",
    "            batch_metrics[\"xgb_law_time\"] += time.perf_counter() - start_time\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            xgb_finance_preds = xgb_finance.predict(embeds)\n",
    "            batch_metrics[\"xgb_finance_time\"] += time.perf_counter() - start_time\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            xgb_health_preds = xgb_healthcare.predict(embeds)\n",
    "            batch_metrics[\"xgb_health_time\"] += time.perf_counter() - start_time\n",
    "\n",
    "            # Create a list of dictionaries, one for each prompt in the batch\n",
    "            results = []\n",
    "            for law_pred, finance_pred, health_pred in zip(\n",
    "                xgb_law_preds, xgb_finance_preds, xgb_health_preds, strict=True\n",
    "            ):\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"finance\": int(finance_pred),\n",
    "                        \"healthcare\": int(health_pred),\n",
    "                        \"law\": int(law_pred),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            xgb_batch_results.append(\n",
    "                {\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"time_taken_embed\": batch_metrics[\"embed_time\"],\n",
    "                    \"time_taken_law\": batch_metrics[\"xgb_law_time\"],\n",
    "                    \"time_taken_finance\": batch_metrics[\"xgb_finance_time\"],\n",
    "                    \"time_taken_healthcare\": batch_metrics[\"xgb_health_time\"],\n",
    "                    \"results\": results,\n",
    "                    \"model_name\": \"xgb\",\n",
    "                    \"embedding_model\": embedding_model_name,\n",
    "                    \"embedding\": True,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    pd.DataFrame(xgb_batch_results).to_csv(\n",
    "        f\"data/results/batch_xgb_{embedding_model_name}.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbdef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
