{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5786d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import dataloader\n",
    "import util\n",
    "\n",
    "util.set_seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6a7294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a03371bf4654f1e916d1b7dc3828637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/410 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3530ff159164f49a10051415f6883e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-9cc2ae3631bff610.parquet:   0%|          | 0.00/38.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158c236ff5ac42758b7476609bf9aa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/24343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36b0b36a0ad4428abc6bfde3d2e3adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d71df0c183345c894c789f657530ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "finance_questions_dataset.json:   0%|          | 0.00/53.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef68a75006bb4a68a848564d8fbba78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/53937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026393a5aa03442b9ffe728e219c9862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ta-ChatDoctor-HealthCareMagic-100k.jsonl:   0%|          | 0.00/23.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e38dda03e384fc18eb729a71e271b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/19999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36335b19055f47a7a803b5292bd2ec0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c96c5317fd4538a3896faa303e4b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de498cc2002d4aebadafae66d814c66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22145064a4a547759ead9e2352a5343b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2f69766aff4d45b9d60f36ec0a8e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/53937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbef1d373267481eab5712efdf148e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cc271583164df1ba3e9ed6e5dc3245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/19999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf65ed4e79ae411d814b62c4b268dec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/19996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ffd3b1396b478f81e68ded2ead0dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = dataloader.get_domain_data()\n",
    "eval_datasets = dataloader.get_eval_datasets()\n",
    "batch_data = dataloader.get_batch_data()\n",
    "\n",
    "batch_sizes = [1, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c3e50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ddee66569a4f3483afa361bf390ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904544d20b9f46a69adc4c5d36ef1e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67014c85bb34015acf78f53894428d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc6b7e04b53452cbfb4e52a2f16915f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcf0d9bf78946219c9de9194cdb296e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dec62c2eb142869a0026272971c5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-08 13:14:46.625632107 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-08 13:14:46.625675278 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb0bf553f3f497d8bc0292da6f6139c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75db7349591d4f13b29f9e38c3afcb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd98fc706bfc4e3298be52f658dc9bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435bcd9c20c6418f898828819feeea1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f4f0c91239454aa863920886b0f028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144ffabd866346c2b62bc0bb9b98d076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-08 13:14:50.421783206 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-08 13:14:50.423256444 [W:onnxruntime:, session_state.cc:1170 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "baai_embedding = TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", providers=[\"CUDAExecutionProvider\"]\n",
    ")\n",
    "mini_embedding = TextEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "tfidf_embedding = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset = next(iter(datasets.values()))[\"prompt\"]\n",
    "train_prompts = first_dataset.sample(frac=0.8, random_state=22)\n",
    "\n",
    "tfidf_embedding.fit(train_prompts)\n",
    "\n",
    "with open(\"models/tfidf.pkl\", \"wb\") as f:\n",
    "    pkl.dump(tfidf_embedding, f)\n",
    "\n",
    "# Create embedding cache directory\n",
    "os.makedirs(\"cache/embeddings\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tf_idf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33151981",
   "metadata": {},
   "source": [
    "# Embedding Cache Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eba3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_embeddings(texts, model_name, domain, cache_dir=\"cache/embeddings\", force_recompute=False):\n",
    "    \"\"\"Get embeddings from cache if available, otherwise compute and cache them.\n",
    "    \n",
    "    Args:\n",
    "        texts: The texts to embed\n",
    "        model_name: The name of the embedding model to use\n",
    "        domain: The domain identifier for the cache\n",
    "        cache_dir: Directory to store/retrieve cached embeddings\n",
    "        force_recompute: If True, ignore cache and recompute embeddings\n",
    "    \n",
    "    Returns:\n",
    "        The embeddings matrix\n",
    "    \"\"\"\n",
    "    cache_file = f\"{cache_dir}/{domain}_{model_name}_embeddings.pkl\"\n",
    "\n",
    "    # Check if cache exists and we're not forcing recomputation\n",
    "    if os.path.exists(cache_file) and not force_recompute:\n",
    "        print(f\"Loading cached embeddings for {domain} using {model_name}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pkl.load(f)\n",
    "\n",
    "    # Cache doesn't exist or forced recomputation\n",
    "    if force_recompute:\n",
    "        print(f\"Force recomputing embeddings for {domain} using {model_name}...\")\n",
    "    else:\n",
    "        print(f\"Computing embeddings for {domain} using {model_name}...\")\n",
    "\n",
    "    if model_name == \"tf_idf\":\n",
    "        embeddings = tfidf_embedding.transform(texts)\n",
    "    else:\n",
    "        # Get the appropriate embedding model\n",
    "        embed_model = embedding_models[model_name]\n",
    "\n",
    "        # Process in batches for better memory efficiency\n",
    "        batch_size = 1  # Adjust based on available RAM\n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = list(embed_model.embed(batch_texts))\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        embeddings = np.array(all_embeddings)\n",
    "\n",
    "    # Cache the results\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pkl.dump(embeddings, f)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cc7c5",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, dataset in datasets.items():\n",
    "    train_data = dataset.sample(frac=0.8, random_state=22).reset_index(drop=True)\n",
    "    test_data = dataset.drop(train_data.index).reset_index(drop=True)\n",
    "\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    prediction_times = []\n",
    "\n",
    "    for model_name, embedding_model in embedding_models.items():\n",
    "        start_time = time.perf_counter_ns()\n",
    "\n",
    "        # Get cached or compute new embeddings\n",
    "        train_embeds = get_cached_embeddings(train_data[\"prompt\"], model_name, f\"{domain}_train\")\n",
    "        test_embeds = get_cached_embeddings(test_data[\"prompt\"], model_name, f\"{domain}_test\")\n",
    "\n",
    "        end_time = time.perf_counter_ns()\n",
    "        embed_times = end_time - start_time\n",
    "        mean_embed_time = embed_times / len(train_data + test_data)\n",
    "\n",
    "        print(f\"Embedding time for {model_name}: {mean_embed_time} ns\")\n",
    "\n",
    "        # Train and evaluate SVM model\n",
    "        util.train_and_evaluate_model(\n",
    "            model_name=\"SVM\",\n",
    "            train_embeds=train_embeds,\n",
    "            test_embeds=test_embeds,\n",
    "            train_labels=train_data[\"label\"],\n",
    "            test_labels=test_data[\"label\"],\n",
    "            domain=domain,\n",
    "            embed_model=model_name,\n",
    "            save_path=f\"models/SVM_{domain}_{model_name}.pkl\",\n",
    "            embedding_time=mean_embed_time,\n",
    "            training=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758447d",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf426782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF model\n",
    "with open(\"models/tfidf.pkl\", \"rb\") as f:\n",
    "    tfidf_embedding = pkl.load(f)\n",
    "\n",
    "embedding_models = {\n",
    "    \"mini\": mini_embedding,\n",
    "    \"tf_idf\": tfidf_embedding,\n",
    "    \"baai\": baai_embedding,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed_model_name, embedding_model in embedding_models.items():\n",
    "    # Load SVM models\n",
    "    with open(f\"models/SVM_finance_{embed_model_name}.pkl\", \"rb\") as f:\n",
    "        svm_finance = pkl.load(f)\n",
    "    with open(f\"models/SVM_healthcare_{embed_model_name}.pkl\", \"rb\") as f:\n",
    "        svm_healthcare = pkl.load(f)\n",
    "    with open(f\"models/SVM_law_{embed_model_name}.pkl\", \"rb\") as f:\n",
    "        svm_law = pkl.load(f)\n",
    "\n",
    "    for domain, inference_df in eval_datasets.items():\n",
    "        # Get actual labels once\n",
    "        actuals_ml = inference_df[\"label\"].tolist()\n",
    "\n",
    "        # Use cached embeddings or compute new ones\n",
    "        test_embeds = get_cached_embeddings(inference_df[\"prompt\"], embed_model_name, f\"{domain}_eval\")\n",
    "\n",
    "        predictions_svm = []\n",
    "        prediction_times_svm = []\n",
    "\n",
    "        # Make batch predictions instead of one-by-one\n",
    "        start_time = time.perf_counter_ns()\n",
    "        pred_finance = svm_finance.predict(test_embeds)\n",
    "        pred_healthcare = svm_healthcare.predict(test_embeds)\n",
    "        pred_law = svm_law.predict(test_embeds)\n",
    "        end_time = time.perf_counter_ns()\n",
    "\n",
    "        # Distribute the prediction time across all samples for latency calculation\n",
    "        prediction_time = end_time - start_time\n",
    "        prediction_times_svm = [prediction_time / test_embeds.shape[0]] * test_embeds.shape[0]\n",
    "\n",
    "        # Combine predictions\n",
    "        predictions_svm = [\n",
    "            0 if (f == 1 or h == 1 or l == 1) else 1\n",
    "            for f, h, l in zip(pred_finance, pred_healthcare, pred_law, strict=True)\n",
    "        ]\n",
    "\n",
    "        # Evaluate results\n",
    "        util.evaluate_run(\n",
    "            predictions=predictions_svm,\n",
    "            true_labels=actuals_ml,\n",
    "            latency=statistics.mean(prediction_times_svm),\n",
    "            domain=domain,\n",
    "            embed_model=embed_model_name,\n",
    "            model_name=\"SVM\",\n",
    "            train_acc=0.0,\n",
    "            cost=0.0,\n",
    "            training=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221c96c0",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_model_name in [\"mini\", \"baai\", \"tf_idf\"]:\n",
    "    svm_batch_results = []\n",
    "    # Load SVM models\n",
    "    with open(f\"models/SVM_finance_{embedding_model_name}.pkl\", \"rb\") as f:\n",
    "        svm_finance = pkl.load(f)\n",
    "    with open(f\"models/SVM_healthcare_{embedding_model_name}.pkl\", \"rb\") as f:\n",
    "        svm_healthcare = pkl.load(f)\n",
    "    with open(f\"models/SVM_law_{embedding_model_name}.pkl\", \"rb\") as f:\n",
    "        svm_law = pkl.load(f)\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Processing batch size {batch_size} with {embedding_model_name} embeddings\")\n",
    "        batches = [\n",
    "            batch_data[i : i + batch_size]\n",
    "            for i in range(0, len(batch_data), batch_size)\n",
    "        ]\n",
    "        for batch in tqdm(batches):\n",
    "            batch_metrics = {\n",
    "                \"embed_time\": 0,\n",
    "                \"svm_law_time\": 0,\n",
    "                \"svm_finance_time\": 0,\n",
    "                \"svm_health_time\": 0,\n",
    "            }\n",
    "\n",
    "            # Time embeddings\n",
    "            start_time = time.perf_counter()\n",
    "            embedding_model = embedding_models[embedding_model_name]\n",
    "            if embedding_model_name == \"tf_idf\":\n",
    "                embeds = embedding_model.transform(batch)\n",
    "            else:\n",
    "                embeds = np.array(list(embedding_model.embed(batch)))\n",
    "            batch_metrics[\"embed_time\"] += time.perf_counter() - start_time\n",
    "\n",
    "            # Get all predictions and time them\n",
    "            start_time = time.perf_counter()\n",
    "            svm_law_preds = svm_law.predict(embeds)\n",
    "            batch_metrics[\"svm_law_time\"] += time.perf_counter() - start_time\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            svm_finance_preds = svm_finance.predict(embeds)\n",
    "            batch_metrics[\"svm_finance_time\"] += time.perf_counter() - start_time\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            svm_health_preds = svm_healthcare.predict(embeds)\n",
    "            batch_metrics[\"svm_health_time\"] += time.perf_counter() - start_time\n",
    "\n",
    "            # Create a list of dictionaries, one for each prompt in the batch\n",
    "            results = []\n",
    "            for law_pred, finance_pred, health_pred in zip(svm_law_preds, svm_finance_preds, svm_health_preds, strict=True):\n",
    "                results.append({\n",
    "                    'finance': int(finance_pred),\n",
    "                    'healthcare': int(health_pred),\n",
    "                    'law': int(law_pred)\n",
    "                })\n",
    "\n",
    "            # Record results for this batch\n",
    "            svm_batch_results.append(\n",
    "                {\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"time_taken_embed\": batch_metrics[\"embed_time\"],\n",
    "                    \"time_taken_law\": batch_metrics[\"svm_law_time\"],\n",
    "                    \"time_taken_finance\": batch_metrics[\"svm_finance_time\"],\n",
    "                    \"time_taken_healthcare\": batch_metrics[\"svm_health_time\"],\n",
    "                    \"results\": results,\n",
    "                    \"model_name\": \"svm\",\n",
    "                    \"embedding_model\": embedding_model_name,\n",
    "                    \"embedding\": True,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    pd.DataFrame(svm_batch_results).to_csv(\n",
    "        f\"data/results/batch_svm_{embedding_model_name}.csv\", index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify results format\n",
    "if svm_batch_results:\n",
    "    print(f\"Number of batch results: {len(svm_batch_results)}\")\n",
    "    print(f\"First batch size: {svm_batch_results[0]['batch_size']}\")\n",
    "    print(f\"Number of results in first batch: {len(svm_batch_results[0]['results'])}\")\n",
    "    print(\"\\nSample results from first batch:\")\n",
    "    for i in range(min(3, len(svm_batch_results[0]['results']))):\n",
    "        print(f\"  Result {i+1}: {svm_batch_results[0]['results'][i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
